#!/usr/bin/env python3
"""
ì»¨í…ìŠ¤íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ê¸°ë°˜ ì§ˆì˜ ì‘ë‹µ í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ
- ë‹¤ì–‘í•œ ë³µì¡ë„ì™€ ì£¼ì œì˜ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ì„¸íŠ¸
- ì‘ë‹µ í’ˆì§ˆ í‰ê°€ ë©”íŠ¸ë¦­
- ê°œì„  í¬ì¸íŠ¸ ì‹ë³„
"""

import asyncio
import json
import time
from typing import List, Dict, Any, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
import statistics

# í”„ë¡œì íŠ¸ ëª¨ë“ˆë“¤
from api.services.chat_service import ChatService
from api.services.enhanced_chat_service import EnhancedChatService
from api.config import settings

@dataclass
class TestQuestion:
    """í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ì •ì˜"""
    id: str
    question: str
    category: str
    complexity: str  # simple, medium, complex
    expected_elements: List[str]  # ì‘ë‹µì— í¬í•¨ë˜ì–´ì•¼ í•  í•µì‹¬ ìš”ì†Œë“¤
    evaluation_criteria: Dict[str, str]  # í‰ê°€ ê¸°ì¤€

@dataclass
class ResponseEvaluation:
    """ì‘ë‹µ í‰ê°€ ê²°ê³¼"""
    question_id: str
    response_text: str
    response_time: float
    relevance_score: float  # 0-10
    completeness_score: float  # 0-10
    accuracy_score: float  # 0-10
    context_usage_score: float  # 0-10
    overall_score: float  # 0-10
    missing_elements: List[str]
    strengths: List[str]
    weaknesses: List[str]
    improvement_suggestions: List[str]

class ContextEngineeringQualityTester:
    """ì»¨í…ìŠ¤íŠ¸ ì—”ì§€ë‹ˆì–´ë§ í’ˆì§ˆ í…ŒìŠ¤í„°"""

    def __init__(self):
        self.chat_service = None
        self.enhanced_chat_service = None
        self.test_results = []

    async def initialize_services(self):
        """ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        try:
            self.enhanced_chat_service = EnhancedChatService()
            print("âœ… Enhanced Chat Service ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ Enhanced Chat Service ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            try:
                self.chat_service = ChatService()
                print("âœ… Basic Chat Service ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e2:
                print(f"âŒ Chat Service ì´ˆê¸°í™” ì‹¤íŒ¨: {e2}")
                raise

    def create_test_questions(self) -> List[TestQuestion]:
        """ë‹¤ì–‘í•œ ë³µì¡ë„ì™€ ì£¼ì œì˜ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ìƒì„±"""
        questions = [
            # === ë‹¨ìˆœ ì§ˆë¬¸ (Simple) ===
            TestQuestion(
                id="simple_01",
                question="ì‚¼ì„±ì „ì ì£¼ê°€ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
                category="ì£¼ì‹ì •ë³´",
                complexity="simple",
                expected_elements=["ì‚¼ì„±ì „ì", "ì£¼ê°€", "í˜„ì¬ ê°€ê²©", "ë“±ë½ë¥ "],
                evaluation_criteria={
                    "relevance": "ì‚¼ì„±ì „ì ì£¼ê°€ ì •ë³´ ì§ì ‘ ì œê³µ",
                    "completeness": "í˜„ì¬ê°€, ë“±ë½ë¥ , ê±°ë˜ëŸ‰ ë“± ê¸°ë³¸ ì •ë³´",
                    "accuracy": "ìµœì‹  ì£¼ê°€ ë°ì´í„° ì •í™•ì„±"
                }
            ),

            TestQuestion(
                id="simple_02",
                question="ì˜¤ëŠ˜ ì£¼ìš” ë‰´ìŠ¤ê°€ ë­ê°€ ìˆë‚˜ìš”?",
                category="ë‰´ìŠ¤ì •ë³´",
                complexity="simple",
                expected_elements=["ì˜¤ëŠ˜", "ì£¼ìš” ë‰´ìŠ¤", "í—¤ë“œë¼ì¸", "ìš”ì•½"],
                evaluation_criteria={
                    "relevance": "ë‹¹ì¼ ì£¼ìš” ë‰´ìŠ¤ ì œê³µ",
                    "completeness": "ë‹¤ì–‘í•œ ë¶„ì•¼ ë‰´ìŠ¤ í¬ê´„",
                    "accuracy": "ìµœì‹ ì„±ê³¼ ì •í™•ì„±"
                }
            ),

            # === ì¤‘ê°„ ë³µì¡ë„ ì§ˆë¬¸ (Medium) ===
            TestQuestion(
                id="medium_01",
                question="ë°˜ë„ì²´ ì—…ê³„ì˜ ìµœê·¼ ë™í–¥ê³¼ ê´€ë ¨ ì£¼ì‹ë“¤ì˜ ì „ë§ì€ ì–´ë–¤ê°€ìš”?",
                category="ì‚°ì—…ë¶„ì„",
                complexity="medium",
                expected_elements=["ë°˜ë„ì²´ ì—…ê³„", "ìµœê·¼ ë™í–¥", "ê´€ë ¨ ì£¼ì‹", "ì „ë§", "ë¶„ì„"],
                evaluation_criteria={
                    "relevance": "ë°˜ë„ì²´ ì—…ê³„ ë™í–¥ê³¼ ì£¼ì‹ ì—°ê²° ë¶„ì„",
                    "completeness": "ì—…ê³„ í˜„í™©, ì£¼ìš” ê¸°ì—…, ì „ë§ ì¢…í•© ì œê³µ",
                    "accuracy": "ì—…ê³„ ì „ë¬¸ ì§€ì‹ê³¼ ìµœì‹  ì •ë³´ ê²°í•©"
                }
            ),

            TestQuestion(
                id="medium_02",
                question="ë¯¸êµ­ ê¸ˆë¦¬ ì¸ìƒì´ í•œêµ­ ê²½ì œì™€ ì£¼ì‹ì‹œì¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•´ì£¼ì„¸ìš”",
                category="ê²½ì œë¶„ì„",
                complexity="medium",
                expected_elements=["ë¯¸êµ­ ê¸ˆë¦¬", "í•œêµ­ ê²½ì œ", "ì£¼ì‹ì‹œì¥", "ì˜í–¥ ë¶„ì„", "ì¸ê³¼ê´€ê³„"],
                evaluation_criteria={
                    "relevance": "ê¸ˆë¦¬-ê²½ì œ-ì£¼ì‹ ì—°ê´€ì„± ë¶„ì„",
                    "completeness": "ê±°ì‹œê²½ì œì  ì˜í–¥ê³¼ êµ¬ì²´ì  ì‹œì¥ ë°˜ì‘",
                    "accuracy": "ê²½ì œí•™ì  ë…¼ë¦¬ì™€ ì‹¤ì œ ë°ì´í„° ê¸°ë°˜"
                }
            ),

            TestQuestion(
                id="medium_03",
                question="ESG íˆ¬ìê°€ ì£¼ëª©ë°›ëŠ” ì´ìœ ì™€ ê´€ë ¨ ìœ ë§ ì¢…ëª©ë“¤ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”",
                category="íˆ¬ìì „ëµ",
                complexity="medium",
                expected_elements=["ESG íˆ¬ì", "ì£¼ëª©ë°›ëŠ” ì´ìœ ", "ìœ ë§ ì¢…ëª©", "ì¶”ì²œ ê·¼ê±°"],
                evaluation_criteria={
                    "relevance": "ESG íˆ¬ì íŠ¸ë Œë“œì™€ ì¢…ëª© ì—°ê²°",
                    "completeness": "ESG ê°œë…, ì‹œì¥ ë™í–¥, êµ¬ì²´ì  ì¢…ëª©",
                    "accuracy": "ESG í‰ê°€ ê¸°ì¤€ê³¼ ì¢…ëª© ì •ë³´ ì •í™•ì„±"
                }
            ),

            # === ë³µí•© ì§ˆë¬¸ (Complex) ===
            TestQuestion(
                id="complex_01",
                question="ìµœê·¼ ê¸€ë¡œë²Œ ê³µê¸‰ë§ ì´ìŠˆì™€ ì¸í”Œë ˆì´ì…˜ ìš°ë ¤ê°€ IT ëŒ€ê¸°ì—…ë“¤ì˜ ì‹¤ì ê³¼ ì£¼ê°€ì— ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì¹˜ê³  ìˆìœ¼ë©°, í–¥í›„ 6ê°œì›”ê°„ íˆ¬ì ì „ëµì€ ì–´ë–»ê²Œ ìˆ˜ë¦½í•´ì•¼ í• ê¹Œìš”?",
                category="ì¢…í•©ë¶„ì„",
                complexity="complex",
                expected_elements=["ê³µê¸‰ë§ ì´ìŠˆ", "ì¸í”Œë ˆì´ì…˜", "IT ëŒ€ê¸°ì—…", "ì‹¤ì  ì˜í–¥", "ì£¼ê°€ ì˜í–¥", "íˆ¬ì ì „ëµ", "6ê°œì›” ì „ë§"],
                evaluation_criteria={
                    "relevance": "ë‹¤ì¤‘ ìš”ì¸ ë¶„ì„ê³¼ íˆ¬ì ì „ëµ ì—°ê²°",
                    "completeness": "ê±°ì‹œ ì´ìŠˆ, ê¸°ì—… ë¶„ì„, ì „ëµ ì œì‹œ",
                    "accuracy": "ë³µí•©ì  ì¸ê³¼ê´€ê³„ ì •í™•í•œ ë¶„ì„"
                }
            ),

            TestQuestion(
                id="complex_02",
                question="íƒ„ì†Œì¤‘ë¦½ ì •ì±…ì´ ì „í†µ ì œì¡°ì—…ì²´ë“¤ì˜ ì‚¬ì—… ëª¨ë¸ ë³€í™”ë¥¼ ì–´ë–»ê²Œ ìœ ë„í•˜ê³  ìˆìœ¼ë©°, ì´ ê³¼ì •ì—ì„œ ìƒˆë¡œìš´ íˆ¬ì ê¸°íšŒì™€ ë¦¬ìŠ¤í¬ëŠ” ë¬´ì—‡ì¸ì§€ êµ¬ì²´ì ì¸ ê¸°ì—… ì‚¬ë¡€ì™€ í•¨ê»˜ ë¶„ì„í•´ì£¼ì„¸ìš”",
                category="ì •ì±…ë¶„ì„",
                complexity="complex",
                expected_elements=["íƒ„ì†Œì¤‘ë¦½", "ì œì¡°ì—…ì²´", "ì‚¬ì—…ëª¨ë¸ ë³€í™”", "íˆ¬ì ê¸°íšŒ", "ë¦¬ìŠ¤í¬", "ê¸°ì—… ì‚¬ë¡€"],
                evaluation_criteria={
                    "relevance": "ì •ì±…-ê¸°ì—…-íˆ¬ì ë‹¤ì¸µ ë¶„ì„",
                    "completeness": "ì •ì±… ë°°ê²½, ê¸°ì—… ëŒ€ì‘, íˆ¬ì ê´€ì ",
                    "accuracy": "ì‹¤ì œ ê¸°ì—… ì‚¬ë¡€ì™€ ë°ì´í„° ê¸°ë°˜"
                }
            ),

            # === ì¶”ìƒì  ì§ˆë¬¸ ===
            TestQuestion(
                id="abstract_01",
                question="ë””ì§€í„¸ ì „í™˜ì´ ê°€ì†í™”ë˜ëŠ” ì‹œëŒ€ì— 'ê°€ì¹˜ íˆ¬ì'ì˜ ì˜ë¯¸ì™€ ë°©ë²•ë¡ ì€ ì–´ë–»ê²Œ ì§„í™”í•´ì•¼ í• ê¹Œìš”?",
                category="íˆ¬ìì² í•™",
                complexity="complex",
                expected_elements=["ë””ì§€í„¸ ì „í™˜", "ê°€ì¹˜ íˆ¬ì", "ì˜ë¯¸ ì§„í™”", "ë°©ë²•ë¡  ë³€í™”"],
                evaluation_criteria={
                    "relevance": "ì‹œëŒ€ ë³€í™”ì™€ íˆ¬ì ì² í•™ ì—°ê²°",
                    "completeness": "ê°œë… ì •ì˜, ë³€í™” ìš”ì¸, ìƒˆë¡œìš´ ì ‘ê·¼ë²•",
                    "accuracy": "íˆ¬ì ì´ë¡ ê³¼ í˜„ì‹¤ ì ìš© ì •í™•ì„±"
                }
            ),

            # === ì‹œê³„ì—´ ë¶„ì„ ì§ˆë¬¸ ===
            TestQuestion(
                id="timeseries_01",
                question="ì§€ë‚œ 3ë…„ê°„ ì½”ìŠ¤í”¼ ì§€ìˆ˜ì˜ ë³€ë™ íŒ¨í„´ê³¼ ì£¼ìš” ì´ë²¤íŠ¸ë“¤ì˜ ì˜í–¥ì„ ë¶„ì„í•˜ê³ , í–¥í›„ 1ë…„ê°„ì˜ ì‹œì¥ ë°©í–¥ì„±ì„ ì˜ˆì¸¡í•´ì£¼ì„¸ìš”",
                category="ì‹œì¥ë¶„ì„",
                complexity="complex",
                expected_elements=["3ë…„ê°„", "ì½”ìŠ¤í”¼", "ë³€ë™ íŒ¨í„´", "ì£¼ìš” ì´ë²¤íŠ¸", "ì˜í–¥ ë¶„ì„", "1ë…„ ì˜ˆì¸¡"],
                evaluation_criteria={
                    "relevance": "ê³¼ê±° íŒ¨í„´ê³¼ ë¯¸ë˜ ì˜ˆì¸¡ ì—°ê²°",
                    "completeness": "ì—­ì‚¬ì  ë¶„ì„ê³¼ ì˜ˆì¸¡ ê·¼ê±°",
                    "accuracy": "ë°ì´í„° ê¸°ë°˜ ë¶„ì„ê³¼ í•©ë¦¬ì  ì˜ˆì¸¡"
                }
            )
        ]

        return questions

    async def run_test(self, question: TestQuestion) -> Tuple[str, float]:
        """ê°œë³„ ì§ˆë¬¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        start_time = time.time()

        try:
            if self.enhanced_chat_service:
                response = await self.enhanced_chat_service.generate_answer(question.question)
            else:
                response = await self.chat_service.generate_answer(question.question)

            response_time = time.time() - start_time

            # ì‘ë‹µ êµ¬ì¡°ì— ë”°ë¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            if isinstance(response, dict):
                response_text = response.get("answer", "") or response.get("markdown", "") or response.get("response", "")
            else:
                response_text = str(response)

            return response_text, response_time

        except Exception as e:
            response_time = time.time() - start_time
            return f"ERROR: {str(e)}", response_time

    def evaluate_response(self, question: TestQuestion, response: str, response_time: float) -> ResponseEvaluation:
        """ì‘ë‹µ í’ˆì§ˆ í‰ê°€"""

        # ê¸°ë³¸ í‰ê°€ ì ìˆ˜ ì´ˆê¸°í™”
        relevance_score = 0
        completeness_score = 0
        accuracy_score = 0
        context_usage_score = 0

        missing_elements = []
        strengths = []
        weaknesses = []
        suggestions = []

        response_lower = response.lower()

        # === ê´€ë ¨ì„± í‰ê°€ (Relevance) ===
        expected_count = 0
        for element in question.expected_elements:
            if element.lower() in response_lower:
                expected_count += 1
            else:
                missing_elements.append(element)

        relevance_score = (expected_count / len(question.expected_elements)) * 10

        # === ì™„ì„±ë„ í‰ê°€ (Completeness) ===
        if len(response) < 50:
            completeness_score = 2
            weaknesses.append("ì‘ë‹µì´ ë„ˆë¬´ ì§§ìŒ")
        elif len(response) < 200:
            completeness_score = 5
            suggestions.append("ë” ìƒì„¸í•œ ì„¤ëª… í•„ìš”")
        elif len(response) < 500:
            completeness_score = 7
            strengths.append("ì ì ˆí•œ ë¶„ëŸ‰")
        else:
            completeness_score = 9
            strengths.append("ì¶©ë¶„íˆ ìƒì„¸í•œ ì‘ë‹µ")

        # === ì •í™•ì„± í‰ê°€ (Accuracy) ===
        if "ERROR" in response or "ì˜¤ë¥˜" in response or "ì‹¤íŒ¨" in response:
            accuracy_score = 1
            weaknesses.append("ì‹œìŠ¤í…œ ì˜¤ë¥˜ ë°œìƒ")
        elif "ì£„ì†¡" in response or "ì•Œ ìˆ˜ ì—†" in response:
            accuracy_score = 3
            weaknesses.append("ì •ë³´ ë¶€ì¡±ìœ¼ë¡œ ë‹µë³€ ì œí•œ")
        elif "ì¶”ì •" in response or "ì˜ˆìƒ" in response:
            accuracy_score = 6
            strengths.append("ë¶ˆí™•ì‹¤ì„± ëª…ì‹œ")
        else:
            accuracy_score = 8
            strengths.append("í™•ì‹ ìˆëŠ” ë‹µë³€")

        # === ì»¨í…ìŠ¤íŠ¸ í™œìš©ë„ í‰ê°€ ===
        context_indicators = ["ìµœê·¼", "í˜„ì¬", "ì˜¤ëŠ˜", "ì‹¤ì‹œê°„", "ì—…ë°ì´íŠ¸", "ë¶„ì„", "ë°ì´í„°"]
        context_count = sum(1 for indicator in context_indicators if indicator in response_lower)
        context_usage_score = min(context_count * 1.5, 10)

        if context_count >= 3:
            strengths.append("ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ì˜ í™œìš©")
        else:
            suggestions.append("ë” ë§ì€ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ í™œìš© í•„ìš”")

        # === ë³µì¡ë„ë³„ ì¶”ê°€ í‰ê°€ ===
        if question.complexity == "complex":
            if "ë”°ë¼ì„œ" in response or "ê·¸ëŸ¬ë¯€ë¡œ" in response or "ê²°ë¡ ì ìœ¼ë¡œ" in response:
                strengths.append("ë…¼ë¦¬ì  ê²°ë¡  ì œì‹œ")
            else:
                suggestions.append("ë¶„ì„ ê²°ê³¼ì˜ ë…¼ë¦¬ì  ê²°ë¡  í•„ìš”")

        # === ì „ì²´ ì ìˆ˜ ê³„ì‚° ===
        overall_score = (relevance_score * 0.3 +
                        completeness_score * 0.25 +
                        accuracy_score * 0.25 +
                        context_usage_score * 0.2)

        # ì‘ë‹µ ì‹œê°„ íŒ¨ë„í‹°
        if response_time > 10:
            weaknesses.append(f"ì‘ë‹µ ì‹œê°„ ë„ˆë¬´ ê¸¸ìŒ ({response_time:.1f}ì´ˆ)")
            overall_score *= 0.9
        elif response_time > 5:
            suggestions.append("ì‘ë‹µ ì‹œê°„ ìµœì í™” í•„ìš”")
            overall_score *= 0.95

        return ResponseEvaluation(
            question_id=question.id,
            response_text=response,
            response_time=response_time,
            relevance_score=relevance_score,
            completeness_score=completeness_score,
            accuracy_score=accuracy_score,
            context_usage_score=context_usage_score,
            overall_score=overall_score,
            missing_elements=missing_elements,
            strengths=strengths,
            weaknesses=weaknesses,
            improvement_suggestions=suggestions
        )

    async def run_comprehensive_test(self) -> List[ResponseEvaluation]:
        """ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        await self.initialize_services()

        questions = self.create_test_questions()
        evaluations = []

        print(f"\nğŸš€ ì»¨í…ìŠ¤íŠ¸ ì—”ì§€ë‹ˆì–´ë§ í’ˆì§ˆ í…ŒìŠ¤íŠ¸ ì‹œì‘ ({len(questions)}ê°œ ì§ˆë¬¸)")
        print("=" * 80)

        for i, question in enumerate(questions, 1):
            print(f"\n[{i}/{len(questions)}] {question.complexity.upper()} - {question.category}")
            print(f"Q: {question.question}")
            print("-" * 60)

            response, response_time = await self.run_test(question)
            evaluation = self.evaluate_response(question, response, response_time)
            evaluations.append(evaluation)

            print(f"ì‘ë‹µì‹œê°„: {response_time:.2f}ì´ˆ")
            print(f"ì¢…í•©ì ìˆ˜: {evaluation.overall_score:.1f}/10")
            print(f"ì„¸ë¶€ì ìˆ˜: ê´€ë ¨ì„±({evaluation.relevance_score:.1f}) "
                  f"ì™„ì„±ë„({evaluation.completeness_score:.1f}) "
                  f"ì •í™•ì„±({evaluation.accuracy_score:.1f}) "
                  f"ì»¨í…ìŠ¤íŠ¸({evaluation.context_usage_score:.1f})")

            if evaluation.strengths:
                print(f"âœ… ê°•ì : {', '.join(evaluation.strengths)}")
            if evaluation.weaknesses:
                print(f"âš ï¸ ì•½ì : {', '.join(evaluation.weaknesses)}")

            # ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 200ì)
            preview = response[:200] + ("..." if len(response) > 200 else "")
            print(f"ì‘ë‹µ: {preview}")

        return evaluations

    def generate_comprehensive_report(self, evaluations: List[ResponseEvaluation]) -> Dict[str, Any]:
        """ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ ìƒì„±"""
        if not evaluations:
            return {"error": "í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤"}

        # ê¸°ë³¸ í†µê³„
        overall_scores = [e.overall_score for e in evaluations]
        avg_score = statistics.mean(overall_scores)
        median_score = statistics.median(overall_scores)

        # ë³µì¡ë„ë³„ ë¶„ì„
        complexity_analysis = {}
        for complexity in ["simple", "medium", "complex"]:
            complex_evals = [e for e in evaluations if complexity in e.question_id]
            if complex_evals:
                complexity_analysis[complexity] = {
                    "count": len(complex_evals),
                    "avg_score": statistics.mean([e.overall_score for e in complex_evals]),
                    "avg_time": statistics.mean([e.response_time for e in complex_evals])
                }

        # ì£¼ìš” ë¬¸ì œì  ì§‘ê³„
        all_weaknesses = []
        all_suggestions = []
        missing_elements_count = {}

        for eval in evaluations:
            all_weaknesses.extend(eval.weaknesses)
            all_suggestions.extend(eval.improvement_suggestions)
            for element in eval.missing_elements:
                missing_elements_count[element] = missing_elements_count.get(element, 0) + 1

        # ë¹ˆë„ìˆ˜ ê¸°ì¤€ ìƒìœ„ ë¬¸ì œì 
        weakness_frequency = {}
        for weakness in all_weaknesses:
            weakness_frequency[weakness] = weakness_frequency.get(weakness, 0) + 1

        top_weaknesses = sorted(weakness_frequency.items(), key=lambda x: x[1], reverse=True)[:5]
        top_missing_elements = sorted(missing_elements_count.items(), key=lambda x: x[1], reverse=True)[:5]

        return {
            "summary": {
                "total_questions": len(evaluations),
                "average_score": round(avg_score, 2),
                "median_score": round(median_score, 2),
                "score_distribution": {
                    "excellent (9-10)": sum(1 for s in overall_scores if s >= 9),
                    "good (7-8.9)": sum(1 for s in overall_scores if 7 <= s < 9),
                    "average (5-6.9)": sum(1 for s in overall_scores if 5 <= s < 7),
                    "poor (0-4.9)": sum(1 for s in overall_scores if s < 5)
                }
            },
            "complexity_analysis": complexity_analysis,
            "top_issues": {
                "frequent_weaknesses": top_weaknesses,
                "commonly_missing_elements": top_missing_elements
            },
            "improvement_priorities": list(set(all_suggestions)),
            "detailed_results": [asdict(e) for e in evaluations]
        }

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    tester = ContextEngineeringQualityTester()

    try:
        # ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        evaluations = await tester.run_comprehensive_test()

        # ë³´ê³ ì„œ ìƒì„±
        report = tester.generate_comprehensive_report(evaluations)

        # ê²°ê³¼ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = f"/data/dev/git/ontology_chat/context_quality_report_{timestamp}.json"

        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ“Š ì¢…í•© ë¶„ì„ ê²°ê³¼")
        print("=" * 80)
        print(f"ì „ì²´ í‰ê·  ì ìˆ˜: {report['summary']['average_score']}/10")
        print(f"ì ìˆ˜ ë¶„í¬:")
        for grade, count in report['summary']['score_distribution'].items():
            print(f"  - {grade}: {count}ê°œ")

        if report['complexity_analysis']:
            print(f"\në³µì¡ë„ë³„ ì„±ëŠ¥:")
            for complexity, analysis in report['complexity_analysis'].items():
                print(f"  - {complexity}: {analysis['avg_score']:.1f}ì  (í‰ê·  {analysis['avg_time']:.1f}ì´ˆ)")

        print(f"\nì£¼ìš” ê°œì„  í¬ì¸íŠ¸:")
        for i, (issue, count) in enumerate(report['top_issues']['frequent_weaknesses'][:3], 1):
            print(f"  {i}. {issue} ({count}íšŒ)")

        print(f"\nìƒì„¸ ë³´ê³ ì„œ: {report_file}")

        return report

    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    asyncio.run(main())