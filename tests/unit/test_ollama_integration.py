#!/usr/bin/env python3
"""Ollama í†µí•© í…ŒìŠ¤íŠ¸"""
import asyncio
import sys
import time
sys.path.append('.')

async def test_ollama_adapter():
    """Ollama ì–´ëŒ‘í„° í…ŒìŠ¤íŠ¸"""
    print("ğŸ¦™ Ollama LLM ì–´ëŒ‘í„° í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    try:
        from langchain_ollama import OllamaLLM
        from api.config import settings

        # ì„¤ì • í™•ì¸
        print(f"ğŸ“‹ Ollama ì„¤ì •:")
        print(f"   ëª¨ë¸: {settings.ollama_model}")
        print(f"   ì„œë²„: {settings.get_ollama_base_url()}")

        # langchain_ollama ì§ì ‘ ì‚¬ìš©
        print(f"\nâš™ï¸  Ollama LLM ì´ˆê¸°í™” ì¤‘...")
        llm = OllamaLLM(
            model=settings.ollama_model,
            base_url=settings.get_ollama_base_url(),
            temperature=0.1,
            timeout=30
        )

        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸
        test_queries = [
            "SMR ê´€ë ¨ ìœ ë§ ì¢…ëª© ì°¾ê¸°",
            "ë°˜ë„ì²´ ì‚°ì—… íˆ¬ì ì „ë§"
        ]

        for i, query in enumerate(test_queries, 1):
            print(f"\n{i}ï¸âƒ£  í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: '{query}'")
            print("-" * 50)

            # ì§ì ‘ í‚¤ì›Œë“œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸
            print("ğŸ” ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ:")
            start_time = time.perf_counter()

            try:
                prompt = f"ë‹¤ìŒ ì§ˆë¬¸ì˜ í•µì‹¬ í‚¤ì›Œë“œ 5ê°œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”: '{query}'\ní‚¤ì›Œë“œ:"
                response = await llm.ainvoke(prompt)
                elapsed = (time.perf_counter() - start_time) * 1000

                print(f"   â±ï¸  ì‹¤í–‰ ì‹œê°„: {elapsed:.1f}ms")
                print(f"   ğŸ“ ì‘ë‹µ: {response.strip()}")

            except Exception as e:
                print(f"   âŒ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

    except ImportError as e:
        print(f"âŒ ì˜ì¡´ì„± ëˆ„ë½: {e}")
        print("ğŸ’¡ langchain-ollamaë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”:")
        print("   pip install langchain-ollama")
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

async def test_chat_service_integration():
    """ChatServiceì™€ì˜ í†µí•© í…ŒìŠ¤íŠ¸"""
    print(f"\n" + "=" * 60)
    print("ğŸ”— ChatService í†µí•© í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    try:
        from api.services.chat_service import ChatService

        service = ChatService()

        # Ollama LLMì´ ì´ˆê¸°í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸
        if service.ollama_llm:
            print("âœ… ChatServiceì— Ollama LLMì´ ì„±ê³µì ìœ¼ë¡œ í†µí•©ë¨")
            print(f"ğŸ“‹ ëª¨ë¸ ì •ë³´: {service.ollama_llm.model} @ {service.ollama_llm.base_url}")

            # í‚¤ì›Œë“œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸
            test_query = "SMR ì›ìë ¥ ì—ë„ˆì§€ íˆ¬ì"
            print(f"\nğŸ” í†µí•© í‚¤ì›Œë“œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸: '{test_query}'")

            start_time = time.perf_counter()
            keywords = await service._get_context_keywords(test_query)
            elapsed = (time.perf_counter() - start_time) * 1000

            print(f"   â±ï¸  ì‹¤í–‰ ì‹œê°„: {elapsed:.1f}ms")
            print(f"   ğŸ“ ì¶”ì¶œëœ í‚¤ì›Œë“œ: '{keywords}'")

        else:
            print("âš ï¸  ChatServiceì—ì„œ Ollama LLM ì´ˆê¸°í™” ì‹¤íŒ¨")

        # ì •ë¦¬
        await service.neo.close()

    except Exception as e:
        print(f"âŒ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

async def test_model_availability():
    """ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ì„± í…ŒìŠ¤íŠ¸"""
    print(f"\n" + "=" * 60)
    print("ğŸ“¡ ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ì„± í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    from api.config import settings

    # ê¸°ë³¸ ì—°ê²° í…ŒìŠ¤íŠ¸
    print(f"ğŸ”— Ollama ì„œë²„ ì—°ê²° í…ŒìŠ¤íŠ¸:")
    print(f"   ì„œë²„: {settings.get_ollama_base_url()}")
    print(f"   ëª¨ë¸: {settings.ollama_model}")

    try:
        import requests

        # Ollama ì„œë²„ ìƒíƒœ í™•ì¸
        response = requests.get(f"{settings.get_ollama_base_url()}/api/tags", timeout=5)

        if response.status_code == 200:
            models = response.json().get("models", [])
            print(f"   âœ… ì„œë²„ ì—°ê²° ì„±ê³µ")
            print(f"   ğŸ“¦ ì„¤ì¹˜ëœ ëª¨ë¸ ìˆ˜: {len(models)}ê°œ")

            # ì„¤ì¹˜ëœ ëª¨ë¸ ëª©ë¡
            if models:
                print(f"   ğŸ“‹ ì„¤ì¹˜ëœ ëª¨ë¸:")
                for model in models[:5]:  # ìƒìœ„ 5ê°œë§Œ í‘œì‹œ
                    name = model.get("name", "unknown")
                    size = model.get("size", 0)
                    size_gb = size / (1024**3) if size else 0
                    print(f"      - {name} ({size_gb:.1f}GB)")

            # ì„¤ì •ëœ ëª¨ë¸ì´ ìˆëŠ”ì§€ í™•ì¸
            model_names = [m.get("name", "") for m in models]
            if settings.ollama_model in model_names:
                print(f"   âœ… ì„¤ì •ëœ ëª¨ë¸ '{settings.ollama_model}' ì‚¬ìš© ê°€ëŠ¥")
            else:
                print(f"   âš ï¸  ì„¤ì •ëœ ëª¨ë¸ '{settings.ollama_model}' ì°¾ì„ ìˆ˜ ì—†ìŒ")
                print(f"   ğŸ’¡ ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ëª¨ë¸ì„ ì„¤ì¹˜í•˜ì„¸ìš”:")
                print(f"      ollama pull {settings.ollama_model}")
        else:
            print(f"   âŒ ì„œë²„ ì‘ë‹µ ì˜¤ë¥˜: {response.status_code}")

    except requests.exceptions.ConnectionError:
        print(f"   âŒ ì„œë²„ ì—°ê²° ì‹¤íŒ¨ - Ollamaê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”")
    except Exception as e:
        print(f"   âŒ ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    asyncio.run(test_model_availability())
    asyncio.run(test_ollama_adapter())
    asyncio.run(test_chat_service_integration())