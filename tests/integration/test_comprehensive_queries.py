#!/usr/bin/env python3
"""
í¬ê´„ì  ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ ì¢…í•© í…ŒìŠ¤íŠ¸
- ë‹¤ì–‘í•œ ìœ í˜•ì˜ ìƒˆë¡œìš´ ì§ˆë¬¸ í…ŒìŠ¤íŠ¸
- ì‘ë‹µ íƒ€ë‹¹ì„± ê²€ì¦
- í”„ë¡œì„¸ìŠ¤ ì˜¤ë²„í—¤ë“œ ë° ì˜¤ì‘ë™ ì ê²€
- ë¡±í…€ ë©”ëª¨ë¦¬ ë™ì‘ í™•ì¸
"""

import asyncio
import json
import time
from typing import Dict, Any, List
from datetime import datetime

# í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ì„¸íŠ¸ (ë‹¤ì–‘í•œ ì¹´í…Œê³ ë¦¬)
TEST_QUERIES = [
    {
        "id": 1,
        "query": "2ì°¨ì „ì§€ ê´€ë ¨ ìµœê·¼ 3ê°œì›”ê°„ ì£¼ìš” ê¸°ì—…ë“¤ì˜ ìˆ˜ì£¼ í˜„í™©ì€?",
        "category": "ì—ë„ˆì§€/ë°°í„°ë¦¬",
        "expected_keywords": ["2ì°¨ì „ì§€", "ë°°í„°ë¦¬", "ìˆ˜ì£¼", "LG", "ì‚¼ì„±SDI"],
        "expected_sources": ["opensearch", "neo4j"],
    },
    {
        "id": 2,
        "query": "AI ë°˜ë„ì²´ ì‹œì¥ì—ì„œ HBM ê¸°ìˆ  ê²½ìŸë ¥ì„ ê°€ì§„ ê¸°ì—…ì€ ì–´ë””ì¸ê°€?",
        "category": "ë°˜ë„ì²´/ê¸°ìˆ ",
        "expected_keywords": ["AI", "HBM", "ë°˜ë„ì²´", "SKí•˜ì´ë‹‰ìŠ¤", "ì‚¼ì„±ì „ì"],
        "expected_sources": ["opensearch", "neo4j"],
    },
    {
        "id": 3,
        "query": "ìµœê·¼ ì›ìë ¥ ë°œì „ ê´€ë ¨ ì •ì±… ë³€í™”ê°€ ì£¼ì‹ ì‹œì¥ì— ë¯¸ì¹œ ì˜í–¥ì€?",
        "category": "ì—ë„ˆì§€/ì •ì±…",
        "expected_keywords": ["ì›ìë ¥", "SMR", "ì •ì±…", "í•œêµ­ìˆ˜ë ¥ì›ìë ¥", "ë‘ì‚°ì—ë„ˆë¹Œë¦¬í‹°"],
        "expected_sources": ["opensearch", "neo4j"],
    },
    {
        "id": 4,
        "query": "ì „ê¸°ì°¨ ë°°í„°ë¦¬ í™”ì¬ ì´ìŠˆë¡œ ì˜í–¥ë°›ì€ ê¸°ì—…ë“¤ì˜ ëŒ€ì‘ ì „ëµì€?",
        "category": "ìë™ì°¨/ë¦¬ìŠ¤í¬",
        "expected_keywords": ["ì „ê¸°ì°¨", "ë°°í„°ë¦¬", "í™”ì¬", "ì•ˆì „", "ëŒ€ì‘"],
        "expected_sources": ["opensearch", "neo4j"],
    },
    {
        "id": 5,
        "query": "K-ë°©ì‚° ìˆ˜ì¶œ í™•ëŒ€ê°€ êµ­ë‚´ ë°©ìœ„ì‚°ì—…ì²´ ì‹¤ì ì— ë¯¸ì¹œ ì˜í–¥ì€?",
        "category": "ë°©ì‚°/ì‹¤ì ",
        "expected_keywords": ["ë°©ì‚°", "ìˆ˜ì¶œ", "í•œí™”", "LIGë„¥ìŠ¤ì›", "í˜„ëŒ€ë¡œí…œ"],
        "expected_sources": ["opensearch", "neo4j"],
    },
    {
        "id": 6,
        "query": "ë°˜ë„ì²´ ì¥ë¹„ êµ­ì‚°í™” ì¶”ì§„ í˜„í™©ê³¼ ê´€ë ¨ ìˆ˜í˜œ ê¸°ì—…ì€?",
        "category": "ë°˜ë„ì²´/ê³µê¸‰ë§",
        "expected_keywords": ["ë°˜ë„ì²´", "ì¥ë¹„", "êµ­ì‚°í™”", "ì›ìµIPS", "í…ŒìŠ¤"],
        "expected_sources": ["opensearch", "neo4j"],
    },
    {
        "id": 7,
        "query": "ìµœê·¼ ë©”ëª¨ë¦¬ ë°˜ë„ì²´ ê°€ê²© ë³€ë™ì´ ì£¼ìš” ê¸°ì—… ì‹¤ì ì— ë¯¸ì¹œ ì˜í–¥ ë¶„ì„",
        "category": "ë°˜ë„ì²´/ì‹¤ì ",
        "expected_keywords": ["ë©”ëª¨ë¦¬", "Dë¨", "ë‚¸ë“œ", "ê°€ê²©", "ì‹¤ì "],
        "expected_sources": ["opensearch", "neo4j"],
    },
    {
        "id": 8,
        "query": "ë°”ì´ì˜¤ ì‹ ì•½ ê°œë°œ ê´€ë ¨ ì„ìƒ ì„±ê³µ ì‚¬ë¡€ì™€ íˆ¬ì ìœ ë§ ê¸°ì—…ì€?",
        "category": "ë°”ì´ì˜¤/R&D",
        "expected_keywords": ["ë°”ì´ì˜¤", "ì‹ ì•½", "ì„ìƒ", "ì…€íŠ¸ë¦¬ì˜¨", "ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤"],
        "expected_sources": ["opensearch", "neo4j"],
    },
]


class ComprehensiveQueryTester:
    """í¬ê´„ì  ì§ˆì˜ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""

    def __init__(self, api_base_url: str = "http://localhost:8000"):
        self.api_base_url = api_base_url
        self.results = []
        self.cache_stats = []

    async def test_single_query(
        self, test_case: Dict[str, Any], run_number: int = 1
    ) -> Dict[str, Any]:
        """ë‹¨ì¼ ì§ˆë¬¸ í…ŒìŠ¤íŠ¸"""
        import aiohttp

        query_id = test_case["id"]
        query = test_case["query"]
        category = test_case["category"]

        print(f"\n{'='*80}")
        print(f"[í…ŒìŠ¤íŠ¸ {query_id}.{run_number}] {category}")
        print(f"ì§ˆë¬¸: {query}")
        print(f"{'='*80}")

        start_time = time.time()

        try:
            async with aiohttp.ClientSession() as session:
                # API í˜¸ì¶œ
                async with session.post(
                    f"{self.api_base_url}/chat",
                    json={"query": query},
                    timeout=aiohttp.ClientTimeout(total=30),
                ) as response:
                    if response.status != 200:
                        error_text = await response.text()
                        return {
                            "query_id": query_id,
                            "run_number": run_number,
                            "success": False,
                            "error": f"HTTP {response.status}: {error_text}",
                            "latency_ms": (time.time() - start_time) * 1000,
                        }

                    result = await response.json()

            latency_ms = (time.time() - start_time) * 1000

            # ì‘ë‹µ ë¶„ì„
            analysis = self._analyze_response(result, test_case, latency_ms, run_number)

            # ê²°ê³¼ ì¶œë ¥
            self._print_result(analysis)

            return analysis

        except asyncio.TimeoutError:
            return {
                "query_id": query_id,
                "run_number": run_number,
                "success": False,
                "error": "Timeout (30s)",
                "latency_ms": 30000,
            }
        except Exception as e:
            return {
                "query_id": query_id,
                "run_number": run_number,
                "success": False,
                "error": str(e),
                "latency_ms": (time.time() - start_time) * 1000,
            }

    def _analyze_response(
        self,
        result: Dict[str, Any],
        test_case: Dict[str, Any],
        latency_ms: float,
        run_number: int,
    ) -> Dict[str, Any]:
        """ì‘ë‹µ ë¶„ì„"""
        query_id = test_case["id"]
        query = test_case["query"]
        expected_keywords = test_case.get("expected_keywords", [])
        expected_sources = test_case.get("expected_sources", [])

        # ê¸°ë³¸ ì •ë³´ (answer ë˜ëŠ” markdown í•„ë“œ í™•ì¸)
        answer = result.get("answer") or result.get("markdown", "")
        sources = result.get("sources", [])
        meta = result.get("meta", {})

        # ë””ë²„ê¹…: meta ë‚´ìš© í™•ì¸
        if "graph_samples_shown" in meta:
            print(f"[DEBUG í…ŒìŠ¤íŠ¸] graph_samples_shown found: {meta['graph_samples_shown']}")
        else:
            print(f"[DEBUG í…ŒìŠ¤íŠ¸] graph_samples_shown NOT found, meta keys: {list(meta.keys())}")

        # graph_samplesëŠ” meta.graph_samples_shownì—ì„œ ê°€ì ¸ì˜´
        graph_samples_count = meta.get("graph_samples_shown", 0)
        graph_samples = [] if graph_samples_count == 0 else [{"count": graph_samples_count}]  # í˜¸í™˜ì„±

        # 1. í‚¤ì›Œë“œ ë§¤ì¹­ ê²€ì¦
        keyword_matches = []
        for keyword in expected_keywords:
            if keyword.lower() in answer.lower():
                keyword_matches.append(keyword)

        keyword_match_rate = (
            len(keyword_matches) / len(expected_keywords) if expected_keywords else 0
        )

        # 2. ë°ì´í„° ì†ŒìŠ¤ ê²€ì¦
        sources_used = []
        if sources:
            sources_used.append("opensearch")
        if graph_samples:
            sources_used.append("neo4j")

        source_coverage = len(set(sources_used) & set(expected_sources)) / len(
            expected_sources
        ) if expected_sources else 0

        # 3. ì‘ë‹µ í’ˆì§ˆ í‰ê°€
        quality_score = 0.0

        # ë‹µë³€ ê¸¸ì´ (ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸¸ì§€ ì•Šì€ì§€)
        answer_length = len(answer)
        if 100 <= answer_length <= 5000:
            quality_score += 0.2
        elif answer_length > 50:
            quality_score += 0.1

        # ì¶œì²˜ ê°œìˆ˜ (ì ì ˆí•œ ì¶œì²˜ê°€ ìˆëŠ”ì§€)
        if 1 <= len(sources) <= 10:
            quality_score += 0.2
        elif len(sources) > 0:
            quality_score += 0.1

        # ê·¸ë˜í”„ ë°ì´í„° í™œìš©
        if len(graph_samples) > 0:
            quality_score += 0.2

        # í‚¤ì›Œë“œ ë§¤ì¹­
        quality_score += keyword_match_rate * 0.2

        # ì†ŒìŠ¤ ì»¤ë²„ë¦¬ì§€
        quality_score += source_coverage * 0.2

        # 4. ì„±ëŠ¥ í‰ê°€
        performance_grade = "A" if latency_ms < 1500 else "B" if latency_ms < 3000 else "C"

        # 5. ìºì‹œ ì •ë³´ ì¶”ì¶œ
        cache_hit = meta.get("cache_hit", False)
        cache_info = meta.get("cache_info", {})

        # 6. ì˜¤ë¥˜ ê²€ì¦
        errors = []
        if not answer or answer == "":
            errors.append("ë¹ˆ ì‘ë‹µ")
        if latency_ms > 5000:
            errors.append(f"ë†’ì€ ì§€ì—°ì‹œê°„ ({latency_ms:.0f}ms)")
        if quality_score < 0.3:
            errors.append(f"ë‚®ì€ í’ˆì§ˆ ì ìˆ˜ ({quality_score:.2f})")

        return {
            "query_id": query_id,
            "run_number": run_number,
            "query": query,
            "category": test_case["category"],
            "success": True,
            "latency_ms": latency_ms,
            "performance_grade": performance_grade,
            "quality_score": quality_score,
            "keyword_match_rate": keyword_match_rate,
            "keywords_matched": keyword_matches,
            "source_coverage": source_coverage,
            "sources_used": sources_used,
            "answer_length": answer_length,
            "num_sources": len(sources),
            "num_graph_samples": len(graph_samples),
            "cache_hit": cache_hit,
            "cache_info": cache_info,
            "errors": errors,
            "answer_preview": answer[:200] + "..." if len(answer) > 200 else answer,
            "raw_meta": meta,
        }

    def _print_result(self, analysis: Dict[str, Any]):
        """ê²°ê³¼ ì¶œë ¥"""
        print(f"\n[ê²°ê³¼ ë¶„ì„]")
        print(f"  ì„±ê³µ: {'âœ“' if analysis['success'] else 'âœ—'}")
        print(f"  ì§€ì—°ì‹œê°„: {analysis['latency_ms']:.0f}ms ({analysis['performance_grade']})")
        print(f"  í’ˆì§ˆ ì ìˆ˜: {analysis['quality_score']:.2f}/1.0")
        print(f"  í‚¤ì›Œë“œ ë§¤ì¹­: {analysis['keyword_match_rate']:.1%} ({len(analysis['keywords_matched'])}/{len(analysis.get('expected_keywords', []))})")
        print(f"  ì†ŒìŠ¤ ì»¤ë²„ë¦¬ì§€: {analysis['source_coverage']:.1%}")
        print(f"  ì‚¬ìš©ëœ ì†ŒìŠ¤: {', '.join(analysis['sources_used'])}")
        print(f"  ì¶œì²˜ ê°œìˆ˜: {analysis['num_sources']}")
        print(f"  ê·¸ë˜í”„ ìƒ˜í”Œ: {analysis['num_graph_samples']}")
        print(f"  ìºì‹œ íˆíŠ¸: {'âœ“' if analysis['cache_hit'] else 'âœ—'}")

        if analysis.get("cache_info"):
            cache_info = analysis["cache_info"]
            print(f"    - íˆíŠ¸ìœ¨: {cache_info.get('hit_rate', 0):.1%}")
            print(f"    - ìºì‹œ í¬ê¸°: {cache_info.get('cache_size', 0)}")

        if analysis["errors"]:
            print(f"  âš ï¸  ê²½ê³ : {', '.join(analysis['errors'])}")

        print(f"\n[ë‹µë³€ ë¯¸ë¦¬ë³´ê¸°]")
        print(f"  {analysis['answer_preview']}")

    async def test_long_term_memory(self):
        """ë¡±í…€ ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸ - ê°™ì€ ì§ˆë¬¸ ë°˜ë³µ"""
        print(f"\n{'='*80}")
        print(f"[ë¡±í…€ ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸]")
        print(f"{'='*80}")

        # ì²« ë²ˆì§¸ ì§ˆë¬¸ (ìºì‹œ ì—†ìŒ)
        test_case = TEST_QUERIES[0]
        result_1 = await self.test_single_query(test_case, run_number=1)

        # ì ì‹œ ëŒ€ê¸°
        await asyncio.sleep(1)

        # ê°™ì€ ì§ˆë¬¸ ì¬ì‹œë„ (ìºì‹œ íˆíŠ¸ ì˜ˆìƒ)
        result_2 = await self.test_single_query(test_case, run_number=2)

        # ë¶„ì„
        print(f"\n[ë¡±í…€ ë©”ëª¨ë¦¬ ë¶„ì„]")
        print(f"  1ì°¨ ì‹œë„:")
        print(f"    - ì§€ì—°ì‹œê°„: {result_1.get('latency_ms', 0):.0f}ms")
        print(f"    - ìºì‹œ íˆíŠ¸: {result_1.get('cache_hit', False)}")

        print(f"  2ì°¨ ì‹œë„ (ê°™ì€ ì§ˆë¬¸):")
        print(f"    - ì§€ì—°ì‹œê°„: {result_2.get('latency_ms', 0):.0f}ms")
        print(f"    - ìºì‹œ íˆíŠ¸: {result_2.get('cache_hit', False)}")

        if result_2.get("cache_hit"):
            speedup = result_1.get("latency_ms", 1) / result_2.get("latency_ms", 1)
            print(f"  âœ“ ìºì‹œ ë™ì‘: ì†ë„ {speedup:.1f}x í–¥ìƒ")
        else:
            print(f"  âœ— ìºì‹œ ë¯¸ë™ì‘: ë¡±í…€ ë©”ëª¨ë¦¬ ì´ìŠˆ ê°€ëŠ¥")

        return {
            "first_run": result_1,
            "second_run": result_2,
            "cache_working": result_2.get("cache_hit", False),
        }

    async def run_all_tests(self):
        """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print(f"\n{'#'*80}")
        print(f"# í¬ê´„ì  ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ ì¢…í•© í…ŒìŠ¤íŠ¸")
        print(f"# ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"# ì´ í…ŒìŠ¤íŠ¸: {len(TEST_QUERIES)}ê°œ")
        print(f"{'#'*80}")

        # 1. ê°œë³„ ì§ˆë¬¸ í…ŒìŠ¤íŠ¸
        for test_case in TEST_QUERIES:
            result = await self.test_single_query(test_case)
            self.results.append(result)
            await asyncio.sleep(0.5)  # API ë¶€í•˜ ë°©ì§€

        # 2. ë¡±í…€ ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸
        memory_test = await self.test_long_term_memory()

        # 3. ì¢…í•© ë¶„ì„
        self._print_summary(memory_test)

        # 4. JSON ë³´ê³ ì„œ ì €ì¥
        self._save_report(memory_test)

    def _print_summary(self, memory_test: Dict[str, Any]):
        """ì¢…í•© ë¶„ì„ ì¶œë ¥"""
        print(f"\n{'='*80}")
        print(f"[ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸]")
        print(f"{'='*80}")

        # ì„±ê³µë¥ 
        successful = [r for r in self.results if r.get("success", False)]
        success_rate = len(successful) / len(self.results) if self.results else 0

        # í‰ê·  ì„±ëŠ¥
        avg_latency = (
            sum(r.get("latency_ms", 0) for r in successful) / len(successful)
            if successful
            else 0
        )
        avg_quality = (
            sum(r.get("quality_score", 0) for r in successful) / len(successful)
            if successful
            else 0
        )

        # ì„±ëŠ¥ ë“±ê¸‰ ë¶„í¬
        grade_dist = {}
        for r in successful:
            grade = r.get("performance_grade", "F")
            grade_dist[grade] = grade_dist.get(grade, 0) + 1

        # ì¹´í…Œê³ ë¦¬ë³„ í’ˆì§ˆ
        category_quality = {}
        for r in successful:
            cat = r.get("category", "Unknown")
            if cat not in category_quality:
                category_quality[cat] = []
            category_quality[cat].append(r.get("quality_score", 0))

        print(f"\n1. ì „ì²´ ì„±ëŠ¥")
        print(f"  - ì„±ê³µë¥ : {success_rate:.1%} ({len(successful)}/{len(self.results)})")
        print(f"  - í‰ê·  ì§€ì—°ì‹œê°„: {avg_latency:.0f}ms")
        print(f"  - í‰ê·  í’ˆì§ˆ ì ìˆ˜: {avg_quality:.2f}/1.0")
        print(f"  - ì„±ëŠ¥ ë“±ê¸‰ ë¶„í¬:")
        for grade in ["A", "B", "C"]:
            count = grade_dist.get(grade, 0)
            print(f"    â€¢ {grade}ë“±ê¸‰: {count}ê°œ ({count/len(successful)*100:.0f}%)")

        print(f"\n2. ì¹´í…Œê³ ë¦¬ë³„ í’ˆì§ˆ")
        for cat, scores in sorted(category_quality.items()):
            avg_score = sum(scores) / len(scores)
            print(f"  - {cat}: {avg_score:.2f}/1.0")

        print(f"\n3. ë°ì´í„° ì†ŒìŠ¤ í™œìš©")
        opensearch_used = len(
            [r for r in successful if "opensearch" in r.get("sources_used", [])]
        )
        neo4j_used = len(
            [r for r in successful if "neo4j" in r.get("sources_used", [])]
        )
        print(f"  - OpenSearch í™œìš©: {opensearch_used}/{len(successful)}")
        print(f"  - Neo4j í™œìš©: {neo4j_used}/{len(successful)}")

        print(f"\n4. ë¡±í…€ ë©”ëª¨ë¦¬")
        print(f"  - ìºì‹œ ë™ì‘: {'âœ“' if memory_test.get('cache_working') else 'âœ—'}")

        print(f"\n5. ë¬¸ì œì  ë° ê°œì„ ì‚¬í•­")
        errors_found = [r for r in self.results if r.get("errors")]
        if errors_found:
            print(f"  âš ï¸  {len(errors_found)}ê°œ ì§ˆë¬¸ì—ì„œ ë¬¸ì œ ë°œê²¬:")
            for r in errors_found[:3]:  # ìƒìœ„ 3ê°œë§Œ ì¶œë ¥
                print(f"    - Q{r['query_id']}: {', '.join(r['errors'])}")
        else:
            print(f"  âœ“ ì‹¬ê°í•œ ë¬¸ì œ ì—†ìŒ")

        # ê¶Œì¥ì‚¬í•­
        print(f"\n6. ê¶Œì¥ì‚¬í•­")
        if avg_latency > 2000:
            print(f"  â€¢ í‰ê·  ì§€ì—°ì‹œê°„ì´ ë†’ìŒ â†’ ìºì‹± ê°•í™” ë˜ëŠ” ì¸ë±ìŠ¤ ìµœì í™” í•„ìš”")
        if avg_quality < 0.6:
            print(f"  â€¢ í’ˆì§ˆ ì ìˆ˜ê°€ ë‚®ìŒ â†’ ê²€ìƒ‰ ì „ëµ ê°œì„  í•„ìš”")
        if not memory_test.get("cache_working"):
            print(f"  â€¢ ìºì‹œê°€ ë™ì‘í•˜ì§€ ì•ŠìŒ â†’ ìºì‹œ ì„¤ì • ì ê²€ í•„ìš”")

        # ìµœì¢… í‰ê°€
        print(f"\n{'='*80}")
        if success_rate >= 0.9 and avg_quality >= 0.7 and avg_latency < 2000:
            print(f"âœ“ ì‹œìŠ¤í…œ ìƒíƒœ: ìš°ìˆ˜ (ëª¨ë“  ì§€í‘œ ì–‘í˜¸)")
        elif success_rate >= 0.8 and avg_quality >= 0.5:
            print(f"â–³ ì‹œìŠ¤í…œ ìƒíƒœ: ì–‘í˜¸ (ì¼ë¶€ ê°œì„  í•„ìš”)")
        else:
            print(f"âœ— ì‹œìŠ¤í…œ ìƒíƒœ: ê°œì„  í•„ìš” (ì£¼ìš” ì§€í‘œ ë¯¸ë‹¬)")
        print(f"{'='*80}\n")

    def _save_report(self, memory_test: Dict[str, Any]):
        """JSON ë³´ê³ ì„œ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"test_report_{timestamp}.json"

        report = {
            "timestamp": datetime.now().isoformat(),
            "summary": {
                "total_tests": len(self.results),
                "successful": len([r for r in self.results if r.get("success")]),
                "avg_latency_ms": sum(
                    r.get("latency_ms", 0) for r in self.results if r.get("success")
                )
                / len([r for r in self.results if r.get("success")])
                if self.results
                else 0,
                "avg_quality": sum(
                    r.get("quality_score", 0)
                    for r in self.results
                    if r.get("success")
                )
                / len([r for r in self.results if r.get("success")])
                if self.results
                else 0,
            },
            "tests": self.results,
            "memory_test": memory_test,
        }

        with open(filename, "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        print(f"ğŸ“„ ìƒì„¸ ë³´ê³ ì„œ ì €ì¥: {filename}")


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    tester = ComprehensiveQueryTester()
    await tester.run_all_tests()


if __name__ == "__main__":
    asyncio.run(main())