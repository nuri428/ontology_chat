# src/ontology_chat/services/chat_service.py
from __future__ import annotations
from typing import Any, Dict, List, Optional, Tuple
import re
import time
import anyio
# from api.logging import setup_logging
# logger = setup_logging()

from api.config import settings
from api.adapters.mcp_opensearch import OpenSearchMCP
from api.adapters.mcp_neo4j import Neo4jMCP
from api.adapters.mcp_stock import StockMCP
from api.services.cypher_builder import build_label_aware_search_cypher
from api.services.formatters import summarize_graph_rows
from api.services.search_strategy import advanced_search_engine, SearchResult
from api.services.response_formatter import response_formatter
from api.services.cache_manager import cache_decorator, cache_manager
from api.services.error_handler import error_handler, with_retry, with_error_handling

# --- NEW: ê°„ë‹¨í•œ ë„ë©”ì¸/ë£©ë°± ì¶”ë¡  ---
def _infer_domain_and_lookback(query: str) -> tuple[str, int]:
    q = query.lower()
    domain = settings.neo4j_search_default_domain or ""
    lookback = settings.neo4j_search_lookback_days

    # ì§ˆì˜ì— 'ìµœê·¼', 'ìš”ì¦˜', 'ìµœê·¼ 3ê°œì›”' ë¥˜ê°€ ìˆìœ¼ë©´ lookback ê°€ë³€ ì ìš©(ê°„ë‹¨ ê·œì¹™)
    # ì˜ˆ) "ìµœê·¼ 90ì¼", "ìµœê·¼ 6ê°œì›”"
    m = re.search(r"ìµœê·¼\s*(\d+)\s*(ì¼|ê°œì›”)", q)
    if m:
        val = int(m.group(1))
        unit = m.group(2)
        if unit == "ì¼":
            lookback = max(1, min(365*2, val))
        else:  # ê°œì›”
            lookback = max(7, min(365*2, val * 30))

    # ìƒì¥ì‚¬/íˆ¬ì ê´€ë ¨ í‚¤ì›Œë“œ íŒíŠ¸ (ìƒˆ ìŠ¤í‚¤ë§ˆ ê¸°ë°˜)
    if any(tok in q for tok in ["ìƒì¥ì‚¬", "íˆ¬ì", "ì‹¤ì ", "ì¬ë¬´", "ë§¤ì¶œ", "ì˜ì—…ì´ìµ"]):
        domain = (domain + " ìƒì¥ì‚¬ íˆ¬ì ì‹¤ì  ì¬ë¬´").strip()

    # íšŒì‚¬ëª… íŒíŠ¸: ëŒ€í‘œ ìƒì¥ì‚¬ë“¤ (ë™ì  í™•ì¥ ê°€ëŠ¥)
    company_hints = {
        ("ì‚¼ì„±ì „ì", "005930"): "ì‚¼ì„±ì „ì ë°˜ë„ì²´ ì „ì",
        ("í˜„ëŒ€ì°¨", "005380"): "í˜„ëŒ€ì°¨ ìë™ì°¨",
        ("LG", "LGì „ì"): "LG ì „ì ê°€ì „",
        ("SK", "SKí•˜ì´ë‹‰ìŠ¤"): "SK ë°˜ë„ì²´ ë©”ëª¨ë¦¬"
    }

    for keywords, hint in company_hints.items():
        if any(keyword in q for keyword in keywords):
            domain = (domain + " " + hint).strip()
            break

    # ì¤‘ë³µ ê³µë°± ì •ë¦¬
    domain = " ".join(domain.split())
    return domain, lookback


# @cache_decorator.cached("keyword_extraction", ttl=3600.0)  # ìºì‹± ë¹„í™œì„±í™”
def _extract_keywords_for_search(query: str) -> List[str]:
    """ê°œì„ ëœ í‚¤ì›Œë“œ ì¶”ì¶œ ë¡œì§ - ë™ì  í™•ì¥, ê°€ì¤‘ì¹˜ ê¸°ë°˜, í˜•íƒœì†Œ ë¶„ì„ (ì»¨í…ìŠ¤íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ê°•í™”)"""
    from api.config.keyword_mappings import get_all_keyword_mappings
    from api.utils.text_analyzer import enhance_query_with_morphology, suggest_related_terms
    
    q = query.lower()
    keyword_mappings = get_all_keyword_mappings()
    
    # ê°€ì¤‘ì¹˜ê°€ ìˆëŠ” í‚¤ì›Œë“œ ì €ì¥ì†Œ
    weighted_keywords = []
    
    # 0. í˜•íƒœì†Œ ë¶„ì„ì„ í†µí•œ ì¿ ë¦¬ ê°•í™”
    morphology_result = enhance_query_with_morphology(query)
    high_importance_words = morphology_result["high_importance_keywords"]
    companies = morphology_result["companies"]
    tech_terms = morphology_result["tech_terms"]
    finance_terms = morphology_result["finance_terms"]
    
    # í˜•íƒœì†Œ ë¶„ì„ ê²°ê³¼ë¡œ ì¶”ê°€ ê°€ì¤‘ì¹˜ ë¶€ì—¬
    for word in high_importance_words:
        weighted_keywords.append((word, 2.0))
    
    for word in companies:
        weighted_keywords.append((word, 2.5))
        # ì—°ê´€ ìš©ì–´ ì¶”ê°€
        related = suggest_related_terms(word)
        for rel_word in related[:3]:  # ìƒìœ„ 3ê°œë§Œ
            weighted_keywords.append((rel_word, 1.8))
    
    for word in tech_terms:
        weighted_keywords.append((word, 2.2))
    
    for word in finance_terms:
        weighted_keywords.append((word, 2.3))
    
    # 1. ë„ë©”ì¸ë³„ í‚¤ì›Œë“œ ì¶”ì¶œ
    for domain_name, domain_data in keyword_mappings["domain"].items():
        for trigger in domain_data["triggers"]:
            if trigger in q:
                # í™•ì¥ í‚¤ì›Œë“œ ì¶”ê°€ (ê°€ì¤‘ì¹˜ ìˆœìœ¼ë¡œ ì •ë ¬)
                for kw in sorted(domain_data["expansions"], key=lambda x: (x.priority, -x.weight)):
                    weighted_keywords.append((kw.keyword, kw.weight))
                
                # ìœ ì‚¬ì–´ ì¶”ê°€
                for base_word, synonyms in domain_data.get("synonyms", {}).items():
                    if base_word in q:
                        for syn in synonyms:
                            weighted_keywords.append((syn, 1.2))  # ìœ ì‚¬ì–´ëŠ” ê¸°ë³¸ ê°€ì¤‘ì¹˜
                break
    
    # 2. ì‚°ì—…ë³„ í‚¤ì›Œë“œ ì¶”ì¶œ (ë™ì  í™•ì¥ ê°€ëŠ¥)
    for industry_name, keywords in keyword_mappings["industry"].items():
        # ì‚°ì—…ë³„ íŠ¸ë¦¬ê±°ë¥¼ ì„¤ì •ì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ê¸°ë³¸ê°’ ì‚¬ìš©
        industry_triggers = {
            "technology": ["IT", "ì†Œí”„íŠ¸ì›¨ì–´", "ê¸°ìˆ ", "AI", "ì¸ê³µì§€ëŠ¥"],
            "automotive": ["ìë™ì°¨", "ì „ê¸°ì°¨", "ë°°í„°ë¦¬", "ëª¨ë¹Œë¦¬í‹°"],
            "semiconductor": ["ë°˜ë„ì²´", "ì¹©", "íŒŒìš´ë“œë¦¬", "ë©”ëª¨ë¦¬"],
            "energy": ["ì—ë„ˆì§€", "ì‹ ì¬ìƒ", "íƒœì–‘ê´‘", "í’ë ¥", "ì›ì „", "ì›ìë ¥"],
            "bio": ["ë°”ì´ì˜¤", "ì œì•½", "í—¬ìŠ¤ì¼€ì–´", "ì˜ë£Œ"]
        }.get(industry_name, [])
        
        if any(trigger in q for trigger in industry_triggers):
            for kw in keywords:
                weighted_keywords.append((kw.keyword, kw.weight))
    
    # 3. íšŒì‚¬ë³„ í‚¤ì›Œë“œ ì¶”ì¶œ
    for company_name, company_data in keyword_mappings["company"].items():
        for trigger in company_data["triggers"]:
            if trigger in q:
                for kw in company_data["expansions"]:
                    weighted_keywords.append((kw.keyword, kw.weight))
                break
    
    # 4. ì‹œê°„ ê´€ë ¨ í‚¤ì›Œë“œ ì¶”ì¶œ
    for time_type, time_data in keyword_mappings["time"].items():
        for trigger in time_data["triggers"]:
            if trigger in q:
                for kw in time_data["expansions"]:
                    weighted_keywords.append((kw.keyword, kw.weight))
                break
    
    # 5. ì§€ì—­ë³„ í‚¤ì›Œë“œ ì¶”ì¶œ
    for region_name, region_data in keyword_mappings["region"].items():
        for trigger in region_data["triggers"]:
            if trigger in q:
                for kw in region_data["expansions"]:
                    weighted_keywords.append((kw.keyword, kw.weight))
                break
    
    # 6. ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì •ë ¬ ë° ì¤‘ë³µ ì œê±°
    keyword_weights = {}
    for keyword, weight in weighted_keywords:
        if keyword not in keyword_weights:
            keyword_weights[keyword] = weight
        else:
            # ì¤‘ë³µ í‚¤ì›Œë“œëŠ” ìµœëŒ€ ê°€ì¤‘ì¹˜ ì‚¬ìš©
            keyword_weights[keyword] = max(keyword_weights[keyword], weight)
    
    # ê°€ì¤‘ì¹˜ ìˆœìœ¼ë¡œ ì •ë ¬
    sorted_keywords = sorted(keyword_weights.items(), key=lambda x: -x[1])
    
    # 7. í‚¤ì›Œë“œê°€ ë¶€ì¡±í•˜ë©´ ì›ë³¸ ì§ˆë¬¸ì—ì„œ ì¶”ê°€ ì¶”ì¶œ
    if len(sorted_keywords) < 5:
        stopwords = keyword_mappings["stopwords"]
        # í˜•íƒ„ì†Œ ë¶„ì„ ê²°ê³¼ë¥¼ í™œìš©í•œ ì¶”ê°€ í‚¤ì›Œë“œ
        key_phrases = morphology_result["key_phrases"]
        for phrase, importance in key_phrases:
            if phrase not in keyword_weights and importance > 1.0:
                sorted_keywords.append((phrase, importance * 0.8))  # ì•½ê°„ ë‚®ì€ ê°€ì¤‘ì¹˜
        
        # ì—¬ì „íˆ ë¶€ì¡±í•˜ë©´ ê¸°ë³¸ ì²˜ë¦¬
        if len(sorted_keywords) < 5:
            words = [w for w in q.split() if len(w) > 1 and w not in stopwords]
            for word in words:
                if word not in keyword_weights:
                    sorted_keywords.append((word, 0.5))
    
    # 8. ìµœì¢… í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (ìƒìœ„ 15ê°œ)
    final_keywords = [kw[0] for kw in sorted_keywords[:15]]
    
    # ë¡œê·¸ë¡œ ë¶„ì„ ê²°ê³¼ ì¶”ê°€ (ê°œë°œìš©)
    # logger.info(f"í˜•íƒœì†Œ ë¶„ì„ ê²°ê³¼: ì¤‘ìš” í‚¤ì›Œë“œ={high_importance_words}, íšŒì‚¬={companies}")
    
    return final_keywords


def _detect_symbol(text: str) -> Optional[str]:
    m = re.search(r"\b\d{6}\.(KS|KQ)\b", text)
    if m:
        return m.group(0)
    return None


def _format_sources(hits: List[Dict[str, Any]], limit: int = 5) -> List[Dict[str, Any]]:
    out: List[Dict[str, Any]] = []
    for h in hits[:limit]:
        src = h.get("_source", {}) or {}
        meta = src.get("metadata") or {}
        
        # ì œëª© ì¶”ì¶œ (ì—¬ëŸ¬ í•„ë“œì—ì„œ ì‹œë„)
        title = (
            src.get("title") or 
            meta.get("title") or 
            src.get("headline") or 
            "(no title)"
        )
        
        # URL ì¶”ì¶œ
        url = (
            src.get("url") or 
            meta.get("url") or 
            src.get("link") or 
            src.get("article_url")
        )
        
        # ë‚ ì§œ ì¶”ì¶œ (ì—¬ëŸ¬ í•„ë“œì—ì„œ ì‹œë„)
        date = (
            src.get("created_datetime") or
            src.get("created_date") or
            src.get("published_at") or
            src.get("publish_date") or
            meta.get("created_datetime") or
            meta.get("created_date") or
            meta.get("published_at")
        )
        
        # ë¯¸ë””ì–´ ì •ë³´ ì¶”ì¶œ
        media = src.get("media") or meta.get("media") or src.get("source") or "Unknown"
        
        out.append(
            {
                "id": h.get("_id"),
                "title": title,
                "url": url,
                "date": date,
                "media": media,
                "score": h.get("_score"),
                "index": h.get("_index"),
            }
        )
    return out


class ChatService:
    def __init__(self):
        self.os = OpenSearchMCP()
        self.neo = Neo4jMCP()
        self.st = StockMCP()
        # OpenSearchTool ì¸ìŠ¤í„´ìŠ¤ ì¶”ê°€ (ì„ë² ë”© ê¸°ëŠ¥ ì‚¬ìš©)
        from api.mcp.tools import OpenSearchTool
        self.os_tool = OpenSearchTool(self.os)

    async def _get_context_keywords(self, query: str) -> str:
        """LLMì„ í†µí•œ ë™ì  í‚¤ì›Œë“œ ì¶”ì¶œ ë° í™•ì¥"""
        try:
            # LLMì„ í†µí•œ ì¿¼ë¦¬ ë¶„ì„
            analysis_result = await self._analyze_query_with_llm(query)

            # LLM ë¶„ì„ ê²°ê³¼ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
            if analysis_result and "keywords" in analysis_result:
                keywords = analysis_result["keywords"]
                # ìµœëŒ€ 8ê°œ í‚¤ì›Œë“œë¡œ ì œí•œ
                return " ".join(keywords[:8])
            else:
                # LLM ë¶„ì„ ì‹¤íŒ¨ì‹œ í´ë°± - ê¸°ë³¸ í‚¤ì›Œë“œ ì¶”ì¶œ
                return self._fallback_keyword_extraction(query)

        except Exception as e:
            print(f"[WARNING] LLM í‚¤ì›Œë“œ ë¶„ì„ ì‹¤íŒ¨, í´ë°± ì‚¬ìš©: {e}")
            return self._fallback_keyword_extraction(query)

    async def _analyze_query_with_llm(self, query: str) -> dict:
        """LLMì„ í†µí•œ ì¿¼ë¦¬ ë¶„ì„"""
        from langchain_ollama import ChatOllama

        # Ollama LLM ì„¤ì • (ë˜ëŠ” OpenAI ì‚¬ìš© ê°€ëŠ¥)
        llm = ChatOllama(
            model=settings.ollama_model,  # ë˜ëŠ” ë‹¤ë¥¸ ëª¨ë¸
            temperature=0.1,
            base_url=settings.get_ollama_base_url()
        )

        prompt = f"""
ë‹¤ìŒ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ê²€ìƒ‰ì— ìµœì í™”ëœ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
---ì£¼ì˜ì‚¬í•­---
ë„ˆë¬´ ì¼ë°˜ì ì¸ ë‹¨ì–´ë¥¼ ì§€ì–‘í•©ë‹ˆë‹¤

ì§ˆë¬¸: "{query}"

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "main_topic": "ì£¼ìš” ì£¼ì œ (ì˜ˆ: SMR, ë°˜ë„ì²´, 2ì°¨ì „ì§€ ë“±)",
    "industry": "ì‚°ì—… ë¶„ì•¼ (ì˜ˆ: ì›ìë ¥, IT, ìë™ì°¨ ë“±)",
    "keywords": ["ê²€ìƒ‰ìš© í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (5-8ê°œ)"],
    "companies": ["ê´€ë ¨ íšŒì‚¬ëª…ë“¤ (ìˆë‹¤ë©´)"],
    "intent": "ì§ˆë¬¸ ì˜ë„ (ì˜ˆ: íˆ¬ìì¢…ëª©ì°¾ê¸°, ë‰´ìŠ¤ê²€ìƒ‰, ê¸°ìˆ ë™í–¥ ë“±)"
}}

ì¤‘ìš”í•œ ì :
1. í•œêµ­ ìƒì¥ì‚¬ ì¤‘ì‹¬ìœ¼ë¡œ ìƒê°í•˜ì„¸ìš”
2. ê²€ìƒ‰ì— íš¨ê³¼ì ì¸ í‚¤ì›Œë“œë¥¼ ì„ íƒí•˜ì„¸ìš”
3. ì•½ì–´ì™€ ì „ì²´ ìš©ì–´ë¥¼ ëª¨ë‘ í¬í•¨í•˜ì„¸ìš” (ì˜ˆ: SMR, ì†Œí˜•ëª¨ë“ˆì›ìë¡œ)
4. ê´€ë ¨ ì‚°ì—… ì „ë°˜ì˜ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ì„¸ìš”
"""

        try:
            response = await llm.ainvoke(prompt)
            content = response.content.strip()

            # JSON íŒŒì‹±
            import json
            import re

            # JSON ë¸”ë¡ ì¶”ì¶œ
            json_match = re.search(r'\{[^}]+\}', content, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                return json.loads(json_str)
            else:
                print(f"[WARNING] LLM ì‘ë‹µì—ì„œ JSONì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {content}")
                return None

        except Exception as e:
            print(f"[ERROR] LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return None

    def _fallback_keyword_extraction(self, query: str) -> str:
        """LLM ì‹¤íŒ¨ì‹œ í´ë°± í‚¤ì›Œë“œ ì¶”ì¶œ"""
        import re

        # ê¸°ë³¸ í† í°í™”
        words = re.findall(r'\b\w+\b', query)

        # ë¶ˆìš©ì–´ ì œê±°
        stopwords = ["ì„", "ë¥¼", "ì´", "ê°€", "ì€", "ëŠ”", "ì˜", "ì—", "ì—ì„œ", "ìœ¼ë¡œ", "ë¡œ", "ì™€", "ê³¼", "ê´€ë ¨", "ëŒ€í•œ"]
        keywords = [w for w in words if len(w) > 1 and w not in stopwords]

        # ìµœëŒ€ 5ê°œë¡œ ì œí•œ
        return " ".join(keywords[:5])

    async def _graph(self, query: str) -> tuple[list[dict], dict | None]:
        rows, ms, err = await self._query_graph(query, limit=30)
        summary = summarize_graph_rows(rows, max_each=5) if rows else None
        return rows, summary

    async def _search_news_advanced(
        self, 
        original_query: str, 
        keywords: List[str], 
        size: int = 5
    ) -> Tuple[SearchResult, float, Optional[str]]:
        """ê³ ê¸‰ ë‹¤ë‹¨ê³„ ë‰´ìŠ¤ ê²€ìƒ‰"""
        t0 = time.perf_counter()
        
        # 1. ë„ë©”ì¸ ê°ì§€ ë° ì—”í‹°í‹° ì¶”ì¶œ
        domains = advanced_search_engine.detect_query_domain(original_query)
        entities = advanced_search_engine.extract_entities(original_query)
        
        print(f"[INFO] ê²€ìƒ‰ ë„ë©”ì¸: {domains}, ì¶”ì¶œ ì—”í‹°í‹°: {entities}")
        
        # 2. ê²€ìƒ‰ ì „ëµ ìƒì„±
        search_strategies = advanced_search_engine.build_enhanced_queries(
            original_query, keywords, domains, entities
        )
        
        best_result = None
        best_quality = 0.0
        
        # 3. ì „ëµë³„ ìˆœì°¨ ê²€ìƒ‰
        for strategy in search_strategies[:4]:  # ìƒìœ„ 4ê°œ ì „ëµë§Œ ì‹œë„
            try:
                print(f"[INFO] ê²€ìƒ‰ ì „ëµ ì‹œë„: {strategy.name} - {strategy.query}")
                
                hits, ms, err = await self._search_news(strategy.query, size)
                
                if hits:
                    # í’ˆì§ˆ í‰ê°€
                    quality = advanced_search_engine.evaluate_search_quality(
                        hits, original_query, strategy
                    )
                    
                    print(f"[INFO] ì „ëµ {strategy.name} í’ˆì§ˆ: {quality:.2f}, ê²°ê³¼: {len(hits)}ê±´")
                    
                    if quality > best_quality:
                        best_quality = quality
                        best_result = SearchResult(
                            hits=hits,
                            query_used=strategy.query,
                            strategy=strategy.name,
                            confidence=quality,
                            latency_ms=ms,
                            total_found=len(hits)
                        )
                    
                    # í’ˆì§ˆì´ ì¶©ë¶„íˆ ë†’ìœ¼ë©´ ì¡°ê¸° ì¢…ë£Œ
                    if quality > 0.8:
                        break
                        
            except Exception as e:
                print(f"[WARNING] ê²€ìƒ‰ ì „ëµ {strategy.name} ì‹¤íŒ¨: {e}")
                continue
        
        # 4. ê²°ê³¼ ë°˜í™˜
        total_time = (time.perf_counter() - t0) * 1000.0
        
        if best_result:
            print(f"[INFO] ìµœì¢… ì„ íƒ: {best_result.strategy} (í’ˆì§ˆ: {best_result.confidence:.2f})")
            return best_result, total_time, None
        else:
            # ì™„ì „ í´ë°±: ì›ë³¸ ì¿¼ë¦¬ë¡œ ì¬ì‹œë„
            print("[WARNING] ëª¨ë“  ê³ ê¸‰ ê²€ìƒ‰ ì‹¤íŒ¨, ì›ë³¸ ì¿¼ë¦¬ë¡œ í´ë°±")
            hits, ms, err = await self._search_news(original_query, size)
            
            fallback_result = SearchResult(
                hits=hits or [],
                query_used=original_query,
                strategy="fallback_original",
                confidence=0.3,
                latency_ms=ms,
                total_found=len(hits) if hits else 0
            )
            
            return fallback_result, total_time, err

    async def _extract_core_keywords(self, query: str) -> List[str]:
        """LLM ê¸°ë°˜ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        try:
            # LLMì„ í†µí•œ í‚¤ì›Œë“œ ë¶„ì„
            keywords_str = await self._get_context_keywords(query)
            keywords_list = keywords_str.split()

            # ì¤‘ë³µ ì œê±°í•˜ê³  ìµœëŒ€ 5ê°œ ë°˜í™˜
            unique_keywords = []
            seen = set()
            for keyword in keywords_list:
                if keyword.lower() not in seen:
                    unique_keywords.append(keyword)
                    seen.add(keyword.lower())

            return unique_keywords[:5]

        except Exception as e:
            print(f"[WARNING] í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨, ê¸°ë³¸ ì²˜ë¦¬: {e}")
            # í´ë°±: ê¸°ë³¸ í† í°í™”
            import re
            words = re.findall(r'\b\w+\b', query)
            stopwords = ["ë¥¼", "ì„", "ì´", "ê°€", "ì˜", "ì—", "ê´€ë ¨", "ìœ ë§", "ì¢…ëª©"]
            return [w for w in words if len(w) > 1 and w not in stopwords][:5]

    async def _search_news_with_ontology(self, query: str, size: int = 5) -> Tuple[List[Dict[str, Any]], float, Optional[str]]:
        """ì˜¨í†¨ë¡œì§€ ê°•í™” ë‰´ìŠ¤ ê²€ìƒ‰ (ë‰´ìŠ¤ + ê·¸ë˜í”„ ë°ì´í„° í†µí•©)"""
        t0 = time.perf_counter()

        try:
            # 1. í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
            core_keywords = await self._extract_core_keywords(query)
            search_text = " ".join(core_keywords) if core_keywords else query

            print(f"[DEBUG] ì›ë³¸: '{query}' â†’ í•µì‹¬ í‚¤ì›Œë“œ: {core_keywords}")

            # 2. ì˜¨í†¨ë¡œì§€ì—ì„œ ê´€ë ¨ ì—”í‹°í‹° í™•ì¥
            ontology_entities = await self._get_ontology_expansion(core_keywords)

            # 3. í™•ì¥ëœ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰
            expanded_keywords = core_keywords + ontology_entities
            expanded_search_text = " ".join(expanded_keywords[:10])  # ìµœëŒ€ 10ê°œë¡œ ì œí•œ

            print(f"[DEBUG] ì˜¨í†¨ë¡œì§€ í™•ì¥: {ontology_entities}")

            # 4. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤í–‰
            news_hits, search_time, search_error = await self._execute_hybrid_search(
                original_query=query,
                search_text=expanded_search_text,
                core_keywords=expanded_keywords,
                size=size
            )

            # 5. ì˜¨í†¨ë¡œì§€ ê´€ë ¨ì„± ì ìˆ˜ë¡œ ì¬ì •ë ¬
            if news_hits and ontology_entities:
                news_hits = await self._rerank_with_ontology_relevance(news_hits, ontology_entities)

            return news_hits, (time.perf_counter() - t0) * 1000.0, search_error

        except Exception as e:
            print(f"[ERROR] ì˜¨í†¨ë¡œì§€ ê°•í™” ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return [], (time.perf_counter() - t0) * 1000.0, str(e)

    async def _get_ontology_expansion(self, keywords: List[str]) -> List[str]:
        """ì˜¨í†¨ë¡œì§€ ê·¸ë˜í”„ì—ì„œ ê´€ë ¨ ì—”í‹°í‹° í™•ì¥"""
        try:
            expansion_entities = []

            for keyword in keywords[:3]:  # ìƒìœ„ 3ê°œ í‚¤ì›Œë“œë§Œ ì²˜ë¦¬
                # ê·¸ë˜í”„ì—ì„œ ê´€ë ¨ ì—”í‹°í‹° ê²€ìƒ‰
                graph_rows, _ = await self._graph(keyword)

                for row in graph_rows[:5]:  # ê° í‚¤ì›Œë“œë‹¹ ìµœëŒ€ 5ê°œ
                    node = row.get("n", {})
                    if isinstance(node, dict):
                        # íšŒì‚¬ëª… ì¶”ì¶œ
                        if "name" in node:
                            company_name = node["name"]
                            if company_name and company_name not in expansion_entities:
                                expansion_entities.append(company_name)

                        # ì œí’ˆ/ê¸°ìˆ ëª… ì¶”ì¶œ
                        if "title" in node:
                            product_name = node["title"]
                            if product_name and len(product_name) > 2 and product_name not in expansion_entities:
                                expansion_entities.append(product_name)

            print(f"[DEBUG] ì˜¨í†¨ë¡œì§€ í™•ì¥ ì—”í‹°í‹°: {expansion_entities[:8]}")
            return expansion_entities[:8]  # ìµœëŒ€ 8ê°œ

        except Exception as e:
            print(f"[WARNING] ì˜¨í†¨ë¡œì§€ í™•ì¥ ì‹¤íŒ¨: {e}")
            return []

    async def _execute_hybrid_search(self, original_query: str, search_text: str, core_keywords: List[str], size: int) -> Tuple[List[Dict[str, Any]], float, Optional[str]]:
        """í™•ì¥ ê°€ëŠ¥í•œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤í–‰"""
        try:
            os_index = settings.news_embedding_index

            # ê²€ìƒ‰ ì „ëµ êµ¬ì„± í´ë˜ìŠ¤ (í–¥í›„ ë¶„ë¦¬ ê°€ëŠ¥)
            search_config = self._build_search_config(original_query, search_text, core_keywords)

            # ì ì‘ì  ì¿¼ë¦¬ êµ¬ì„±
            bool_clauses = []

            # 1. ë‹¤ë‹¨ê³„ ë§¤ì¹­ ì „ëµ
            for strategy in search_config['matching_strategies']:
                if strategy['enabled']:
                    clause = self._build_matching_clause(
                        query=strategy['query'],
                        fields=strategy['fields'],
                        type=strategy['type'],
                        options=strategy.get('options', {})
                    )
                    if clause:
                        bool_clauses.append(clause)

            # 2. ë™ì  í•„í„°ë§
            filters = self._build_dynamic_filters(original_query, core_keywords)

            # 3. ë²¡í„° ê²€ìƒ‰ êµ¬ì„±
            vector_config = await self._build_vector_config(search_text, size)

            # í•˜ì´ë¸Œë¦¬ë“œ ì¿¼ë¦¬ êµ¬ì„±
            body = {
                "query": {
                    "hybrid": {
                        "queries": [
                            {
                                "bool": {
                                    "should": bool_clauses,
                                    "must": filters.get('must', []),
                                    "must_not": filters.get('must_not', []),
                                    "filter": filters.get('filter', []),
                                    "minimum_should_match": 1
                                }
                            },
                            vector_config
                        ]
                    }
                },
                "sort": self._build_sort_strategy(original_query),
                "size": size * 2,  # ë” ë§ì€ í›„ë³´ í™•ë³´
                "_source": self._get_source_fields(),
                "highlight": self._build_highlight_config()
            }

            # ê²€ìƒ‰ ì‹¤í–‰
            result = await self.os.search(index=os_index, query=body, size=size * 2)

            if result and "hits" in result and "hits" in result["hits"]:
                hits = result["hits"]["hits"]
                # í•˜ì´ë¼ì´íŠ¸ ì •ë³´ í¬í•¨í•˜ì—¬ í¬ë§·íŒ…
                formatted_hits = self._format_with_highlights(hits, size * 2)
                return formatted_hits, 0.0, None
            else:
                return [], 0.0, "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤"

        except Exception as e:
            # print(f"[ERROR] í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            from api.logging import setup_logging
            logger = setup_logging()
            logger.error(f"[ERROR] í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            return [], 0.0, str(e)

    def _build_search_config(self, original_query: str, search_text: str, core_keywords: List[str]) -> dict:
        """ê²€ìƒ‰ êµ¬ì„± ë™ì  ìƒì„± (í™•ì¥ ê°€ëŠ¥)"""
        config = {
            'matching_strategies': []
        }

        # í•µì‹¬ í‚¤ì›Œë“œ ì •í™• ë§¤ì¹­
        if core_keywords:
            config['matching_strategies'].append({
                'enabled': True,
                'query': search_text,
                'fields': ["title^4", "content^2", "metadata.title^4", "metadata.content^2"],
                'type': 'multi_match',
                'options': {
                    'type': 'best_fields',
                    'minimum_should_match': '60%',
                    'boost': 2.0
                }
            })

            # ì²« ë²ˆì§¸ í‚¤ì›Œë“œ ê°•ì¡° (ì™€ì¼ë“œì¹´ë“œ)
            if len(core_keywords) > 0:
                config['matching_strategies'].append({
                    'enabled': True,
                    'query': core_keywords[0],
                    'fields': ["title"],
                    'type': 'wildcard',
                    'options': {
                        'boost': 1.5
                    }
                })

        # í´ë°± ì „ëµ (í¼ì§€ ë§¤ì¹­)
        config['matching_strategies'].append({
            'enabled': True,
            'query': original_query,
            'fields': ["title^2", "content"],
            'type': 'multi_match',
            'options': {
                'type': 'best_fields',
                'fuzziness': 'AUTO',
                'boost': 1.0
            }
        })

        # êµ¬ë¬¸ ë§¤ì¹­ ì¶”ê°€ (ì •í™•í•œ êµ¬ë¬¸ ê²€ìƒ‰)
        if len(original_query.split()) > 1:
            config['matching_strategies'].append({
                'enabled': True,
                'query': original_query,
                'fields': ["title^3", "content"],
                'type': 'match_phrase',
                'options': {
                    'slop': 2,  # ë‹¨ì–´ ê°„ ê±°ë¦¬ í—ˆìš©
                    'boost': 1.5
                }
            })

        return config

    def _build_matching_clause(self, query: str, fields: List[str], type: str, options: dict) -> dict:
        """ë§¤ì¹­ ì ˆ ë™ì  ìƒì„±"""
        if type == 'multi_match':
            return {
                "multi_match": {
                    "query": query,
                    "fields": fields,
                    **options
                }
            }
        elif type == 'wildcard':
            field = fields[0] if fields else "title"
            return {
                "wildcard": {
                    field: {
                        "value": f"*{query}*",
                        "boost": options.get('boost', 1.0)
                    }
                }
            }
        elif type == 'match_phrase':
            return {
                "multi_match": {
                    "query": query,
                    "fields": fields,
                    "type": "phrase",
                    "slop": options.get('slop', 0),
                    "boost": options.get('boost', 1.0)
                }
            }
        return {}

    def _build_dynamic_filters(self, query: str, keywords: List[str]) -> dict:
        """ë™ì  í•„í„° êµ¬ì„± (í™•ì¥ ê°€ëŠ¥)"""
        filters = {
            'must': [],
            'must_not': [],
            'filter': []
        }

        # ì‹œê°„ ë²”ìœ„ í•„í„° (ì„¤ì • ê°€ëŠ¥)
        if hasattr(settings, 'search_date_range_days'):
            filters['filter'].append({
                "range": {
                    "created_datetime": {
                        "gte": f"now-{settings.search_date_range_days}d"
                    }
                }
            })

        # í’ˆì§ˆ í•„í„° (ìŠ¤íŒ¸ ì œê±°)
        if hasattr(settings, 'exclude_spam_keywords'):
            spam_keywords = settings.exclude_spam_keywords
            if spam_keywords:
                filters['must_not'].append({
                    "terms": {"title": spam_keywords}
                })
                filters['must_not'].append({
                    "terms": {"content": spam_keywords}
                })

        return filters

    async def _build_vector_config(self, search_text: str, size: int) -> dict:
        vector_field_name = getattr(settings, 'vector_field_name', 'vector_field')
        embedded_vector = await self.os_tool.embed_query(search_text)
        return {
            "knn": {
                vector_field_name: {
                    "vector": embedded_vector,
                    "k": size * 3
                }
            }
        }

    def _build_sort_strategy(self, query: str) -> List[dict]:
        """ì •ë ¬ ì „ëµ ë™ì  êµ¬ì„±"""
        sort_strategy = []

        # ìµœì‹ ìˆœ ìš°ì„ 
        sort_strategy.append({
            "created_datetime": {
                "order": "desc",
                "missing": "_last"
            }
        })

        # ê´€ë ¨ë„ ì ìˆ˜
        sort_strategy.append("_score")

        return sort_strategy

    def _get_source_fields(self) -> dict:
        """ë°˜í™˜í•  í•„ë“œ êµ¬ì„±"""
        return {
            "includes": [
                "title", "url", "media", "portal",
                "image_url", "created_date", "created_datetime",
                "content", "metadata.title", "metadata.content"
            ]
        }

    def _build_highlight_config(self) -> dict:
        """í•˜ì´ë¼ì´íŠ¸ êµ¬ì„±"""
        return {
            "fields": {
                "title": {"number_of_fragments": 0},
                "content": {"fragment_size": 150, "number_of_fragments": 3}
            },
            "pre_tags": ["<mark>"],
            "post_tags": ["</mark>"]
        }

    def _format_with_highlights(self, hits: List[dict], limit: int) -> List[Dict[str, Any]]:
        """í•˜ì´ë¼ì´íŠ¸ ì •ë³´ í¬í•¨ í¬ë§·íŒ…"""
        formatted = []
        for hit in hits[:limit]:
            source = hit.get("_source", {})
            highlight = hit.get("highlight", {})

            formatted_hit = {
                "id": hit.get("_id"),
                "title": source.get("title") or source.get("metadata", {}).get("title", "(no title)"),
                "url": source.get("url") or source.get("metadata", {}).get("url"),
                "date": source.get("created_datetime") or source.get("created_date"),
                "media": source.get("media") or source.get("portal", "Unknown"),
                "score": hit.get("_score"),
                "index": hit.get("_index")
            }

            # í•˜ì´ë¼ì´íŠ¸ ì •ë³´ ì¶”ê°€
            if highlight:
                formatted_hit["highlights"] = {
                    "title": highlight.get("title", []),
                    "content": highlight.get("content", [])
                }

            formatted.append(formatted_hit)

        return formatted

    async def _rerank_with_ontology_relevance(self, news_hits: List[Dict], ontology_entities: List[str]) -> List[Dict]:
        """ì˜¨í†¨ë¡œì§€ ê´€ë ¨ì„± ê¸°ë°˜ ì¬ì •ë ¬"""
        try:
            for hit in news_hits:
                title = hit.get("title", "").lower()
                content = hit.get("content", "").lower()

                # ì˜¨í†¨ë¡œì§€ ì—”í‹°í‹° ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
                ontology_score = 0
                for entity in ontology_entities:
                    entity_lower = entity.lower()
                    if entity_lower in title:
                        ontology_score += 3  # ì œëª©ì— ìˆìœ¼ë©´ ë†’ì€ ì ìˆ˜
                    elif entity_lower in content:
                        ontology_score += 1  # ë‚´ìš©ì— ìˆìœ¼ë©´ ê¸°ë³¸ ì ìˆ˜

                # ê¸°ì¡´ ì ìˆ˜ì— ì˜¨í†¨ë¡œì§€ ì ìˆ˜ ì¶”ê°€
                original_score = hit.get("score", 0)
                hit["ontology_score"] = ontology_score
                hit["combined_score"] = original_score + (ontology_score * 0.5)

            # í†µí•© ì ìˆ˜ë¡œ ì¬ì •ë ¬
            news_hits.sort(key=lambda x: x.get("combined_score", 0), reverse=True)

            print(f"[DEBUG] ì˜¨í†¨ë¡œì§€ ì¬ì •ë ¬ ì™„ë£Œ: í‰ê·  ì˜¨í†¨ë¡œì§€ ì ìˆ˜ {sum(h.get('ontology_score', 0) for h in news_hits)/len(news_hits):.1f}")

            return news_hits

        except Exception as e:
            print(f"[WARNING] ì˜¨í†¨ë¡œì§€ ì¬ì •ë ¬ ì‹¤íŒ¨: {e}")
            return news_hits

    # ê¸°ì¡´ ë©”ì†Œë“œë¥¼ ì˜¨í†¨ë¡œì§€ í†µí•© ë²„ì „ìœ¼ë¡œ ëŒ€ì²´
    async def _search_news_simple_hybrid(self, query: str, size: int = 5) -> Tuple[List[Dict[str, Any]], float, Optional[str]]:
        """ì˜¨í†¨ë¡œì§€ í†µí•©ëœ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ ìœ ì§€)"""
        return await self._search_news_with_ontology(query, size)

    @with_error_handling("opensearch", fallback_value=([], 0.0, "OpenSearch ì„œë¹„ìŠ¤ ì‚¬ìš© ë¶ˆê°€"))
    @with_retry(max_retries=2, exceptions=(Exception,))
    # @cache_decorator.cached("news_search", ttl=180.0)  # ìºì‹± ë¹„í™œì„±í™”
    async def _search_news(self, query: str, size: int = 5) -> Tuple[List[Dict[str, Any]], float, Optional[str]]:
        t0 = time.perf_counter()
        err: Optional[str] = None
        try:
            os_index = settings.news_embedding_index
            # í‚¤ì›Œë“œ ë³€í™˜ ì‚¬ìš© (ì›ë³¸ ì§ˆë¬¸ë³´ë‹¤ ì •í™•í•œ í‚¤ì›Œë“œ ì‚¬ìš©)
            search_keywords = self._get_context_keywords(query)
            print(f"[DEBUG] ì›ë³¸ ì¿¼ë¦¬: '{query}' â†’ ê²€ìƒ‰ í‚¤ì›Œë“œ: '{search_keywords}'")

            body = {
                "query": {
                    "bool": {
                        "should": [
                            {
                                "multi_match": {
                                    "query": search_keywords,
                                    "fields": ["title^4", "content^2", "text^3", "metadata.title^4", "metadata.content^2"],
                                    "type": "best_fields",
                                    "operator": "and",  # AND ì—°ì‚°ìœ¼ë¡œ ì •í™•ë„ í–¥ìƒ
                                    "minimum_should_match": "60%"
                                }
                            },
                            {
                                "query_string": {
                                    "query": search_keywords,
                                    "fields": ["title^3", "content", "metadata.title^3", "metadata.content", "text"],
                                    "default_operator": "AND",  # AND ì—°ì‚°ìœ¼ë¡œ ì •í™•ë„ í–¥ìƒ
                                }
                            },
                            {
                                "multi_match": {
                                    "query": search_keywords,
                                    "fields": ["title^2", "content"],
                                    "type": "best_fields",
                                    "operator": "or",
                                }
                            }
                        ],
                        "must": [],
                        "must_not": [],
                        "minimum_should_match": 1,
                    }
                },
                "sort": [
                    {"created_datetime": {"order": "desc", "missing": "_last"}},
                    {"created_date": {"order": "desc", "missing": "_last"}},
                    "_score"
                ],
                "_source": {"includes": ["title", "url", "media", "portal", "image_url", "created_date", "created_datetime"]}
            }
            # OpenSearch ì§ì ‘ í˜¸ì¶œ (ë³µì¡í•œ ì¿¼ë¦¬ ì§€ì›)
            result = await self.os.search(
                index=os_index,
                query=body,
                size=size
            )
            
            if result and result.get("hits"):
                hits = result["hits"].get("hits", [])
                out = _format_sources(hits)
                return out, (time.perf_counter() - t0) * 1000.0, None
            else:
                return [], (time.perf_counter() - t0) * 1000.0, "No results found"
                
        except Exception as e:
            print(f"[ERROR] [/chat] OpenSearch error: {e}")
            err = str(e)
            return [], (time.perf_counter() - t0) * 1000.0, err

    @with_error_handling("neo4j", fallback_value=([], 0.0, "Neo4j ì„œë¹„ìŠ¤ ì‚¬ìš© ë¶ˆê°€"))
    @with_retry(max_retries=2, exceptions=(Exception,))
    # @cache_decorator.cached("graph_query", ttl=600.0)  # ìºì‹± ë¹„í™œì„±í™”
    async def _query_graph(self, query: str, limit: int = 10):
        t0 = time.perf_counter()
        try:
            cypher = settings.resolve_search_cypher()
            if not cypher:
                # ë¼ë²¨ë³„ í‚¤ ë§¤í•‘ìœ¼ë¡œ ë™ì  ìƒì„± (ë°±ì—…)
                keys_map = settings.get_graph_search_keys()
                cypher = build_label_aware_search_cypher(keys_map)

            # --- NEW: ìƒˆ ìŠ¤í‚¤ë§ˆ íŒŒë¼ë¯¸í„° (domain ì œê±°) ---
            _, lookback_infer = _infer_domain_and_lookback(query)
            lookback_default = settings.neo4j_search_lookback_days

            params = {
                "q": query,
                "limit": limit,
                "lookback_days": lookback_infer or lookback_default or 180,
            }

            rows = await self.neo.query(cypher, params)
            return rows, (time.perf_counter() - t0) * 1000.0, None
        except Exception as e:
            print(f"[ERROR] [/chat] Neo4j label-aware search error: {e}")
            return [], (time.perf_counter() - t0) * 1000.0, str(e)

    @with_error_handling("stock_api", fallback_value=(None, 0.0, "ì£¼ì‹ API ì„œë¹„ìŠ¤ ì‚¬ìš© ë¶ˆê°€"))
    @with_retry(max_retries=2, exceptions=(Exception,))
    # @cache_decorator.cached("stock_price", ttl=60.0)  # ìºì‹± ë¹„í™œì„±í™”
    async def _get_stock(self, symbol: Optional[str]) -> Tuple[Optional[Dict[str, Any]], float, Optional[str]]:
        t0 = time.perf_counter()
        if not symbol:
            return None, 0.0, None
        try:
            price = await self.st.get_price(symbol)
            return price, (time.perf_counter() - t0) * 1000.0, None
        except Exception as e:
            print(f"[ERROR] [/chat] Stock error: {e}")
            return None, (time.perf_counter() - t0) * 1000.0, str(e)

    async def _compose_answer(
        self,
        query: str,
        news_hits: List[Dict[str, Any]],
        graph_rows: List[Dict[str, Any]],
        stock: Optional[Dict[str, Any]],
        search_meta: Optional[Dict[str, Any]] = None,
    ) -> str:
        """ìƒˆë¡œìš´ í¬ë§·í„°ë¥¼ ì‚¬ìš©í•œ ë‹µë³€ ìƒì„±"""
        
        # LLM ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ì¸ì‚¬ì´íŠ¸ ìƒì„± (ì„ì‹œ ë¹„í™œì„±í™”)
        insights_content = None

        return response_formatter.format_comprehensive_answer(
            query=query,
            news_hits=news_hits,
            graph_rows=graph_rows, 
            stock=stock,
            insights=insights_content,
            search_meta=search_meta
        )

    async def generate_answer(self, query: str) -> Dict[str, Any]:
        """ë©”ì¸ ë‹µë³€ ìƒì„± ë©”ì„œë“œ - ê°•í™”ëœ ì˜¤ë¥˜ ì²˜ë¦¬ í¬í•¨"""
        start_time = time.time()
        services_attempted = []
        try:
            symbol = _detect_symbol(query)
            
            # ê¸°ì¡´ í‚¤ì›Œë“œ ì¶”ì¶œ ì‚¬ìš© (LLMì€ ì¸ì‚¬ì´íŠ¸ ìƒì„±ì—ì„œë§Œ ì‚¬ìš©)
            keywords_str = await self._get_context_keywords(query)
            keywords = keywords_str.split() if keywords_str else [query]
            search_query = " ".join(keywords) if keywords else query
            
            print(f"[INFO] ì›ë³¸ ì§ˆë¬¸: {query}")
            print(f"[INFO] ì¶”ì¶œëœ í‚¤ì›Œë“œ: {keywords}")
            print(f"[INFO] ê²€ìƒ‰ ì¿¼ë¦¬: {search_query}")

            async with anyio.create_task_group() as tg:
                news_res: Dict[str, Any] = {}
                graph_res: Dict[str, Any] = {}
                stock_res: Dict[str, Any] = {}

                async def _news():
                    try:
                        services_attempted.append("opensearch")
                        # ë‹¨ìˆœ OpenSearch + ë²¡í„° í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‚¬ìš©
                        hits, ms, err = await self._search_news_simple_hybrid(query, size=5)

                        news_res.update({
                            "hits": hits,
                            "latency_ms": ms,
                            "error": err,
                            "search_strategy": "hybrid_search",
                            "search_confidence": 0.8,
                            "query_used": query
                        })
                        
                        if not err:
                            error_handler.record_success("opensearch")
                    except Exception as e:
                        error_handler.record_error("opensearch", e)
                        # í´ë°± ë‰´ìŠ¤ ë°ì´í„°
                        news_res.update({
                            "hits": [], 
                            "latency_ms": 0.0, 
                            "error": f"ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}",
                            "search_strategy": "fallback",
                            "search_confidence": 0.1,
                            "query_used": query
                        })

                async def _graph():
                    try:
                        services_attempted.append("neo4j")
                        # ë‹¨ê³„ë³„ ê·¸ë˜í”„ ê²€ìƒ‰ ì „ëµ (ì˜¤ë¥˜ ì²˜ë¦¬ëŠ” ë©”ì„œë“œ ë°ì½”ë ˆì´í„°ì—ì„œ)
                        rows, ms, err = await self._query_graph(search_query, limit=30)
                        
                        # ê²€ìƒ‰ ê²°ê³¼ê°€ ë¶€ì¡±í•  ë•Œ ì¶”ê°€ ì‹œë„ (í•µì‹¬ í‚¤ì›Œë“œ)
                        if not rows or len(rows) < 3:
                            # ì¼ë°˜ì ì¸ ë¹„ì¦ˆë‹ˆìŠ¤ í‚¤ì›Œë“œë¡œ ëŒ€ì²´
                            core_keywords = [k for k in keywords if k in ["ìƒì¥ì‚¬", "íˆ¬ì", "ì‹¤ì ", "ê¸°ì—…", "ë§¤ì¶œ", "ì„±ì¥"]]
                            if core_keywords:
                                core_query = " ".join(core_keywords)
                                rows2, ms2, err2 = await self._query_graph(core_query, limit=30)
                                if len(rows2) > len(rows):
                                    rows, ms, err = rows2, ms2, err2
                        
                        graph_res.update({"rows": rows, "latency_ms": ms, "error": err})
                        
                        if not err:
                            error_handler.record_success("neo4j")
                    except Exception as e:
                        error_handler.record_error("neo4j", e)
                        graph_res.update({"rows": [], "latency_ms": 0.0, "error": f"ê·¸ë˜í”„ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}"})

                async def _stock():
                    try:
                        services_attempted.append("stock_api")
                        price, ms, err = await self._get_stock(symbol)
                        stock_res.update({"price": price, "latency_ms": ms, "error": err})
                        
                        if not err:
                            error_handler.record_success("stock_api")
                    except Exception as e:
                        error_handler.record_error("stock_api", e)
                        stock_res.update({"price": None, "latency_ms": 0.0, "error": f"ì£¼ê°€ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"})

                tg.start_soon(_news)
                tg.start_soon(_graph)
                tg.start_soon(_stock)

            # ê²€ìƒ‰ ë©”íƒ€ë°ì´í„° ì¤€ë¹„
            search_meta = {
                "search_strategy": news_res.get("search_strategy"),
                "search_confidence": news_res.get("search_confidence"),
                "query_used": news_res.get("query_used")
            }

            # ë‹µë³€ ìƒì„± (ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ìµœëŒ€í•œ ì •ë³´ ì œê³µ)
            answer = await self._compose_answer(
                query=query,
                news_hits=news_res.get("hits") or [],
                graph_rows=graph_res.get("rows") or [],
                stock=stock_res.get("price"),
                search_meta=search_meta
            )

            # ì „ì²´ ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
            total_time = (time.time() - start_time) * 1000.0
            
            # ë©”íƒ€ë°ì´í„° êµ¬ì„±
            meta = {
                "orchestrator": "v0_enhanced",
                "total_latency_ms": round(total_time, 2),
                "latency_ms": {
                    "opensearch": round(news_res.get("latency_ms", 0.0), 2),
                    "neo4j": round(graph_res.get("latency_ms", 0.0), 2),
                    "stock": round(stock_res.get("latency_ms", 0.0), 2),
                },
                "errors": {
                    "opensearch": news_res.get("error"),
                    "neo4j": graph_res.get("error"),
                    "stock": stock_res.get("error"),
                },
                "services_attempted": services_attempted,
                "system_health": error_handler.get_health_report(),
                "symbol_detected": symbol,
                "indices": {
                    "news_bulk_index": settings.news_bulk_index,
                    "news_embedding_index": settings.news_embedding_index,
                },
                "database": settings.neo4j_database,
            }

            sources = news_res.get("hits") or []

            result = {
                "query": query,
                "answer": answer,
                "sources": sources,
                "graph_samples": graph_res.get("rows")[:3] if graph_res.get("rows") else [],
                "graph_summary": summarize_graph_rows(graph_res.get("rows") or [], max_each=5) if graph_res.get("rows") else None,
                "stock": stock_res.get("price"),
                "meta": meta,
            }
            
            print(f"[INFO] ë‹µë³€ ìƒì„± ì™„ë£Œ: {total_time:.2f}ms, ì„œë¹„ìŠ¤: {services_attempted}")
            return result
            
        except Exception as e:
            # ìµœì¢… í´ë°±: ì‹œìŠ¤í…œ ì „ì²´ ì˜¤ë¥˜ ì‹œì—ë„ ê¸°ë³¸ ì‘ë‹µ ì œê³µ
            error_handler.record_error("system", e, context={"query": query})
            total_time = (time.time() - start_time) * 1000.0
            
            print(f"[ERROR] ì „ì²´ ì‹œìŠ¤í…œ ì˜¤ë¥˜: {e}")
            
            # ìµœì†Œí•œì˜ ì‘ë‹µ ìƒì„±
            fallback_answer = f"""## âš ï¸ ì‹œìŠ¤í…œ ì¼ì‹œ ì¥ì• 

ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì‹œìŠ¤í…œì— ì¼ì‹œì ì¸ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.

**ì§ˆì˜**: {query}

### ğŸ“Š ì¼ë°˜ì ì¸ ì‹œì¥ ì •ë³´
- **ì‹œì¥ ë™í–¥**: ì‹¤ì‹œê°„ ì •ë³´ëŠ” ë³„ë„ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤
- **íˆ¬ì ì°¸ê³ **: ì „ë¬¸ê°€ ìƒë‹´ì„ ê¶Œì¥í•©ë‹ˆë‹¤
- **ì„œë¹„ìŠ¤ ìƒíƒœ**: ë³µêµ¬ ì¤‘ì´ë©° ê³§ ì •ìƒ ì„œë¹„ìŠ¤ë©ë‹ˆë‹¤

### ğŸ”§ ì¶”ì²œ ì¡°ì¹˜
- ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”
- ë” êµ¬ì²´ì ì¸ í‚¤ì›Œë“œë¡œ ì§ˆì˜í•´ ë³´ì„¸ìš”
- ì‹œìŠ¤í…œ ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”

**ì˜¤ë¥˜ ID**: {int(time.time())}
"""

            return {
                "query": query,
                "answer": fallback_answer,
                "sources": [],
                "graph_samples": [],
                "graph_summary": None,
                "stock": None,
                "meta": {
                    "orchestrator": "fallback",
                    "total_latency_ms": round(total_time, 2),
                    "error": str(e),
                    "services_attempted": services_attempted,
                    "system_health": error_handler.get_health_report()
                }
            }