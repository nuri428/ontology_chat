# src/ontology_chat/services/chat_service.py
from __future__ import annotations
from typing import Any, Dict, List, Optional, Tuple
import re
import time
import anyio
# from api.logging import setup_logging
# logger = setup_logging()

from api.config import settings
from api.adapters.mcp_opensearch import OpenSearchMCP
from api.adapters.mcp_neo4j import Neo4jMCP
from api.adapters.mcp_stock import StockMCP
from api.services.cypher_builder import build_label_aware_search_cypher
from api.services.formatters import summarize_graph_rows
from api.services.search_strategy import advanced_search_engine, SearchResult
from api.services.response_formatter import response_formatter
from api.services.cache_manager import cache_decorator, cache_manager
from api.services.error_handler import error_handler, with_retry, with_error_handling

# --- NEW: ê°„ë‹¨í•œ ë„ë©”ì¸/ë£©ë°± ì¶”ë¡  ---
def _infer_domain_and_lookback(query: str) -> tuple[str, int]:
    q = query.lower()
    domain = settings.neo4j_search_default_domain or ""
    lookback = settings.neo4j_search_lookback_days

    # ì§ˆì˜ì— 'ìµœê·¼', 'ìš”ì¦˜', 'ìµœê·¼ 3ê°œì›”' ë¥˜ê°€ ìˆìœ¼ë©´ lookback ê°€ë³€ ì ìš©(ê°„ë‹¨ ê·œì¹™)
    # ì˜ˆ) "ìµœê·¼ 90ì¼", "ìµœê·¼ 6ê°œì›”"
    m = re.search(r"ìµœê·¼\s*(\d+)\s*(ì¼|ê°œì›”)", q)
    if m:
        val = int(m.group(1))
        unit = m.group(2)
        if unit == "ì¼":
            lookback = max(1, min(365*2, val))
        else:  # ê°œì›”
            lookback = max(7, min(365*2, val * 30))

    # ë„ë©”ì¸ íŒíŠ¸: ì§€ìƒë¬´ê¸°/ì „ì°¨/ìì£¼í¬/ì¥ê°‘ì°¨ ë“± í¬í•¨ ì‹œ ìë™ ë³´ê°•
    if any(tok in q for tok in ["ì§€ìƒë¬´ê¸°", "ì „ì°¨", "ìì£¼í¬", "ì¥ê°‘ì°¨"]):
        domain = (domain + " ì§€ìƒë¬´ê¸° ì „ì°¨ ìì£¼í¬ ì¥ê°‘ì°¨").strip()

    # íšŒì‚¬ëª… íŒíŠ¸: í•œí™”/í•œí™”ë””íœìŠ¤ ë“± í¬í•¨ ì‹œ ìë™ ë³´ê°•
    if "í•œí™”" in q:
        domain = (domain + " í•œí™” í•œí™”ë””íœìŠ¤").strip()
    if "kai" in q or "ì¹´ì´" in q:
        domain = (domain + " kai í•œêµ­í•­ê³µìš°ì£¼ k-a1").strip()

    # ì¤‘ë³µ ê³µë°± ì •ë¦¬
    domain = " ".join(domain.split())
    return domain, lookback


@cache_decorator.cached("keyword_extraction", ttl=3600.0)  # 1ì‹œê°„ ìºì‹œ
def _extract_keywords_for_search(query: str) -> List[str]:
    """ê°œì„ ëœ í‚¤ì›Œë“œ ì¶”ì¶œ ë¡œì§ - ë™ì  í™•ì¥, ê°€ì¤‘ì¹˜ ê¸°ë°˜, í˜•íƒœì†Œ ë¶„ì„ (ì»¨í…ìŠ¤íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ê°•í™”)"""
    from api.config.keyword_mappings import get_all_keyword_mappings
    from api.utils.text_analyzer import enhance_query_with_morphology, suggest_related_terms
    
    q = query.lower()
    keyword_mappings = get_all_keyword_mappings()
    
    # ê°€ì¤‘ì¹˜ê°€ ìˆëŠ” í‚¤ì›Œë“œ ì €ì¥ì†Œ
    weighted_keywords = []
    
    # 0. í˜•íƒœì†Œ ë¶„ì„ì„ í†µí•œ ì¿ ë¦¬ ê°•í™”
    morphology_result = enhance_query_with_morphology(query)
    high_importance_words = morphology_result["high_importance_keywords"]
    companies = morphology_result["companies"]
    tech_terms = morphology_result["tech_terms"]
    finance_terms = morphology_result["finance_terms"]
    
    # í˜•íƒœì†Œ ë¶„ì„ ê²°ê³¼ë¡œ ì¶”ê°€ ê°€ì¤‘ì¹˜ ë¶€ì—¬
    for word in high_importance_words:
        weighted_keywords.append((word, 2.0))
    
    for word in companies:
        weighted_keywords.append((word, 2.5))
        # ì—°ê´€ ìš©ì–´ ì¶”ê°€
        related = suggest_related_terms(word)
        for rel_word in related[:3]:  # ìƒìœ„ 3ê°œë§Œ
            weighted_keywords.append((rel_word, 1.8))
    
    for word in tech_terms:
        weighted_keywords.append((word, 2.2))
    
    for word in finance_terms:
        weighted_keywords.append((word, 2.3))
    
    # 1. ë„ë©”ì¸ë³„ í‚¤ì›Œë“œ ì¶”ì¶œ
    for domain_name, domain_data in keyword_mappings["domain"].items():
        for trigger in domain_data["triggers"]:
            if trigger in q:
                # í™•ì¥ í‚¤ì›Œë“œ ì¶”ê°€ (ê°€ì¤‘ì¹˜ ìˆœìœ¼ë¡œ ì •ë ¬)
                for kw in sorted(domain_data["expansions"], key=lambda x: (x.priority, -x.weight)):
                    weighted_keywords.append((kw.keyword, kw.weight))
                
                # ìœ ì‚¬ì–´ ì¶”ê°€
                for base_word, synonyms in domain_data.get("synonyms", {}).items():
                    if base_word in q:
                        for syn in synonyms:
                            weighted_keywords.append((syn, 1.2))  # ìœ ì‚¬ì–´ëŠ” ê¸°ë³¸ ê°€ì¤‘ì¹˜
                break
    
    # 2. ì‚°ì—…ë³„ í‚¤ì›Œë“œ ì¶”ì¶œ
    for industry_name, keywords in keyword_mappings["industry"].items():
        industry_triggers = {
            "defense": ["ë°©ì‚°", "ë¬´ê¸°", "êµ­ë°©", "êµ°ì‚¬"],
            "aerospace": ["í•­ê³µ", "ìš°ì£¼", "ìœ„ì„±"],
            "nuclear": ["ì›ì „", "ì›ìë ¥", "í•µ"]
        }.get(industry_name, [])
        
        if any(trigger in q for trigger in industry_triggers):
            for kw in keywords:
                weighted_keywords.append((kw.keyword, kw.weight))
    
    # 3. íšŒì‚¬ë³„ í‚¤ì›Œë“œ ì¶”ì¶œ
    for company_name, company_data in keyword_mappings["company"].items():
        for trigger in company_data["triggers"]:
            if trigger in q:
                for kw in company_data["expansions"]:
                    weighted_keywords.append((kw.keyword, kw.weight))
                break
    
    # 4. ì‹œê°„ ê´€ë ¨ í‚¤ì›Œë“œ ì¶”ì¶œ
    for time_type, time_data in keyword_mappings["time"].items():
        for trigger in time_data["triggers"]:
            if trigger in q:
                for kw in time_data["expansions"]:
                    weighted_keywords.append((kw.keyword, kw.weight))
                break
    
    # 5. ì§€ì—­ë³„ í‚¤ì›Œë“œ ì¶”ì¶œ
    for region_name, region_data in keyword_mappings["region"].items():
        for trigger in region_data["triggers"]:
            if trigger in q:
                for kw in region_data["expansions"]:
                    weighted_keywords.append((kw.keyword, kw.weight))
                break
    
    # 6. ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì •ë ¬ ë° ì¤‘ë³µ ì œê±°
    keyword_weights = {}
    for keyword, weight in weighted_keywords:
        if keyword not in keyword_weights:
            keyword_weights[keyword] = weight
        else:
            # ì¤‘ë³µ í‚¤ì›Œë“œëŠ” ìµœëŒ€ ê°€ì¤‘ì¹˜ ì‚¬ìš©
            keyword_weights[keyword] = max(keyword_weights[keyword], weight)
    
    # ê°€ì¤‘ì¹˜ ìˆœìœ¼ë¡œ ì •ë ¬
    sorted_keywords = sorted(keyword_weights.items(), key=lambda x: -x[1])
    
    # 7. í‚¤ì›Œë“œê°€ ë¶€ì¡±í•˜ë©´ ì›ë³¸ ì§ˆë¬¸ì—ì„œ ì¶”ê°€ ì¶”ì¶œ
    if len(sorted_keywords) < 5:
        stopwords = keyword_mappings["stopwords"]
        # í˜•íƒ„ì†Œ ë¶„ì„ ê²°ê³¼ë¥¼ í™œìš©í•œ ì¶”ê°€ í‚¤ì›Œë“œ
        key_phrases = morphology_result["key_phrases"]
        for phrase, importance in key_phrases:
            if phrase not in keyword_weights and importance > 1.0:
                sorted_keywords.append((phrase, importance * 0.8))  # ì•½ê°„ ë‚®ì€ ê°€ì¤‘ì¹˜
        
        # ì—¬ì „íˆ ë¶€ì¡±í•˜ë©´ ê¸°ë³¸ ì²˜ë¦¬
        if len(sorted_keywords) < 5:
            words = [w for w in q.split() if len(w) > 1 and w not in stopwords]
            for word in words:
                if word not in keyword_weights:
                    sorted_keywords.append((word, 0.5))
    
    # 8. ìµœì¢… í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (ìƒìœ„ 15ê°œ)
    final_keywords = [kw[0] for kw in sorted_keywords[:15]]
    
    # ë¡œê·¸ë¡œ ë¶„ì„ ê²°ê³¼ ì¶”ê°€ (ê°œë°œìš©)
    # logger.info(f"í˜•íƒœì†Œ ë¶„ì„ ê²°ê³¼: ì¤‘ìš” í‚¤ì›Œë“œ={high_importance_words}, íšŒì‚¬={companies}")
    
    return final_keywords


def _detect_symbol(text: str) -> Optional[str]:
    m = re.search(r"\b\d{6}\.(KS|KQ)\b", text)
    if m:
        return m.group(0)
    return None


def _format_sources(hits: List[Dict[str, Any]], limit: int = 5) -> List[Dict[str, Any]]:
    out: List[Dict[str, Any]] = []
    for h in hits[:limit]:
        src = h.get("_source", {}) or {}
        meta = src.get("metadata") or {}
        
        # ì œëª© ì¶”ì¶œ (ì—¬ëŸ¬ í•„ë“œì—ì„œ ì‹œë„)
        title = (
            src.get("title") or 
            meta.get("title") or 
            src.get("headline") or 
            "(no title)"
        )
        
        # URL ì¶”ì¶œ
        url = (
            src.get("url") or 
            meta.get("url") or 
            src.get("link") or 
            src.get("article_url")
        )
        
        # ë‚ ì§œ ì¶”ì¶œ (ì—¬ëŸ¬ í•„ë“œì—ì„œ ì‹œë„)
        date = (
            src.get("created_datetime") or
            src.get("created_date") or
            src.get("published_at") or
            src.get("publish_date") or
            meta.get("created_datetime") or
            meta.get("created_date") or
            meta.get("published_at")
        )
        
        # ë¯¸ë””ì–´ ì •ë³´ ì¶”ì¶œ
        media = src.get("media") or meta.get("media") or src.get("source") or "Unknown"
        
        out.append(
            {
                "id": h.get("_id"),
                "title": title,
                "url": url,
                "date": date,
                "media": media,
                "score": h.get("_score"),
                "index": h.get("_index"),
            }
        )
    return out


class ChatService:
    def __init__(self):
        self.os = OpenSearchMCP()
        self.neo = Neo4jMCP()
        self.st = StockMCP()

    async def _graph(self, query: str) -> tuple[list[dict], dict | None]:
        rows, ms, err = await self._query_graph(query, limit=30)
        summary = summarize_graph_rows(rows, max_each=5) if rows else None
        return rows, summary

    async def _search_news_advanced(
        self, 
        original_query: str, 
        keywords: List[str], 
        size: int = 5
    ) -> Tuple[SearchResult, float, Optional[str]]:
        """ê³ ê¸‰ ë‹¤ë‹¨ê³„ ë‰´ìŠ¤ ê²€ìƒ‰"""
        t0 = time.perf_counter()
        
        # 1. ë„ë©”ì¸ ê°ì§€ ë° ì—”í‹°í‹° ì¶”ì¶œ
        domains = advanced_search_engine.detect_query_domain(original_query)
        entities = advanced_search_engine.extract_entities(original_query)
        
        print(f"[INFO] ê²€ìƒ‰ ë„ë©”ì¸: {domains}, ì¶”ì¶œ ì—”í‹°í‹°: {entities}")
        
        # 2. ê²€ìƒ‰ ì „ëµ ìƒì„±
        search_strategies = advanced_search_engine.build_enhanced_queries(
            original_query, keywords, domains, entities
        )
        
        best_result = None
        best_quality = 0.0
        
        # 3. ì „ëµë³„ ìˆœì°¨ ê²€ìƒ‰
        for strategy in search_strategies[:4]:  # ìƒìœ„ 4ê°œ ì „ëµë§Œ ì‹œë„
            try:
                print(f"[INFO] ê²€ìƒ‰ ì „ëµ ì‹œë„: {strategy.name} - {strategy.query}")
                
                hits, ms, err = await self._search_news(strategy.query, size)
                
                if hits:
                    # í’ˆì§ˆ í‰ê°€
                    quality = advanced_search_engine.evaluate_search_quality(
                        hits, original_query, strategy
                    )
                    
                    print(f"[INFO] ì „ëµ {strategy.name} í’ˆì§ˆ: {quality:.2f}, ê²°ê³¼: {len(hits)}ê±´")
                    
                    if quality > best_quality:
                        best_quality = quality
                        best_result = SearchResult(
                            hits=hits,
                            query_used=strategy.query,
                            strategy=strategy.name,
                            confidence=quality,
                            latency_ms=ms,
                            total_found=len(hits)
                        )
                    
                    # í’ˆì§ˆì´ ì¶©ë¶„íˆ ë†’ìœ¼ë©´ ì¡°ê¸° ì¢…ë£Œ
                    if quality > 0.8:
                        break
                        
            except Exception as e:
                print(f"[WARNING] ê²€ìƒ‰ ì „ëµ {strategy.name} ì‹¤íŒ¨: {e}")
                continue
        
        # 4. ê²°ê³¼ ë°˜í™˜
        total_time = (time.perf_counter() - t0) * 1000.0
        
        if best_result:
            print(f"[INFO] ìµœì¢… ì„ íƒ: {best_result.strategy} (í’ˆì§ˆ: {best_result.confidence:.2f})")
            return best_result, total_time, None
        else:
            # ì™„ì „ í´ë°±: ì›ë³¸ ì¿¼ë¦¬ë¡œ ì¬ì‹œë„
            print(f"[WARNING] ëª¨ë“  ê³ ê¸‰ ê²€ìƒ‰ ì‹¤íŒ¨, ì›ë³¸ ì¿¼ë¦¬ë¡œ í´ë°±")
            hits, ms, err = await self._search_news(original_query, size)
            
            fallback_result = SearchResult(
                hits=hits or [],
                query_used=original_query,
                strategy="fallback_original",
                confidence=0.3,
                latency_ms=ms,
                total_found=len(hits) if hits else 0
            )
            
            return fallback_result, total_time, err

    @with_error_handling("opensearch", fallback_value=([], 0.0, "OpenSearch ì„œë¹„ìŠ¤ ì‚¬ìš© ë¶ˆê°€"))
    @with_retry(max_retries=2, exceptions=(Exception,))
    @cache_decorator.cached("news_search", ttl=180.0)  # 3ë¶„ ìºì‹œ
    async def _search_news(self, query: str, size: int = 5) -> Tuple[List[Dict[str, Any]], float, Optional[str]]:
        t0 = time.perf_counter()
        err: Optional[str] = None
        try:
            os_index = settings.news_bulk_index or "news_article_bulk"
            body = {
                "query": {
                    "bool": {
                        "should": [
                            {
                                "multi_match": {
                                    "query": query,
                                    "fields": ["title^4", "content^2", "text^3", "metadata.title^4", "metadata.content^2"],
                                    "type": "best_fields",
                                    "operator": "or",
                                }
                            },
                            {
                                "query_string": {
                                    "query": query,
                                    "fields": ["title^3", "content", "metadata.title^3", "metadata.content", "text"],
                                    "default_operator": "OR",
                                }
                            },
                            {
                                "multi_match": {
                                    "query": "í•œí™” ë¡œë´‡ ë°©ì‚° ë¬´ê¸° ìˆ˜ì¶œ êµ­ë°© K-ë°©ì‚°",
                                    "fields": ["title^2", "content"],
                                    "type": "best_fields",
                                    "operator": "or",
                                }
                            },
                            {
                                "multi_match": {
                                    "query": "í•œí™”ì‹œìŠ¤í…œ í•œí™”ì—ì–´ë¡œìŠ¤í˜ì´ìŠ¤ í•œí™”ë””íœìŠ¤",
                                    "fields": ["title^3", "content"],
                                    "type": "best_fields",
                                    "operator": "or",
                                }
                            }
                        ],
                        "must_not": [
                            {
                                "terms": {
                                    "title": ["ë·°í‹°", "Kì—´í’", "í”¼ë¶€ê³¼", "ë¯¸ìš©", "í™”ì¥í’ˆ", "ìŠ¤í‚¨ì¼€ì–´"]
                                }
                            },
                            {
                                "terms": {
                                    "content": ["ë·°í‹°", "Kì—´í’", "í”¼ë¶€ê³¼", "ë¯¸ìš©", "í™”ì¥í’ˆ", "ìŠ¤í‚¨ì¼€ì–´"]
                                }
                            }
                        ],
                        "minimum_should_match": 1,
                    }
                },
                "sort": [
                    {"created_datetime": {"order": "desc", "missing": "_last"}},
                    {"created_date": {"order": "desc", "missing": "_last"}},
                    "_score"
                ],
                "_source": {"includes": ["title", "url", "media", "portal", "image_url", "created_date", "created_datetime"]}
            }
            # OpenSearch ì§ì ‘ í˜¸ì¶œ (ë³µì¡í•œ ì¿¼ë¦¬ ì§€ì›)
            result = await self.os.search(
                index=os_index,
                query=body,
                size=size
            )
            
            if result and result.get("hits"):
                hits = result["hits"].get("hits", [])
                out = _format_sources(hits)
                return out, (time.perf_counter() - t0) * 1000.0, None
            else:
                return [], (time.perf_counter() - t0) * 1000.0, "No results found"
                
        except Exception as e:
            print(f"[ERROR] [/chat] OpenSearch error: {e}")
            err = str(e)
            return [], (time.perf_counter() - t0) * 1000.0, err

    @with_error_handling("neo4j", fallback_value=([], 0.0, "Neo4j ì„œë¹„ìŠ¤ ì‚¬ìš© ë¶ˆê°€"))
    @with_retry(max_retries=2, exceptions=(Exception,))
    @cache_decorator.cached("graph_query", ttl=600.0)  # 10ë¶„ ìºì‹œ
    async def _query_graph(self, query: str, limit: int = 10):
        t0 = time.perf_counter()
        try:
            cypher = settings.resolve_search_cypher()
            if not cypher:
                # ë¼ë²¨ë³„ í‚¤ ë§¤í•‘ìœ¼ë¡œ ë™ì  ìƒì„± (ë°±ì—…)
                keys_map = settings.get_graph_search_keys()
                cypher = build_label_aware_search_cypher(keys_map)

            # --- NEW: ê¸°ë³¸/ì¶”ë¡  íŒŒë¼ë¯¸í„° í•©ì„± ---
            domain_default, lookback_default = settings.get_graph_search_defaults().values()
            domain_infer, lookback_infer = _infer_domain_and_lookback(query)

            params = {
                "q": query,
                "limit": limit,
                "domain": domain_infer or domain_default or "",
                "lookback_days": lookback_infer or lookback_default or 180,
            }

            rows = await self.neo.query(cypher, params)
            return rows, (time.perf_counter() - t0) * 1000.0, None
        except Exception as e:
            print(f"[ERROR] [/chat] Neo4j label-aware search error: {e}")
            return [], (time.perf_counter() - t0) * 1000.0, str(e)

    @with_error_handling("stock_api", fallback_value=(None, 0.0, "ì£¼ì‹ API ì„œë¹„ìŠ¤ ì‚¬ìš© ë¶ˆê°€"))
    @with_retry(max_retries=2, exceptions=(Exception,))
    @cache_decorator.cached("stock_price", ttl=60.0)  # 1ë¶„ ìºì‹œ
    async def _get_stock(self, symbol: Optional[str]) -> Tuple[Optional[Dict[str, Any]], float, Optional[str]]:
        t0 = time.perf_counter()
        if not symbol:
            return None, 0.0, None
        try:
            price = await self.st.get_price(symbol)
            return price, (time.perf_counter() - t0) * 1000.0, None
        except Exception as e:
            print(f"[ERROR] [/chat] Stock error: {e}")
            return None, (time.perf_counter() - t0) * 1000.0, str(e)

    async def _compose_answer(
        self,
        query: str,
        news_hits: List[Dict[str, Any]],
        graph_rows: List[Dict[str, Any]],
        stock: Optional[Dict[str, Any]],
        search_meta: Optional[Dict[str, Any]] = None,
    ) -> str:
        """ìƒˆë¡œìš´ í¬ë§·í„°ë¥¼ ì‚¬ìš©í•œ ë‹µë³€ ìƒì„±"""
        
        # LLM ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ì¸ì‚¬ì´íŠ¸ ìƒì„± (ì„ì‹œ ë¹„í™œì„±í™”)
        insights_content = None
        # try:
        #     from api.services.context_insight_generator import insight_generator
        #     
        #     # ë¹„ë™ê¸°ë¡œ ì¸ì‚¬ì´íŠ¸ ìƒì„± ìš”ì²­
        #     insight_result = await insight_generator.generate_insights(
        #         query=query,
        #         news_hits=news_hits,
        #         graph_summary=summarize_graph_rows(graph_rows, max_each=5) if graph_rows else None,
        #         stock_info=stock
        #     )
        #     
        #     # ì¸ì‚¬ì´íŠ¸ê°€ ìƒì„±ë˜ë©´ í¬ë§·íŒ…
        #     if insight_result.insights:
        #         insights_content = insight_generator.format_insights_for_display(insight_result)
        #         logger.info(f"ë™ì  ì¸ì‚¬ì´íŠ¸ ìƒì„± ì„±ê³µ: {len(insight_result.insights)}ê°œ, ì‹ ë¢°ë„: {insight_result.confidence:.2f}")
        #     else:
        #         logger.warning("ë™ì  ì¸ì‚¬ì´íŠ¸ ìƒì„± ì‹¤íŒ¨")
        #         
        # except Exception as e:
        #     logger.error(f"ì¸ì‚¬ì´íŠ¸ ìƒì„± ì˜¤ë¥˜: {e}")
        
        # ìƒˆë¡œìš´ í¬ë§·í„°ë¡œ ì¢…í•© ë‹µë³€ ìƒì„±
        return response_formatter.format_comprehensive_answer(
            query=query,
            news_hits=news_hits,
            graph_rows=graph_rows, 
            stock=stock,
            insights=insights_content,
            search_meta=search_meta
        )

    async def generate_answer(self, query: str) -> Dict[str, Any]:
        """ë©”ì¸ ë‹µë³€ ìƒì„± ë©”ì„œë“œ - ê°•í™”ëœ ì˜¤ë¥˜ ì²˜ë¦¬ í¬í•¨"""
        start_time = time.time()
        services_attempted = []
        try:
            symbol = _detect_symbol(query)
            
            # ê¸°ì¡´ í‚¤ì›Œë“œ ì¶”ì¶œ ì‚¬ìš© (LLMì€ ì¸ì‚¬ì´íŠ¸ ìƒì„±ì—ì„œë§Œ ì‚¬ìš©)
            keywords = _extract_keywords_for_search(query)
            search_query = " ".join(keywords) if keywords else query
            
            print(f"[INFO] ì›ë³¸ ì§ˆë¬¸: {query}")
            print(f"[INFO] ì¶”ì¶œëœ í‚¤ì›Œë“œ: {keywords}")
            print(f"[INFO] ê²€ìƒ‰ ì¿¼ë¦¬: {search_query}")

            async with anyio.create_task_group() as tg:
                news_res: Dict[str, Any] = {}
                graph_res: Dict[str, Any] = {}
                stock_res: Dict[str, Any] = {}

                async def _news():
                    try:
                        services_attempted.append("opensearch")
                        # ê³ ê¸‰ ë‹¤ë‹¨ê³„ ê²€ìƒ‰ ì‚¬ìš©
                        search_result, ms, err = await self._search_news_advanced(query, keywords, size=5)
                        
                        news_res.update({
                            "hits": search_result.hits, 
                            "latency_ms": ms, 
                            "error": err,
                            "search_strategy": search_result.strategy,
                            "search_confidence": search_result.confidence,
                            "query_used": search_result.query_used
                        })
                        
                        if not err:
                            error_handler.record_success("opensearch")
                    except Exception as e:
                        error_handler.record_error("opensearch", e)
                        # í´ë°± ë‰´ìŠ¤ ë°ì´í„°
                        news_res.update({
                            "hits": [], 
                            "latency_ms": 0.0, 
                            "error": f"ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}",
                            "search_strategy": "fallback",
                            "search_confidence": 0.1,
                            "query_used": query
                        })

                async def _graph():
                    try:
                        services_attempted.append("neo4j")
                        # ë‹¨ê³„ë³„ ê·¸ë˜í”„ ê²€ìƒ‰ ì „ëµ (ì˜¤ë¥˜ ì²˜ë¦¬ëŠ” ë©”ì„œë“œ ë°ì½”ë ˆì´í„°ì—ì„œ)
                        rows, ms, err = await self._query_graph(search_query, limit=30)
                        
                        # ê²€ìƒ‰ ê²°ê³¼ê°€ ë¶€ì¡±í•  ë•Œ ì¶”ê°€ ì‹œë„
                        if not rows or len(rows) < 3:
                            core_keywords = [k for k in keywords if k in ["ì§€ìƒë¬´ê¸°", "ë¬´ê¸°", "ë°©ì‚°", "í•œí™”", "ìˆ˜ì¶œ", "í•´ì™¸"]]
                            if core_keywords:
                                core_query = " ".join(core_keywords)
                                rows2, ms2, err2 = await self._query_graph(core_query, limit=30)
                                if len(rows2) > len(rows):
                                    rows, ms, err = rows2, ms2, err2
                        
                        graph_res.update({"rows": rows, "latency_ms": ms, "error": err})
                        
                        if not err:
                            error_handler.record_success("neo4j")
                    except Exception as e:
                        error_handler.record_error("neo4j", e)
                        graph_res.update({"rows": [], "latency_ms": 0.0, "error": f"ê·¸ë˜í”„ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}"})

                async def _stock():
                    try:
                        services_attempted.append("stock_api")
                        price, ms, err = await self._get_stock(symbol)
                        stock_res.update({"price": price, "latency_ms": ms, "error": err})
                        
                        if not err:
                            error_handler.record_success("stock_api")
                    except Exception as e:
                        error_handler.record_error("stock_api", e)
                        stock_res.update({"price": None, "latency_ms": 0.0, "error": f"ì£¼ê°€ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"})

                tg.start_soon(_news)
                tg.start_soon(_graph)
                tg.start_soon(_stock)

            # ê²€ìƒ‰ ë©”íƒ€ë°ì´í„° ì¤€ë¹„
            search_meta = {
                "search_strategy": news_res.get("search_strategy"),
                "search_confidence": news_res.get("search_confidence"),
                "query_used": news_res.get("query_used")
            }

            # ë‹µë³€ ìƒì„± (ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ìµœëŒ€í•œ ì •ë³´ ì œê³µ)
            answer = await self._compose_answer(
                query=query,
                news_hits=news_res.get("hits") or [],
                graph_rows=graph_res.get("rows") or [],
                stock=stock_res.get("price"),
                search_meta=search_meta
            )

            # ì „ì²´ ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
            total_time = (time.time() - start_time) * 1000.0
            
            # ë©”íƒ€ë°ì´í„° êµ¬ì„±
            meta = {
                "orchestrator": "v0_enhanced",
                "total_latency_ms": round(total_time, 2),
                "latency_ms": {
                    "opensearch": round(news_res.get("latency_ms", 0.0), 2),
                    "neo4j": round(graph_res.get("latency_ms", 0.0), 2),
                    "stock": round(stock_res.get("latency_ms", 0.0), 2),
                },
                "errors": {
                    "opensearch": news_res.get("error"),
                    "neo4j": graph_res.get("error"),
                    "stock": stock_res.get("error"),
                },
                "services_attempted": services_attempted,
                "system_health": error_handler.get_health_report(),
                "symbol_detected": symbol,
                "indices": {
                    "news_bulk_index": settings.news_bulk_index,
                    "news_embedding_index": settings.news_embedding_index,
                },
                "database": settings.neo4j_database,
            }

            sources = news_res.get("hits") or []

            result = {
                "query": query,
                "answer": answer,
                "sources": sources,
                "graph_samples": graph_res.get("rows")[:3] if graph_res.get("rows") else [],
                "graph_summary": summarize_graph_rows(graph_res.get("rows") or [], max_each=5) if graph_res.get("rows") else None,
                "stock": stock_res.get("price"),
                "meta": meta,
            }
            
            print(f"[INFO] ë‹µë³€ ìƒì„± ì™„ë£Œ: {total_time:.2f}ms, ì„œë¹„ìŠ¤: {services_attempted}")
            return result
            
        except Exception as e:
            # ìµœì¢… í´ë°±: ì‹œìŠ¤í…œ ì „ì²´ ì˜¤ë¥˜ ì‹œì—ë„ ê¸°ë³¸ ì‘ë‹µ ì œê³µ
            error_handler.record_error("system", e, context={"query": query})
            total_time = (time.time() - start_time) * 1000.0
            
            print(f"[ERROR] ì „ì²´ ì‹œìŠ¤í…œ ì˜¤ë¥˜: {e}")
            
            # ìµœì†Œí•œì˜ ì‘ë‹µ ìƒì„±
            fallback_answer = f"""## âš ï¸ ì‹œìŠ¤í…œ ì¼ì‹œ ì¥ì• 

ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì‹œìŠ¤í…œì— ì¼ì‹œì ì¸ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.

**ì§ˆì˜**: {query}

### ğŸ“Š ì¼ë°˜ì ì¸ ì‹œì¥ ì •ë³´
- **ë°©ì‚° ì‚°ì—…**: K-ë°©ì‚° ìˆ˜ì¶œ ì¦ê°€ ì¶”ì„¸ë¡œ ê´€ë ¨ ê¸°ì—…ë“¤ì´ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤
- **íˆ¬ì ì°¸ê³ **: ì‹¤ì‹œê°„ ì •ë³´ëŠ” ë³„ë„ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤
- **ì„œë¹„ìŠ¤ ìƒíƒœ**: ë³µêµ¬ ì¤‘ì´ë©° ê³§ ì •ìƒ ì„œë¹„ìŠ¤ë©ë‹ˆë‹¤

### ğŸ”§ ì¶”ì²œ ì¡°ì¹˜
- ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”
- ë” êµ¬ì²´ì ì¸ í‚¤ì›Œë“œë¡œ ì§ˆì˜í•´ ë³´ì„¸ìš”
- ì‹œìŠ¤í…œ ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”

**ì˜¤ë¥˜ ID**: {int(time.time())}
"""

            return {
                "query": query,
                "answer": fallback_answer,
                "sources": [],
                "graph_samples": [],
                "graph_summary": None,
                "stock": None,
                "meta": {
                    "orchestrator": "fallback",
                    "total_latency_ms": round(total_time, 2),
                    "error": str(e),
                    "services_attempted": services_attempted,
                    "system_health": error_handler.get_health_report()
                }
            }