"""
ë‰´ìŠ¤ ì¡°íšŒ ì „ìš© í•¸ë“¤ëŸ¬
ë‰´ìŠ¤ ê²€ìƒ‰ì— íŠ¹í™”ëœ ì²˜ë¦¬ ë¡œì§
"""

from typing import Dict, List, Any, Optional, Tuple
import logging
from api.services.intent_classifier import IntentResult

logger = logging.getLogger(__name__)

class NewsQueryHandler:
    """ë‰´ìŠ¤ ì¡°íšŒ ì „ìš© í•¸ë“¤ëŸ¬"""

    def __init__(self, chat_service):
        self.chat_service = chat_service
        self._last_graph_rows = []  # ë§ˆì§€ë§‰ ê²€ìƒ‰ì˜ ê·¸ë˜í”„ ê²°ê³¼ ì €ì¥

    async def handle_news_query(self, query: str, intent_result: IntentResult, tracker=None) -> Dict[str, Any]:
        """ë‰´ìŠ¤ ì§ˆì˜ ì²˜ë¦¬"""
        logger.info(f"[ë‰´ìŠ¤ ì¡°íšŒ] ì²˜ë¦¬ ì‹œì‘: {query}")

        # í‚¤ì›Œë“œ ê¸°ë°˜ ë‰´ìŠ¤ ê²€ìƒ‰
        keywords = intent_result.keywords
        if not keywords:
            keywords = [query]

        # ë‰´ìŠ¤ ê²€ìƒ‰ ìˆ˜í–‰ (graph_rowsë„ í•¨ê»˜ ê°€ì ¸ì˜´)
        news_hits = await self._search_news(keywords)
        graph_rows = self._last_graph_rows  # _search_newsì—ì„œ ì €ì¥í•œ ê·¸ë˜í”„ ê²°ê³¼

        print(f"[ë‰´ìŠ¤ ì¡°íšŒ] ê·¸ë˜í”„ ê²°ê³¼: {len(graph_rows)}ê±´")
        logger.info(f"[ë‰´ìŠ¤ ì¡°íšŒ] ê·¸ë˜í”„ ê²°ê³¼ ì‚¬ìš©: {len(graph_rows)}ê±´")

        # ê°œì„ ëœ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì‘ë‹µ ìƒì„±
        from api.services.context_answer_generator import generate_context_answer

        # ê²€ìƒ‰ ê²°ê³¼ë¥¼ í†µí•© í˜•íƒœë¡œ ë³€í™˜
        search_results = {
            "sources": [],
            "graph_samples": graph_rows[:5]  # ìƒìœ„ 5ê°œ ê·¸ë˜í”„ ìƒ˜í”Œ í¬í•¨
        }

        # ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ë¥¼ sources í˜•íƒœë¡œ ë³€í™˜ (ë””ë²„ê¹… ê°•í™”)
        logger.info(f"[ë‰´ìŠ¤ ë³€í™˜] ì…ë ¥ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜: {len(news_hits)}")

        for i, hit in enumerate(news_hits):
            logger.debug(f"[ë‰´ìŠ¤ ë³€í™˜] {i+1}ë²ˆì§¸ hit í‚¤: {list(hit.keys())}")

            source_data = hit.get("_source", {})
            logger.debug(f"[ë‰´ìŠ¤ ë³€í™˜] {i+1}ë²ˆì§¸ _source í‚¤: {list(source_data.keys()) if source_data else 'Empty'}")

            metadata = source_data.get("metadata", {})
            logger.debug(f"[ë‰´ìŠ¤ ë³€í™˜] {i+1}ë²ˆì§¸ metadata í‚¤: {list(metadata.keys()) if metadata else 'Empty'}")

            # ì œëª© ì¶”ì¶œ ì‹œë„ (ì—¬ëŸ¬ ê²½ë¡œ)
            title_from_source = source_data.get("title")
            title_from_metadata = metadata.get("title") if metadata else None
            title_from_hit_direct = hit.get("title")  # hitì—ì„œ ì§ì ‘ ì¶”ì¶œ ì‹œë„

            logger.debug(f"[ë‰´ìŠ¤ ë³€í™˜] ì œëª© ì¶”ì¶œ ì‹œë„:")
            logger.debug(f"  - source.title: {title_from_source}")
            logger.debug(f"  - metadata.title: {title_from_metadata}")
            logger.debug(f"  - hit.title: {title_from_hit_direct}")

            # ì œëª© ì¶”ì¶œ (ë‹¤ì–‘í•œ êµ¬ì¡° ì§€ì›)
            title = (
                title_from_source or
                title_from_metadata or
                title_from_hit_direct or
                "ë‰´ìŠ¤ ê¸°ì‚¬"
            )

            # URL ì¶”ì¶œ (ì—¬ëŸ¬ ê²½ë¡œ ì‹œë„)
            url = (
                source_data.get("url") or
                metadata.get("url") or
                hit.get("url") or
                ""
            )

            # ë‚ ì§œ ì¶”ì¶œ (ì—¬ëŸ¬ ê²½ë¡œ ì‹œë„)
            date = (
                metadata.get("created_date") or
                metadata.get("date") or
                source_data.get("created_date") or
                source_data.get("date") or
                hit.get("date") or
                ""
            )

            # ë¯¸ë””ì–´ ì¶”ì¶œ (ì—¬ëŸ¬ ê²½ë¡œ ì‹œë„)
            media = (
                source_data.get("media") or
                metadata.get("media") or
                hit.get("media") or
                ""
            )

            logger.debug(f"[ë‰´ìŠ¤ ë³€í™˜] ìµœì¢… ì¶”ì¶œ ê²°ê³¼:")
            logger.debug(f"  - title: {title[:50] if title else None}...")
            logger.debug(f"  - url: {url[:50] if url else None}...")
            logger.debug(f"  - media: {media}")
            logger.debug(f"  - date: {date}")

            search_results["sources"].append({
                "title": title,
                "url": url,
                "date": date,
                "media": media,
                "score": hit.get("_score", 0)
            })

        # ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë‹µë³€ ìƒì„±
        context_answer = generate_context_answer(
            query=query,
            intent="news_inquiry",
            search_results=search_results,
            entities=intent_result.extracted_entities
        )

        response = {
            "type": "news_inquiry",
            "markdown": context_answer,
            "news_count": len(news_hits),
            "sources": search_results["sources"],
            "entities": intent_result.extracted_entities,
            "meta": {
                "query": query,
                "search_type": "context_enhanced",
                "total_hits": len(news_hits),
                "graph_samples_shown": len(graph_rows)  # ê·¸ë˜í”„ ìƒ˜í”Œ ìˆ˜ í¬í•¨
            }
        }

        logger.info(f"[ë‰´ìŠ¤ ì¡°íšŒ] ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì‘ë‹µ ìƒì„± ì™„ë£Œ: {len(news_hits)}ê±´")
        return response

    async def _search_news(self, keywords: List[str]) -> List[Dict[str, Any]]:
        """ë‰´ìŠ¤ ê²€ìƒ‰ (ì¤‘ë³µ ì œê±° ë° ìµœì‹ ìˆœ)"""
        try:
            # í‚¤ì›Œë“œ ì •ì œ: ë‰´ìŠ¤ ê²€ìƒ‰ì— íŠ¹í™”ëœ í•„í„°ë§
            print(f"[ë‰´ìŠ¤ ê²€ìƒ‰] ì…ë ¥ í‚¤ì›Œë“œ: {keywords}")
            refined_keywords = self._refine_news_keywords(keywords)
            print(f"[ë‰´ìŠ¤ ê²€ìƒ‰] ì •ì œëœ í‚¤ì›Œë“œ: {refined_keywords}")

            if not refined_keywords:
                logger.warning("ë‰´ìŠ¤ ê²€ìƒ‰ìš© í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤")
                return []

            # ë‰´ìŠ¤ ì „ìš© ìµœì í™”ëœ ê²€ìƒ‰ ìˆ˜í–‰
            # í•µì‹¬ í‚¤ì›Œë“œ ìš°ì„  + ë¶€ê°€ í‚¤ì›Œë“œëŠ” ì„ íƒì ìœ¼ë¡œ
            # ì˜ˆ: "2ì°¨ì „ì§€ ìˆ˜ì£¼ ê¸°ì—…" ë³´ë‹¤ëŠ” "2ì°¨ì „ì§€" ìœ„ì£¼ë¡œ ê²€ìƒ‰í•˜ë˜ "ìˆ˜ì£¼"ë¥¼ í¬í•¨í•œ ê²°ê³¼ ìš°ì„ 
            primary_keywords = [kw for kw in refined_keywords if len(kw) > 2][:2]  # í•µì‹¬ í‚¤ì›Œë“œ ìµœëŒ€ 2ê°œ
            search_query = " ".join(primary_keywords) if primary_keywords else " ".join(refined_keywords[:2])
            print(f"[ë‰´ìŠ¤ ê²€ìƒ‰] ê²€ìƒ‰ì–´: '{search_query}' (ì›ë³¸: {refined_keywords})")
            logger.info(f"[ë‰´ìŠ¤ ê²€ìƒ‰] ê²€ìƒ‰ì–´: '{search_query}'")

            # Neo4j ê·¸ë˜í”„ ê²€ìƒ‰ í¬í•¨ (search_parallel ì‚¬ìš©)
            print(f"[ë‰´ìŠ¤ ê²€ìƒ‰] search_parallel í˜¸ì¶œ ì‹œì‘")
            logger.info(f"[ë‰´ìŠ¤ ê²€ìƒ‰] search_parallel í˜¸ì¶œ (Neo4j ê·¸ë˜í”„ + OpenSearch)")
            news_hits, graph_rows, _, search_time, graph_time, news_time = await self.chat_service.search_parallel(
                search_query,
                size=25
            )

            print(f"[ë‰´ìŠ¤ ê²€ìƒ‰] search_parallel ì™„ë£Œ: ë‰´ìŠ¤ {len(news_hits)}ê±´, ê·¸ë˜í”„ {len(graph_rows)}ê±´")
            logger.info(f"[ë‰´ìŠ¤ ê²€ìƒ‰] ê²°ê³¼: ë‰´ìŠ¤ {len(news_hits)}ê±´, ê·¸ë˜í”„ {len(graph_rows)}ê±´")
            logger.info(f"[ë‰´ìŠ¤ ê²€ìƒ‰] ì‹œê°„: ê²€ìƒ‰ {search_time:.0f}ms, ê·¸ë˜í”„ {graph_time:.0f}ms, ë‰´ìŠ¤ {news_time:.0f}ms")

            # search_parallel ê²°ê³¼ë¥¼ ê¸°ì¡´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            hits = news_hits

            # graph_rowsë¥¼ ì €ì¥í•˜ì—¬ ë°˜í™˜ (ì¤‘ìš”!)
            self._last_graph_rows = graph_rows

            # ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€) - ì§ì ‘ í•„ë“œ ì‚¬ìš©
            seen_urls = set()
            unique_hits = []
            for hit in hits:
                url = hit.get("url", "")
                title = hit.get("title", "").lower()

                # URL ì¤‘ë³µ ì œê±° + ì œëª© ê´€ë ¨ì„± ì²´í¬
                if url and url not in seen_urls:
                    # ê²€ìƒ‰ì–´ì™€ ì œëª© ê´€ë ¨ì„± ê²€ì¦ (ê°„ë‹¨í•œ í•„í„°ë§)
                    if self._is_relevant_news(title, refined_keywords):
                        seen_urls.add(url)
                        unique_hits.append(hit)

            logger.info(f"[ë‰´ìŠ¤ ê²€ìƒ‰] ê²°ê³¼: {len(unique_hits)}ê±´")
            return unique_hits

        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    async def _format_news_response(self, query: str, news_hits: List[Dict[str, Any]],
                                  entities: Dict[str, List[str]]) -> Dict[str, Any]:
        """ë‰´ìŠ¤ ì¤‘ì‹¬ ì‘ë‹µ í¬ë§·íŒ…"""

        # ë‰´ìŠ¤ ëª©ë¡ ì •ë¦¬ (ê°œì„ ëœ ë¡œì§)
        formatted_news = []

        logger.info(f"[ë‰´ìŠ¤ í¬ë§·íŒ…] ì…ë ¥ëœ ë‰´ìŠ¤ ê±´ìˆ˜: {len(news_hits)}")

        for i, hit in enumerate(news_hits[:10]):  # ìµœëŒ€ 10ê°œ
            logger.debug(f"[ë‰´ìŠ¤ í¬ë§·íŒ…] {i+1}ë²ˆì§¸ ë‰´ìŠ¤ ì²˜ë¦¬: {hit.get('title', 'no title')[:50]}...")

            # ê²€ìƒ‰ ê²°ê³¼ëŠ” ì´ë¯¸ ì§ì ‘ í•„ë“œ í˜•íƒœë¡œ ì œê³µë¨ (_source ë˜í•‘ ì—†ìŒ)
            title = hit.get("title", "ì œëª© ì—†ìŒ")
            url = hit.get("url", "")
            date = hit.get("date", "")  # date í•„ë“œ ì§ì ‘ ì‚¬ìš©
            media = hit.get("media", "")

            # ì™„í™”ëœ ìœ íš¨ì„± ê²€ì‚¬: ì œëª©ì´ ìˆê±°ë‚˜ URLì´ ìˆìœ¼ë©´ í¬í•¨
            if (title and title.strip() and len(title.strip()) > 3) or url:
                # ì œëª©ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ì œëª© ìƒì„±
                if not title or title == "ì œëª© ì—†ìŒ" or len(title.strip()) <= 3:
                    title = f"ë‰´ìŠ¤ ê¸°ì‚¬ #{len(formatted_news) + 1}"

                formatted_news.append({
                    "title": title,
                    "url": url,
                    "date": date,
                    "media": media,
                    "score": hit.get("_score", 0)
                })
                logger.info(f"[ë‰´ìŠ¤ í¬ë§·íŒ…] ì¶”ê°€ëœ ë‰´ìŠ¤: {title[:50]}...")
            else:
                logger.warning(f"[ë‰´ìŠ¤ í¬ë§·íŒ…] ìœ íš¨í•˜ì§€ ì•Šì€ ë‰´ìŠ¤: title='{title}', url='{url}'")

        # ì£¼ìš” í…Œë§ˆ/ì¢…ëª© ì •ë³´
        theme_info = self._extract_theme_info(entities)

        # ë§ˆí¬ë‹¤ìš´ ì‘ë‹µ ìƒì„±
        markdown_sections = []

        # í—¤ë”
        entity_str = ""
        if entities.get("company"):
            entity_str = f" - {', '.join(entities['company'])}"
        elif entities.get("theme"):
            entity_str = f" - {', '.join(entities['theme'])}"

        markdown_sections.append(f"## ğŸ“° ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼{entity_str}")
        markdown_sections.append("")

        # ë‰´ìŠ¤ ëª©ë¡ (ê°œì„ ëœ ë¡œì§)
        logger.info(f"[ë‰´ìŠ¤ ì‘ë‹µ] formatted_news ê°œìˆ˜: {len(formatted_news)}")

        if formatted_news:
            markdown_sections.append("### ğŸ“° ì£¼ìš” ë‰´ìŠ¤")
            for i, news in enumerate(formatted_news, 1):
                date_str = news['date'][:10] if news['date'] else ""

                # ì œëª© ì²˜ë¦¬
                title = news['title'] or "ì œëª© ì •ë³´ ì—†ìŒ"
                markdown_sections.append(f"{i}. **{title}**")

                # ë©”íƒ€ ì •ë³´
                meta_parts = []
                if news.get('media'):
                    meta_parts.append(news['media'])
                if date_str:
                    meta_parts.append(date_str)
                if meta_parts:
                    markdown_sections.append(f"   *{' | '.join(meta_parts)}*")

                # URL ë§í¬
                if news.get('url'):
                    markdown_sections.append(f"   ğŸ”— [ê¸°ì‚¬ ë³´ê¸°]({news['url']})")

                # ê´€ë ¨ë„ ì ìˆ˜ (ë””ë²„ê¹…ìš©)
                score = news.get('score', 0)
                if score > 0:
                    markdown_sections.append(f"   ğŸ“Š ê´€ë ¨ë„: {score:.2f}")

                markdown_sections.append("")

            logger.info(f"[ë‰´ìŠ¤ ì‘ë‹µ] {len(formatted_news)}ê°œ ë‰´ìŠ¤ í¬ë§·íŒ… ì™„ë£Œ")
        else:
            # ì›ë³¸ ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆì—ˆëŠ”ì§€ í™•ì¸
            logger.warning(f"[ë‰´ìŠ¤ ì‘ë‹µ] í¬ë§·íŒ…ëœ ë‰´ìŠ¤ ì—†ìŒ. ì›ë³¸ ê²€ìƒ‰ ê²°ê³¼: {len(news_hits)}ê±´")

            if len(news_hits) > 0:
                markdown_sections.append("### âš ï¸ ë‰´ìŠ¤ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤")
                markdown_sections.append(f"ê²€ìƒ‰ëœ ë‰´ìŠ¤ {len(news_hits)}ê±´ì´ ìˆì§€ë§Œ í‘œì‹œ í˜•ì‹ ë³€í™˜ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                markdown_sections.append("ì‹œìŠ¤í…œ ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”.")
            else:
                markdown_sections.append("### âš ï¸ ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                markdown_sections.append("ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ë‹¤ì‹œ ê²€ìƒ‰í•´ë³´ì„¸ìš”.")

        # ê´€ë ¨ ì •ë³´ (í…Œë§ˆ/ì¢…ëª©ì´ ìˆëŠ” ê²½ìš°)
        if theme_info:
            markdown_sections.append("### ğŸ“Š ê´€ë ¨ ì •ë³´")
            markdown_sections.extend(theme_info)

        return {
            "type": "news_inquiry",
            "markdown": "\\n".join(markdown_sections),
            "news_count": len(formatted_news),
            "sources": formatted_news,
            "entities": entities,
            "meta": {
                "query": query,
                "search_type": "news_focused",
                "total_hits": len(news_hits)
            }
        }

    def _extract_theme_info(self, entities: Dict[str, List[str]]) -> List[str]:
        """í…Œë§ˆ/ì¢…ëª© ê´€ë ¨ ì •ë³´ ì¶”ì¶œ"""
        info_lines = []

        # ì¢…ëª© ì •ë³´
        if entities.get("company"):
            companies = entities["company"][:3]  # ìµœëŒ€ 3ê°œ
            info_lines.append("**ê´€ë ¨ ì¢…ëª©:**")
            for company in companies:
                info_lines.append(f"- {company}")
            info_lines.append("")

        # í…Œë§ˆ ì •ë³´
        if entities.get("theme"):
            themes = entities["theme"][:3]  # ìµœëŒ€ 3ê°œ
            info_lines.append("**ê´€ë ¨ í…Œë§ˆ:**")
            for theme in themes:
                info_lines.append(f"- {theme}")
            info_lines.append("")

        return info_lines

    def _refine_news_keywords(self, keywords: List[str]) -> List[str]:
        """ë‰´ìŠ¤ ê²€ìƒ‰ì— íŠ¹í™”ëœ í‚¤ì›Œë“œ ì •ì œ"""
        from api.config.keyword_mappings import STOPWORDS

        # í™•ì¥ëœ stopwords (ë‰´ìŠ¤ ê²€ìƒ‰ íŠ¹í™”)
        # ì£¼ì˜: 'í˜„í™©', 'ìˆ˜ì£¼', 'ì‹¤ì ' ê°™ì€ ë¹„ì¦ˆë‹ˆìŠ¤ ìš©ì–´ëŠ” ì œê±°í•˜ì§€ ì•ŠìŒ!
        news_stopwords = STOPWORDS | {
            'ë‰´ìŠ¤', 'ê¸°ì‚¬', 'ì†Œì‹', 'ì •ë³´', 'ë‚´ìš©', 'ìë£Œ',
            'ë³´ì—¬ì¤˜', 'ì•Œë ¤ì¤˜', 'ë§í•´ì¤˜', 'í•´ì¤˜', 'ì°¾ì•„ì¤˜', 'ê²€ìƒ‰í•´ì¤˜',
            'ê´€ë ¨', 'ëŒ€í•œ', 'ê´€í•´ì„œ', 'ì—ì„œ', 'ìœ¼ë¡œ', 'ë¡œì„œ', 'ì—ê²Œ',
            'ëŠ”', 'ì€', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì˜', 'ì—', 'ë¡œ', 'ìœ¼ë¡œ',
            'ìˆëŠ”', 'ì—†ëŠ”', 'ê°™ì€', 'ë‹¤ë¥¸', 'ê·¸ëŸ°', 'ì´ëŸ°',
            'ì£¼ìš”', 'ìµœê·¼', 'ì˜¤ëŠ˜', 'ì–´ì œ', 'ìš”ì¦˜',
            'ì¢€', 'ë”', 'ë§ì´', 'ì˜', 'ë¹¨ë¦¬',
            'ê°œì›”', 'ê°œì›”ê°„', 'ë“¤ì˜', 'ì¸ê°€', 'ì–´ë””'
        }

        refined = []
        for keyword in keywords:
            # íŠœí”Œì´ë‚˜ ë¦¬ìŠ¤íŠ¸ê°€ ë“¤ì–´ì˜¨ ê²½ìš° ì²˜ë¦¬ (ë°©ì–´ ì½”ë“œ)
            if isinstance(keyword, (tuple, list)):
                # íŠœí”Œ/ë¦¬ìŠ¤íŠ¸ ë‚´ë¶€ì˜ ë¬¸ìì—´ë§Œ ì¶”ì¶œ
                keyword = ''.join([k for k in keyword if isinstance(k, str) and k])

            # ë¬¸ìì—´ì´ ì•„ë‹ˆë©´ ê±´ë„ˆë›°ê¸°
            if not isinstance(keyword, str):
                continue

            if keyword and len(keyword) > 1:
                # stopwords í•„í„°ë§
                if keyword.lower() not in news_stopwords:
                    # ìˆ«ìë§Œ ìˆëŠ” í‚¤ì›Œë“œ ì œì™¸ (1ìë¦¬ë“  2ìë¦¬ë“  ëª¨ë‘)
                    if not keyword.isdigit():
                        # ë„ˆë¬´ ì§§ì€ í‚¤ì›Œë“œ ì œì™¸ (ë‹¨, 2ê¸€ì ì´ìƒ í•œê¸€/ì˜ë¬¸ì€ í—ˆìš©)
                        if len(keyword) >= 2 or (len(keyword) == 1 and keyword.isalpha()):
                            refined.append(keyword)

        return refined[:5]  # ìµœëŒ€ 5ê°œ í‚¤ì›Œë“œë§Œ ì‚¬ìš©

    def _is_relevant_news(self, title: str, keywords: List[str]) -> bool:
        """ë‰´ìŠ¤ ì œëª©ê³¼ í‚¤ì›Œë“œì˜ ê´€ë ¨ì„± ì²´í¬"""
        if not title or not keywords:
            return True  # ê¸°ë³¸ì ìœ¼ë¡œ í†µê³¼

        title_lower = title.lower()

        # ì£¼ìš” í‚¤ì›Œë“œ ì¤‘ í•˜ë‚˜ë¼ë„ ì œëª©ì— í¬í•¨ë˜ì–´ì•¼ í•¨
        for keyword in keywords:
            keyword_lower = keyword.lower()
            if keyword_lower in title_lower:
                return True

            # ë¶€ë¶„ ë§¤ì¹­ë„ ì²´í¬ (2ê¸€ì ì´ìƒ)
            if len(keyword_lower) >= 2:
                if keyword_lower[:2] in title_lower or keyword_lower[-2:] in title_lower:
                    return True

        # í‚¤ì›Œë“œê°€ ì „í˜€ ë§¤ì¹­ë˜ì§€ ì•Šìœ¼ë©´ ê´€ë ¨ì„± ë‚®ìŒìœ¼ë¡œ íŒë‹¨
        logger.debug(f"[ê´€ë ¨ì„± ì²´í¬] ë‚®ì€ ê´€ë ¨ì„±: '{title}' vs {keywords}")
        return False

    async def _fast_news_search(self, query: str, size: int = 10) -> Tuple[List[Dict[str, Any]], float, Optional[str]]:
        """ë‰´ìŠ¤ ì „ìš© ê³ ì† ê²€ìƒ‰ (ì˜¨í†¨ë¡œì§€ í™•ì¥ ì—†ì´ ì§ì ‘ ê²€ìƒ‰)"""
        import time
        t0 = time.perf_counter()

        try:
            from api.config import settings

            # ì§ì ‘ OpenSearch ê²€ìƒ‰ (ì˜¨í†¨ë¡œì§€ ì²˜ë¦¬ ìƒëµ)
            os_index = settings.news_embedding_index

            # ìµœì í™”ëœ ì¿¼ë¦¬ êµ¬ì¡° (metadata êµ¬ì¡°ì— ë§ì¶° ìˆ˜ì •)
            body = {
                "query": {
                    "bool": {
                        "should": [
                            {
                                "multi_match": {
                                    "query": query,
                                    "fields": ["metadata.title^4", "metadata.content^2", "text^2"],
                                    "type": "best_fields",
                                    "operator": "or"
                                }
                            }
                        ],
                        "minimum_should_match": 1
                    }
                },
                "sort": [
                    {"_score": {"order": "desc"}}
                ],
                "_source": {
                    "includes": ["metadata.title", "metadata.url", "metadata.media", "metadata.portal", "metadata.date", "metadata.content", "text"]
                },
                "size": size
            }

            result = await self.chat_service.os.search(
                index=os_index,
                query=body,
                size=size
            )

            if result and result.get("hits"):
                hits = result["hits"].get("hits", [])

                # ê°„ë‹¨í•œ í¬ë§·íŒ…
                formatted_hits = []
                for hit in hits:
                    source = hit.get("_source", {})
                    if source.get("title") and source.get("url"):  # í•„ìˆ˜ í•„ë“œê°€ ìˆëŠ” ê²ƒë§Œ
                        formatted_hits.append(hit)

                search_time_ms = (time.perf_counter() - t0) * 1000
                logger.info(f"[ê³ ì† ë‰´ìŠ¤ ê²€ìƒ‰] ì™„ë£Œ: {len(formatted_hits)}ê±´, {search_time_ms:.1f}ms")
                return formatted_hits, search_time_ms, None
            else:
                return [], (time.perf_counter() - t0) * 1000, "No results found"

        except Exception as e:
            logger.error(f"ê³ ì† ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            search_time_ms = (time.perf_counter() - t0) * 1000
            return [], search_time_ms, str(e)

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ëŠ” chat_serviceê°€ í•„ìš”í•˜ë¯€ë¡œ ë‚˜ì¤‘ì— ì´ˆê¸°í™”