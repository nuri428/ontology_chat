"""
ì§ˆì˜ ë¼ìš°íŒ… ì‹œìŠ¤í…œ
ì‚¬ìš©ì ì§ˆì˜ë¥¼ ì˜ë„ë³„ë¡œ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ í•¸ë“¤ëŸ¬ë¡œ ë¼ìš°íŒ…
"""

from typing import Dict, Any, List
import logging
import time
import asyncio

from api.services.intent_classifier import classify_query_intent, QueryIntent
from api.services.news_handler import NewsQueryHandler
from api.services.stock_analysis_handler import StockAnalysisHandler
from api.services.context_answer_generator import generate_context_answer

# ëª¨ë‹ˆí„°ë§ ë° íŠ¸ë ˆì´ì‹±
try:
    from api.monitoring.metrics_collector import track_query, query_metrics, session_manager, QueryTracker
    MONITORING_AVAILABLE = True
except ImportError:
    MONITORING_AVAILABLE = False
    def track_query(*args, **kwargs):
        from contextlib import nullcontext
        return nullcontext()

logger = logging.getLogger(__name__)

class QueryRouter:
    """ì§ˆì˜ ë¼ìš°íŒ… ë° ì²˜ë¦¬ ê´€ë¦¬ì (í•˜ì´ë¸Œë¦¬ë“œ: ë‹¨ìˆœ í•¸ë“¤ëŸ¬ + Multi-Agent LangGraph)"""

    def __init__(self, chat_service, response_formatter, langgraph_engine=None):
        self.chat_service = chat_service
        self.response_formatter = response_formatter
        self.langgraph_engine = langgraph_engine  # LangGraph Multi-Agent ì—”ì§„

        # í•¸ë“¤ëŸ¬ ì´ˆê¸°í™”
        self.news_handler = NewsQueryHandler(chat_service)
        self.stock_handler = StockAnalysisHandler(chat_service, response_formatter)

    async def process_query(self, query: str, user_id: str = "anonymous", session_id: str = None, force_deep_analysis: bool = False) -> Dict[str, Any]:
        """ì§ˆì˜ ì²˜ë¦¬ ë©”ì¸ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸ (í•˜ì´ë¸Œë¦¬ë“œ ë¼ìš°íŒ…: ë‹¨ìˆœ/ë³µì¡ ìë™ íŒë‹¨)"""
        start_time = time.time()

        # ì„¸ì…˜ ê´€ë¦¬
        if MONITORING_AVAILABLE and session_id:
            session_manager.record_query(session_id)

        # ì§ˆì˜ ì¶”ì  ì‹œì‘
        async with track_query(query, user_id, session_id) as tracker:
            logger.info(f"[ë¼ìš°í„°] ì§ˆì˜ ì²˜ë¦¬ ì‹œì‘: {query}")

            try:
                # 1. ì˜ë„ ë¶„ì„
                tracker.start_stage("intent_classification")
                intent_result = classify_query_intent(query)
                tracker.end_stage("intent_classification", {
                    "intent": intent_result.intent.value,
                    "confidence": intent_result.confidence
                })

                # íŠ¸ë ˆì´ì»¤ì— ì˜ë„ ì„¤ì •
                tracker.set_intent(intent_result.intent.value, intent_result.confidence)

                logger.info(f"[ë¼ìš°í„°] ì˜ë„ ë¶„ì„: {intent_result.intent.value} (ì‹ ë¢°ë„: {intent_result.confidence:.2f})")

                # 2. ë³µì¡ë„ íŒë‹¨ (í•˜ì´ë¸Œë¦¬ë“œ ë¼ìš°íŒ…)
                complexity_score = self._analyze_query_complexity(query, intent_result)
                requires_deep = self._requires_deep_analysis(query)

                logger.info(f"[ë¼ìš°í„°] ë³µì¡ë„: {complexity_score:.2f}, ì‹¬ì¸µë¶„ì„ í•„ìš”: {requires_deep}, ê°•ì œ: {force_deep_analysis}")

                # 3. ë¼ìš°íŒ… ê²°ì •
                tracker.start_stage("query_processing")

                # ë³µì¡í•œ ì§ˆë¬¸ì´ê±°ë‚˜ ëª…ì‹œì  ìš”ì²­ â†’ LangGraph Multi-Agent
                # ì„ê³„ê°’ 0.7 â†’ 0.85ë¡œ ìƒí–¥ ì¡°ì • (ë” ë§ì€ ì§ˆì˜ë¥¼ ë¹ ë¥¸ í•¸ë“¤ëŸ¬ë¡œ ì²˜ë¦¬)
                if force_deep_analysis or complexity_score >= 0.85 or requires_deep:
                    logger.info(f"[ë¼ìš°í„°] â†’ LangGraph Multi-Agent ì‚¬ìš©")
                    response = await self._route_to_langgraph(query, intent_result, tracker, complexity_score, force_deep=force_deep_analysis)

                # ë‹¨ìˆœ ì§ˆë¬¸ â†’ ê¸°ì¡´ ë¹ ë¥¸ í•¸ë“¤ëŸ¬
                elif intent_result.intent == QueryIntent.NEWS_INQUIRY:
                    logger.info(f"[ë¼ìš°í„°] â†’ ë¹ ë¥¸ ë‰´ìŠ¤ í•¸ë“¤ëŸ¬")
                    response = await self.news_handler.handle_news_query(query, intent_result, tracker)

                elif intent_result.intent == QueryIntent.STOCK_ANALYSIS:
                    logger.info(f"[ë¼ìš°í„°] â†’ ë¹ ë¥¸ ì£¼ì‹ ë¶„ì„ í•¸ë“¤ëŸ¬")
                    response = await self.stock_handler.handle_stock_query(query, intent_result, tracker)

                elif intent_result.intent == QueryIntent.GENERAL_QA:
                    logger.info(f"[ë¼ìš°í„°] â†’ ì¼ë°˜ QA í•¸ë“¤ëŸ¬")
                    response = await self._handle_general_qa(query, intent_result, tracker)

                else:
                    # UNKNOWNì´ê±°ë‚˜ ë¶„ë¥˜ ì‹¤íŒ¨í•œ ê²½ìš° - ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
                    logger.info(f"[ë¼ìš°í„°] â†’ í´ë°± í•¸ë“¤ëŸ¬")
                    response = await self._handle_fallback(query, tracker)

                tracker.end_stage("query_processing", {
                    "response_type": response.get("type"),
                    "response_length": len(response.get("markdown", ""))
                })

                # 3. ê³µí†µ ë©”íƒ€ë°ì´í„° ì¶”ê°€
                processing_time = (time.time() - start_time) * 1000
                response["meta"] = response.get("meta", {})
                response["meta"].update({
                    "processing_time_ms": processing_time,
                    "intent": intent_result.intent.value,
                    "confidence": intent_result.confidence,
                    "reasoning": intent_result.reasoning,
                    "router_version": "1.0",
                    "user_id": user_id,
                    "session_id": session_id
                })

                # í’ˆì§ˆ ë©”íŠ¸ë¦­ ê¸°ë¡
                if MONITORING_AVAILABLE:
                    quality_score = min(1.0, intent_result.confidence * 0.7 + 0.3)
                    tracker.record_quality_metrics({
                        "intent_confidence": intent_result.confidence,
                        "overall_quality": quality_score
                    })

                # íŠ¸ë˜ì»¤ì— ì‘ë‹µ ì„¤ì •
                if MONITORING_AVAILABLE and hasattr(tracker, 'set_response'):
                    tracker.set_response(response)

                logger.info(f"[ë¼ìš°í„°] ì²˜ë¦¬ ì™„ë£Œ: {processing_time:.1f}ms")
                return response

            except Exception as e:
                logger.error(f"[ë¼ìš°í„°] ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                # íŠ¸ë˜ì»¤ì— ì˜¤ë¥˜ ê¸°ë¡
                tracker.record_error("processing_error", "query_router", str(e))
                # ì˜¤ë¥˜ ë°œìƒì‹œ í´ë°±
                return await self._handle_error_fallback(query, str(e), tracker)

    async def _handle_general_qa(self, query: str, intent_result, tracker: QueryTracker = None) -> Dict[str, Any]:
        """ì¼ë°˜ ì§ˆë¬¸ ì²˜ë¦¬"""
        logger.info(f"[ì¼ë°˜ QA] ì²˜ë¦¬: {query}")

        if tracker:
            tracker.start_stage("general_qa_processing")

        # ê°„ë‹¨í•œ ì¼ë°˜ ì§ˆë¬¸ ì‘ë‹µ ìƒì„±
        markdown_sections = []

        if any(word in query.lower() for word in ["per", "pbr", "roe", "ë°°ë‹¹"]):
            # ê¸ˆìœµ ìš©ì–´ ì§ˆë¬¸
            markdown_sections.extend(self._generate_financial_term_explanation(query))
        else:
            # ê¸°íƒ€ ì¼ë°˜ ì§ˆë¬¸
            markdown_sections.extend(self._generate_general_response(query))

        if tracker:
            tracker.end_stage("general_qa_processing", {
                "qa_type": "financial_terms" if any(word in query.lower() for word in ["per", "pbr", "roe", "ë°°ë‹¹"]) else "general",
                "sections_count": len(markdown_sections)
            })

        return {
            "type": "general_qa",
            "markdown": "\\n".join(markdown_sections),
            "meta": {
                "query": query,
                "analysis_type": "general_qa"
            }
        }

    async def _handle_fallback(self, query: str, tracker: QueryTracker = None) -> Dict[str, Any]:
        """í´ë°± ì²˜ë¦¬ - ê¸°ì¡´ ë°©ì‹ ì‚¬ìš© (ì¬ê·€ ë°©ì§€ë¥¼ ìœ„í•´ _generate_answer_legacy ì§ì ‘ í˜¸ì¶œ)"""
        logger.info(f"[í´ë°±] ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬: {query}")

        if tracker:
            tracker.start_stage("fallback_processing")

        try:
            # ì¬ê·€ ë°©ì§€: _generate_answer_legacy ì§ì ‘ í˜¸ì¶œ
            result = await self.chat_service._generate_answer_legacy(query)

            # ê²°ê³¼ í˜•íƒœ í†µì¼
            if isinstance(result, dict) and "answer" in result:
                # meta ì •ë³´ ë³‘í•© (graph_samples_shown ë“± í¬í•¨)
                result_meta = result.get("meta", {})
                combined_meta = {
                    "query": query,
                    "analysis_type": "fallback",
                    "fallback_reason": "intent_classification_failed",
                    "graph_samples_shown": result_meta.get("graph_samples_shown", 0),
                    "total_latency_ms": result_meta.get("total_latency_ms", 0),
                    **result_meta  # ê¸°ì¡´ meta ì •ë³´ë„ í¬í•¨
                }

                return {
                    "type": "fallback",
                    "markdown": result["answer"],
                    "sources": result.get("sources", []),
                    "graph_samples": result.get("graph_samples", []),  # ê·¸ë˜í”„ ìƒ˜í”Œ ì¶”ê°€
                    "meta": combined_meta
                }
            else:
                return {
                    "type": "fallback",
                    "markdown": str(result),
                    "meta": {
                        "query": query,
                        "analysis_type": "fallback"
                    }
                }
        except Exception as e:
            logger.error(f"í´ë°± ì²˜ë¦¬ë„ ì‹¤íŒ¨: {e}")
            if tracker:
                tracker.record_error("fallback_error", "fallback_processing", str(e))
            return await self._handle_error_fallback(query, str(e), tracker)
        finally:
            if tracker:
                tracker.end_stage("fallback_processing")

    async def _handle_fallback_fast(self, query: str, intent_result, tracker: QueryTracker = None) -> Dict[str, Any]:
        """ë¹ ë¥¸ í´ë°± ì²˜ë¦¬ - LangGraph íƒ€ì„ì•„ì›ƒ ì‹œ ì‚¬ìš© (ì˜ë„ ê¸°ë°˜ ë¼ìš°íŒ…)"""
        logger.info(f"[ë¹ ë¥¸ í´ë°±] ì˜ë„ ê¸°ë°˜ ë¹ ë¥¸ ì²˜ë¦¬: {query} (ì˜ë„: {intent_result.intent.value})")

        if tracker:
            tracker.start_stage("fast_fallback_processing")

        try:
            # ì˜ë„ì— ë”°ë¼ ì ì ˆí•œ ë¹ ë¥¸ í•¸ë“¤ëŸ¬ ì‚¬ìš©
            if intent_result.intent == QueryIntent.NEWS_INQUIRY:
                response = await self.news_handler.handle_news_query(query, intent_result, tracker)
            elif intent_result.intent == QueryIntent.STOCK_ANALYSIS:
                response = await self.stock_handler.handle_stock_query(query, intent_result, tracker)
            else:
                # ê¸°ë³¸ í´ë°±
                response = await self._handle_fallback(query, tracker)

            # íƒ€ì„ì•„ì›ƒ ê²½ê³  ì¶”ê°€
            response["meta"] = response.get("meta", {})
            response["meta"]["langgraph_timeout"] = True
            response["meta"]["fallback_reason"] = "langgraph_timeout"

            if tracker:
                tracker.end_stage("fast_fallback_processing")

            return response

        except Exception as e:
            logger.error(f"ë¹ ë¥¸ í´ë°± ì²˜ë¦¬ë„ ì‹¤íŒ¨: {e}")
            if tracker:
                tracker.record_error("fast_fallback_error", "fast_fallback_processing", str(e))
                tracker.end_stage("fast_fallback_processing")
            return await self._handle_error_fallback(query, str(e), tracker)

    async def _handle_error_fallback(self, query: str, error_msg: str, tracker: QueryTracker = None) -> Dict[str, Any]:
        """ì˜¤ë¥˜ ë°œìƒì‹œ ìµœì¢… í´ë°±"""
        return {
            "type": "error",
            "markdown": f"""## âš ï¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤

ì£„ì†¡í•©ë‹ˆë‹¤. ìš”ì²­ì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.

**ìš”ì²­**: {query}

ë‹¤ì‹œ ì‹œë„í•´ì£¼ì‹œê±°ë‚˜, ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”.

**ì˜ˆì‹œ:**
- "ì‚¼ì„±ì „ì ê´€ë ¨ ë‰´ìŠ¤ ë³´ì—¬ì¤˜"
- "ë°©ì‚° ìœ ë§ì£¼ ì¶”ì²œí•´ì¤˜"
- "PERì´ ë­ì•¼?"
""",
            "meta": {
                "query": query,
                "error": error_msg,
                "analysis_type": "error"
            }
        }

    def _generate_financial_term_explanation(self, query: str) -> List[str]:
        """ê¸ˆìœµ ìš©ì–´ ì„¤ëª… ìƒì„±"""
        sections = []

        sections.append("## ğŸ“š ê¸ˆìœµ ìš©ì–´ ì„¤ëª…")
        sections.append("")

        query_lower = query.lower()

        if "per" in query_lower:
            sections.extend([
                "### PER (Price Earning Ratio)",
                "- **ì •ì˜**: ì£¼ê°€ìˆ˜ìµë¹„ìœ¨, ì£¼ê°€ë¥¼ ì£¼ë‹¹ìˆœì´ìµ(EPS)ìœ¼ë¡œ ë‚˜ëˆˆ ê°’",
                "- **í•´ì„**: ë‚®ì„ìˆ˜ë¡ ì €í‰ê°€, ë†’ì„ìˆ˜ë¡ ê³ í‰ê°€ (ì—…ì¢…ë³„ ì°¨ì´ ê³ ë ¤ í•„ìš”)",
                "- **í™œìš©**: ê°™ì€ ì—…ì¢… ë‚´ ê¸°ì—… ë¹„êµë‚˜ ê³¼ê±° PERê³¼ ë¹„êµì‹œ ìœ ìš©",
                ""
            ])

        if "pbr" in query_lower:
            sections.extend([
                "### PBR (Price Book-value Ratio)",
                "- **ì •ì˜**: ì£¼ê°€ìˆœìì‚°ë¹„ìœ¨, ì£¼ê°€ë¥¼ ì£¼ë‹¹ìˆœìì‚°(BPS)ìœ¼ë¡œ ë‚˜ëˆˆ ê°’",
                "- **í•´ì„**: 1 ë¯¸ë§Œì´ë©´ ì²­ì‚°ê°€ì¹˜ ëŒ€ë¹„ ì €í‰ê°€",
                "- **í•œê³„**: ìì‚°ì˜ ì‹¤ì œ ê°€ì¹˜ì™€ ì¥ë¶€ê°€ì¹˜ ê°„ ì°¨ì´ ì¡´ì¬",
                ""
            ])

        if "roe" in query_lower:
            sections.extend([
                "### ROE (Return On Equity)",
                "- **ì •ì˜**: ìê¸°ìë³¸ì´ìµë¥ , ë‹¹ê¸°ìˆœì´ìµì„ ìê¸°ìë³¸ìœ¼ë¡œ ë‚˜ëˆˆ ê°’",
                "- **í•´ì„**: ê¸°ì—…ì´ ìê¸°ìë³¸ì„ í™œìš©í•´ ì–¼ë§ˆë‚˜ íš¨ìœ¨ì ìœ¼ë¡œ ì´ìµì„ ì°½ì¶œí•˜ëŠ”ì§€ ì¸¡ì •",
                "- **ê¸°ì¤€**: ì¼ë°˜ì ìœ¼ë¡œ 10% ì´ìƒì´ë©´ ì–‘í˜¸",
                ""
            ])

        if "ë°°ë‹¹" in query_lower:
            sections.extend([
                "### ë°°ë‹¹ ê´€ë ¨ ì§€í‘œ",
                "- **ë°°ë‹¹ìˆ˜ìµë¥ **: ì—°ê°„ ë°°ë‹¹ê¸ˆì„ í˜„ì¬ ì£¼ê°€ë¡œ ë‚˜ëˆˆ ê°’",
                "- **ë°°ë‹¹ì„±í–¥**: ë‹¹ê¸°ìˆœì´ìµ ì¤‘ ë°°ë‹¹ìœ¼ë¡œ ì§€ê¸‰í•˜ëŠ” ë¹„ìœ¨",
                "- **ë°°ë‹¹ ì•ˆì •ì„±**: ê¾¸ì¤€í•œ ë°°ë‹¹ ì§€ê¸‰ ì´ë ¥ê³¼ í–¥í›„ ì§€ì† ê°€ëŠ¥ì„±",
                ""
            ])

        return sections

    def _generate_general_response(self, query: str) -> List[str]:
        """ì¼ë°˜ ì‘ë‹µ ìƒì„±"""
        return [
            "## ğŸ’¡ ì§ˆë¬¸ ë‹µë³€",
            "",
            f"**ì§ˆë¬¸**: {query}",
            "",
            "ë” êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ì›í•˜ì‹œë©´ ë‹¤ìŒê³¼ ê°™ì´ ì§ˆë¬¸í•´ì£¼ì„¸ìš”:",
            "",
            "**ë‰´ìŠ¤ ê´€ë ¨:**",
            '- "ì‚¼ì„±ì „ì ê´€ë ¨ ë‰´ìŠ¤ ë³´ì—¬ì¤˜"',
            '- "ë°©ì‚° ê´€ë ¨ ìµœê·¼ ì†Œì‹ì€?"',
            "",
            "**íˆ¬ì ë¶„ì„:**",
            '- "ì—ì½”í”„ë¡œ ì „ë§ ì–´ë•Œ?"',
            '- "2ì°¨ì „ì§€ ìœ ë§ì£¼ ì¶”ì²œí•´ì¤˜"',
            "",
            "**ê¸ˆìœµ ìš©ì–´:**",
            '- "PERì´ ë­ì•¼?"',
            '- "ë°°ë‹¹ìˆ˜ìµë¥  ê³„ì‚°ë²•ì€?"'
        ]

    def _analyze_query_complexity(self, query: str, intent_result) -> float:
        """ì§ˆë¬¸ ë³µì¡ë„ ì ìˆ˜ ê³„ì‚° (0.0-1.0)"""
        score = 0.0

        # 1. ê¸¸ì´ ê¸°ë°˜ ë³µì¡ë„
        if len(query) > 80:
            score += 0.3
        elif len(query) > 50:
            score += 0.2

        # 2. ë³µì¡í•œ í‚¤ì›Œë“œ ê°ì§€ (ê°œì„ : ê°€ì¤‘ì¹˜ ì¡°ì •)
        complex_keywords = [
            "ë¹„êµ", "ë¶„ì„", "ì „ë§", "íŠ¸ë Œë“œ", "ë³´ê³ ì„œ", "ì¢…í•©",
            "ì‹¬ì¸µ", "ìƒì„¸", "ìì„¸íˆ", "vs", "ëŒ€ë¹„", "ì¶”ì´",
            "ì „ëµ", "ê²½ìŸë ¥", "ë°¸ë¥˜ì²´ì¸", "í¬ì§€ì…”ë‹"  # ì¶”ê°€
        ]
        matched_keywords = sum(1 for kw in complex_keywords if kw in query)
        # í‚¤ì›Œë“œ 1ê°œ: +0.2, 2ê°œ: +0.4, 3ê°œ ì´ìƒ: +0.5
        if matched_keywords >= 3:
            score += 0.5
        elif matched_keywords >= 2:
            score += 0.4
        elif matched_keywords >= 1:
            score += 0.2

        # 3. ì˜ë„ ì‹ ë¢°ë„ ê¸°ë°˜ (ë¶ˆëª…í™•í•˜ë©´ ë³µì¡ë„ ì¦ê°€)
        if intent_result.confidence < 0.6:
            score += 0.2
        elif intent_result.confidence < 0.4:
            score += 0.3

        # 4. ë‹¤ì¤‘ ì—”í‹°í‹° ê°ì§€ (ì—¬ëŸ¬ íšŒì‚¬/ì‚°ì—… ì–¸ê¸‰)
        companies = ["ì‚¼ì„±", "LG", "SK", "í˜„ëŒ€", "í¬ìŠ¤ì½”", "ë„¤ì´ë²„", "ì¹´ì¹´ì˜¤", "ì—ì½”í”„ë¡œ", "ë§ˆì´í¬ë¡ "]
        company_count = sum(1 for company in companies if company in query)
        if company_count >= 3:
            score += 0.4  # 3ê°œ ì´ìƒì´ë©´ í™•ì‹¤íˆ ë³µì¡
        elif company_count >= 2:
            score += 0.3

        # 5. ì‹œê³„ì—´ í‚¤ì›Œë“œ ê°ì§€ (ì¶”ì´, ë³€í™” ë“±)
        temporal_keywords = ["6ê°œì›”", "3ê°œì›”", "ìµœê·¼", "ë³€í™”", "ì¶”ì´", "íšŒë³µ", "ì„±ì¥"]
        if any(kw in query for kw in temporal_keywords):
            score += 0.15

        # 6. ë¹„êµ + ë¶„ì„ ì¡°í•© ê°ì§€ (P0-1: í•µì‹¬ ê°œì„ )
        # "ë¹„êµ ë¶„ì„", "ê²½ìŸë ¥ ë¹„êµ" ê°™ì€ ê³ ë‚œì´ë„ ì§ˆì˜ ê°ì§€
        comparison_keywords = ["ë¹„êµ", "ëŒ€ë¹„", "vs", "versus", "ê²½ìŸ"]
        analysis_keywords = ["ë¶„ì„", "í‰ê°€", "ì „ë§", "ì „ëµ", "ê²½ìŸë ¥"]

        has_comparison = any(kw in query for kw in comparison_keywords)
        has_analysis = any(kw in query for kw in analysis_keywords)

        # ë¹„êµ + ë¶„ì„ ì¡°í•© = ìµœê³  ë‚œì´ë„ (comprehensive í•„ìš”)
        if has_comparison and has_analysis:
            score += 0.35  # ì¶”ê°€ ë³´ë„ˆìŠ¤ë¡œ 0.9+ ë³´ì¥
            logger.debug(f"[ë³µì¡ë„] ë¹„êµ+ë¶„ì„ ì¡°í•© ê°ì§€ â†’ +0.35 ë³´ë„ˆìŠ¤")

        return min(1.0, score)

    def _requires_deep_analysis(self, query: str) -> bool:
        """ëª…ì‹œì  ì‹¬ì¸µ ë¶„ì„ ìš”ì²­ í‚¤ì›Œë“œ ê°ì§€ (ê°œì„ )"""
        deep_keywords = [
            "ìƒì„¸íˆ", "ìì„¸íˆ", "ë³´ê³ ì„œ", "ì¢…í•© ë¶„ì„", "ë¹„êµ ë¶„ì„",
            "ì‹¬ì¸µ", "ê¹Šì´", "ì „ë¬¸ì ", "ì™„ë²½í•œ", "ì „ì²´ì ",
            "ì¶”ì´ ë¶„ì„", "ë³€í™” ì¶”ì´", "íŠ¸ë Œë“œ ë¶„ì„"  # ì¶”ê°€
        ]

        # ë‹¤ì¤‘ í‚¤ì›Œë“œ ì¡°í•© ê°ì§€ (íŠ¸ë Œë“œ + ë¹„êµ, ì¶”ì´ + ë¶„ì„ ë“±)
        has_trend = any(kw in query for kw in ["íŠ¸ë Œë“œ", "ì¶”ì´", "ë³€í™”"])
        has_analysis = any(kw in query for kw in ["ë¶„ì„", "ë¹„êµ", "ì „ëµ"])

        # íŠ¸ë Œë“œ/ì¶”ì´ + ë¶„ì„ ì¡°í•©ì€ ì‹¬ì¸µ ë¶„ì„
        if has_trend and has_analysis:
            return True

        return any(kw in query for kw in deep_keywords)

    async def _route_to_langgraph(self, query: str, intent_result, tracker, complexity_score: float, force_deep: bool = False) -> Dict[str, Any]:
        """ë³µì¡í•œ ì§ˆë¬¸ì„ LangGraph Multi-Agentë¡œ ë¼ìš°íŒ… (íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬ í¬í•¨)"""

        if not self.langgraph_engine:
            logger.warning("[ë¼ìš°í„°] LangGraph ì—”ì§„ ì—†ìŒ, í´ë°± ì‚¬ìš©")
            return await self._handle_fallback(query, tracker)

        try:
            # force_deep_analysis=true ì‹œ ë³µì¡ë„ ì ìˆ˜ ê°•ì œ ìƒí–¥
            if force_deep:
                complexity_score = max(complexity_score, 0.95)
                logger.info(f"[LangGraph] ê°•ì œ ì‹¬ì¸µ ë¶„ì„ ëª¨ë“œ í™œì„±í™” â†’ ë³µì¡ë„ ì ìˆ˜ ê°•ì œ ìƒí–¥: {complexity_score:.2f}")

            # ë³µì¡ë„ì— ë”°ë¥¸ ë¶„ì„ ê¹Šì´ ê²°ì • (ê³ í’ˆì§ˆ ìš°ì„  - íƒ€ì„ì•„ì›ƒ ì—¬ìœ  í™•ë³´)
            # P0-2: ê° depthë³„ 20-30% ì—¬ìœ  ì‹œê°„ ì¶”ê°€
            if complexity_score >= 0.9:
                analysis_depth = "comprehensive"
                timeout_seconds = 240.0  # 4ë¶„ (ê¸°ì¡´ 3ë¶„ â†’ +60ì´ˆ ì—¬ìœ , 10ë‹¨ê³„+ ì›Œí¬í”Œë¡œìš°)
            elif complexity_score >= 0.85:
                analysis_depth = "deep"
                timeout_seconds = 180.0  # 3ë¶„ (ê¸°ì¡´ 2ë¶„ â†’ +60ì´ˆ ì—¬ìœ , 8ë‹¨ê³„+ ì›Œí¬í”Œë¡œìš°)
            elif complexity_score >= 0.7:
                analysis_depth = "standard"
                timeout_seconds = 120.0  # 2ë¶„ (ê¸°ì¡´ 1.5ë¶„ â†’ +30ì´ˆ ì—¬ìœ , 6ë‹¨ê³„+ ì›Œí¬í”Œë¡œìš°)
            else:
                analysis_depth = "shallow"
                timeout_seconds = 90.0   # 1.5ë¶„ (ê¸°ì¡´ 1ë¶„ â†’ +30ì´ˆ ì—¬ìœ , 4ë‹¨ê³„+ ì›Œí¬í”Œë¡œìš°)

            logger.info(f"[LangGraph] ë¶„ì„ ê¹Šì´: {analysis_depth} (ë³µì¡ë„: {complexity_score:.2f}, íƒ€ì„ì•„ì›ƒ: {timeout_seconds}ì´ˆ)")

            if tracker:
                tracker.start_stage("langgraph_multi_agent")

            # LangGraph ì‹¤í–‰ (íƒ€ì„ì•„ì›ƒ ì ìš©)
            try:
                result = await asyncio.wait_for(
                    self.langgraph_engine.generate_langgraph_report(
                        query=query,
                        domain=None,  # ìë™ ì¶”ë¡ 
                        lookback_days=30,
                        analysis_depth=analysis_depth,
                        symbol=None
                    ),
                    timeout=timeout_seconds
                )
            except asyncio.TimeoutError:
                logger.warning(f"[LangGraph] íƒ€ì„ì•„ì›ƒ ({timeout_seconds}ì´ˆ) â†’ ë¹ ë¥¸ í•¸ë“¤ëŸ¬ë¡œ í´ë°±")

                if tracker:
                    tracker.record_error("langgraph_timeout", "langgraph_multi_agent", f"Timeout after {timeout_seconds}s")
                    tracker.end_stage("langgraph_multi_agent")

                # íƒ€ì„ì•„ì›ƒ ì‹œ ë¹ ë¥¸ í•¸ë“¤ëŸ¬ë¡œ í´ë°±
                return await self._handle_fallback_fast(query, intent_result, tracker)

            if tracker:
                tracker.end_stage("langgraph_multi_agent", {
                    "analysis_depth": analysis_depth,
                    "quality_score": result.get("quality_score", 0),
                    "contexts_count": result.get("contexts_count", 0),
                    "insights_count": result.get("insights_count", 0)
                })

            return {
                "type": "langgraph_analysis",
                "markdown": result.get("markdown", "ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨"),
                "report": result,  # ì „ì²´ ë¦¬í¬íŠ¸ ë°ì´í„° í¬í•¨
                "meta": {
                    "processing_method": "multi_agent_langgraph",
                    "analysis_depth": analysis_depth,
                    "complexity_score": complexity_score,
                    "quality_score": result.get("quality_score", 0),
                    "quality_level": result.get("quality_level", "unknown"),
                    "contexts_count": result.get("contexts_count", 0),
                    "insights_count": result.get("insights_count", 0),
                    "relationships_count": result.get("relationships_count", 0),
                    "processing_time": result.get("processing_time", 0.0),
                    "retry_count": result.get("retry_count", 0),
                    "execution_log": result.get("execution_log", [])
                }
            }

        except Exception as e:
            logger.error(f"[LangGraph] ì²˜ë¦¬ ì‹¤íŒ¨: {e}, í´ë°± ì‚¬ìš©")
            if tracker:
                tracker.record_error("langgraph_error", "multi_agent", str(e))
            return await self._handle_fallback(query, tracker)

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ëŠ” ë‚˜ì¤‘ì— ì´ˆê¸°í™”