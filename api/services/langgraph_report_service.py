# src/ontology_chat/services/langgraph_report_service.py
"""
LangGraph ê¸°ë°˜ ê³ ê¸‰ ì»¨í…ìŠ¤íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ë¦¬í¬íŠ¸ ìƒì„± ì„œë¹„ìŠ¤

ì£¼ìš” íŠ¹ì§•:
1. ë‹¤ë‹¨ê³„ ì •ë³´ ìˆ˜ì§‘ ë° ê²€ì¦
2. ì»¨í…ìŠ¤íŠ¸ ê°„ ê´€ê³„ ë¶„ì„
3. ë™ì  ë¶„ì„ ê¹Šì´ ì¡°ì ˆ
4. í’ˆì§ˆ ê¸°ë°˜ ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜
5. êµ¬ì¡°í™”ëœ ì¸ì‚¬ì´íŠ¸ ìƒì„±
"""

from __future__ import annotations
import asyncio
from typing import Any, Dict, List, Optional, TypedDict, Annotated
from dataclasses import dataclass
from enum import Enum
import json

from langgraph.graph import StateGraph, END
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
# from langchain_openai import ChatOpenAI
from langchain_ollama import OllamaLLM

from api.logging import setup_logging
from api.adapters.mcp_neo4j import Neo4jMCP
from api.adapters.mcp_opensearch import OpenSearchMCP
from api.adapters.mcp_stock import StockMCP
from api.adapters.ollama_embedding import OllamaEmbeddingMCP
from api.services.report_service import ReportService
from api.config import settings
import traceback 
from icecream import ic
logger = setup_logging()

# ========== ìƒíƒœ ì •ì˜ ==========

class AnalysisDepth(Enum):
    SHALLOW = "shallow"      # ê¸°ë³¸ ì •ë³´ë§Œ
    STANDARD = "standard"    # ì¼ë°˜ì  ë¶„ì„
    DEEP = "deep"           # ì‹¬í™” ë¶„ì„
    COMPREHENSIVE = "comprehensive"  # ì¢…í•© ë¶„ì„

class ReportQuality(Enum):
    POOR = "poor"           # ì¬ì‹œë„ í•„ìš”
    ACCEPTABLE = "acceptable"  # ìµœì†Œ ê¸°ì¤€ ì¶©ì¡±
    GOOD = "good"           # ì–‘í˜¸
    EXCELLENT = "excellent"    # ìš°ìˆ˜

@dataclass
class ContextItem:
    """ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ë‹¨ìœ„"""
    source: str          # ì¶œì²˜ (neo4j, opensearch, stock, llm)
    type: str           # ìœ í˜• (news, contract, company, analysis)
    content: Dict[str, Any]  # ì‹¤ì œ ë°ì´í„°
    confidence: float   # ì‹ ë¢°ë„ (0.0-1.0)
    relevance: float    # ê´€ë ¨ì„± (0.0-1.0)
    timestamp: str      # ìˆ˜ì§‘ ì‹œê°„

class LangGraphReportState(TypedDict):
    """LangGraph ìƒíƒœ ì •ì˜"""
    # ì…ë ¥ ì •ë³´
    query: str
    domain: Optional[str]
    lookback_days: int
    analysis_depth: AnalysisDepth

    # ìˆ˜ì§‘ëœ ì»¨í…ìŠ¤íŠ¸
    contexts: List[ContextItem]

    # ë¶„ì„ ê²°ê³¼
    insights: List[Dict[str, Any]]
    relationships: List[Dict[str, Any]]

    # ë¦¬í¬íŠ¸ ìƒì„±
    report_sections: Dict[str, str]
    final_report: str

    # í’ˆì§ˆ ê´€ë¦¬
    quality_score: float
    quality_level: ReportQuality
    retry_count: int

    # ë©”íƒ€ë°ì´í„°
    execution_log: List[str]
    processing_time: float

# ========== LangGraph ë…¸ë“œ í•¨ìˆ˜ë“¤ ==========

class LangGraphReportEngine:
    """LangGraph ê¸°ë°˜ ë¦¬í¬íŠ¸ ìƒì„± ì—”ì§„"""

    def __init__(self):
        self.neo4j = Neo4jMCP()
        self.opensearch = OpenSearchMCP()
        self.stock = StockMCP()
        self.report_service = ReportService()

        # Ollama ì„ë² ë”© í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ì§ì ‘ ë²¡í„° ìœ ì‚¬ë„ ê³„ì‚°ìš©)
        try:
            self.embedding_client = OllamaEmbeddingMCP()
            logger.info(f"[LangGraph] Ollama ì„ë² ë”© í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"Ollama ì„ë² ë”© ì´ˆê¸°í™” ì‹¤íŒ¨, í‚¤ì›Œë“œ ê²€ìƒ‰ë§Œ ì‚¬ìš©: {e}")
            self.embedding_client = None

        # LLM ì´ˆê¸°í™” (ë¹„ë™ê¸° í˜¸í™˜)
        self.llm = OllamaLLM(
            model=settings.ollama_model,
            base_url=settings.get_ollama_base_url(),
            temperature=0.1,
            num_predict=4000
        )
        logger.info(f"[LangGraph] LLM ì´ˆê¸°í™” ì™„ë£Œ: {settings.ollama_model} @ {settings.get_ollama_base_url()}")

        # LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„±
        self.workflow = self._build_workflow()

    def _build_workflow(self) -> StateGraph:
        """LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„±"""

        workflow = StateGraph(LangGraphReportState)

        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("analyze_query", self._analyze_query)
        workflow.add_node("collect_parallel_data", self._collect_parallel_data)
        workflow.add_node("cross_validate_contexts", self._cross_validate_contexts)
        workflow.add_node("generate_insights", self._generate_insights)
        workflow.add_node("analyze_relationships", self._analyze_relationships)
        workflow.add_node("synthesize_report", self._synthesize_report)
        workflow.add_node("quality_check", self._quality_check)
        workflow.add_node("enhance_report", self._enhance_report)

        # ì›Œí¬í”Œë¡œìš° ì—°ê²°
        workflow.set_entry_point("analyze_query")

        # ë³‘ë ¬ ë°ì´í„° ìˆ˜ì§‘ì„ ìœ„í•œ ê°œì„ ëœ ì—°ê²°
        workflow.add_edge("analyze_query", "collect_parallel_data")
        workflow.add_edge("collect_parallel_data", "cross_validate_contexts")
        workflow.add_edge("cross_validate_contexts", "generate_insights")
        workflow.add_edge("generate_insights", "analyze_relationships")
        workflow.add_edge("analyze_relationships", "synthesize_report")
        workflow.add_edge("synthesize_report", "quality_check")

        # ì¡°ê±´ë¶€ ë¶„ê¸°
        workflow.add_conditional_edges(
            "quality_check",
            self._should_enhance_report,
            {
                "enhance": "enhance_report",
                "complete": END
            }
        )
        workflow.add_edge("enhance_report", "quality_check")

        return workflow.compile()

    async def _analyze_query(self, state: LangGraphReportState) -> LangGraphReportState:
        """1ë‹¨ê³„: ì¿¼ë¦¬ ë¶„ì„ ë° ë¶„ì„ ì „ëµ ìˆ˜ë¦½"""

        state["execution_log"].append("ğŸ” ì¿¼ë¦¬ ë¶„ì„ ì‹œì‘")

        # ê°œì„ ëœ ë‹¨ê³„ë³„ ì¿¼ë¦¬ ë¶„ì„
        try:
            # 1ë‹¨ê³„: í‚¤ì›Œë“œ ì¶”ì¶œ
            keyword_prompt = f"ë‹¤ìŒ ì§ˆì˜ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ 3-5ê°œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”. ì§ˆì˜: '{state['query']}'. ì‘ë‹µ í˜•ì‹: ['í‚¤ì›Œë“œ1', 'í‚¤ì›Œë“œ2', 'í‚¤ì›Œë“œ3']"
            keyword_response = await self._llm_invoke(keyword_prompt)

            # 2ë‹¨ê³„: ë¶„ì„ ë³µì¡ë„ íŒë‹¨
            complexity_prompt = f"ë‹¤ìŒ ì§ˆì˜ì˜ ë¶„ì„ ë³µì¡ë„ë¥¼ íŒë‹¨í•˜ì„¸ìš”. ì§ˆì˜: '{state['query']}'. ì‘ë‹µ: shallow, standard, deep, comprehensive ì¤‘ í•˜ë‚˜ë§Œ"
            complexity_response = await self._llm_invoke(complexity_prompt)

            # ì•ˆì „í•œ íŒŒì‹±
            keywords = self._safe_parse_keywords(keyword_response)
            complexity = self._safe_parse_complexity(complexity_response)

            state["analysis_depth"] = AnalysisDepth(complexity)
            state["query_analysis"] = {
                "keywords": keywords,
                "complexity": complexity,
                "focus_areas": [state['query']]  # ê¸°ë³¸ê°’
            }

            state["execution_log"].append(f"âœ… ë¶„ì„ ê¹Šì´ ì„¤ì •: {complexity}, í‚¤ì›Œë“œ: {keywords}")

        except Exception as e:
            logger.warning(f"ì¿¼ë¦¬ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            state["analysis_depth"] = AnalysisDepth.STANDARD
            state["query_analysis"] = {"keywords": [state['query']], "complexity": "standard"}
            state["execution_log"].append("âš ï¸ ê¸°ë³¸ ë¶„ì„ ëª¨ë“œë¡œ ì„¤ì •")

        return state

    async def _collect_parallel_data(self, state: LangGraphReportState) -> LangGraphReportState:
        """2ë‹¨ê³„: ë³‘ë ¬ ë°ì´í„° ìˆ˜ì§‘ (ì„±ëŠ¥ ìµœì í™”)"""

        state["execution_log"].append("ğŸš€ ë³‘ë ¬ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")

        try:
            # ë³‘ë ¬ë¡œ ì‹¤í–‰í•  ì‘ì—…ë“¤
            tasks = []

            # 1. êµ¬ì¡°í™”ëœ ë°ì´í„° ìˆ˜ì§‘ (Neo4j, Stock)
            tasks.append(self._collect_structured_data_async(state))

            # 2. ë¹„êµ¬ì¡°í™”ëœ ë°ì´í„° ìˆ˜ì§‘ (OpenSearch)
            tasks.append(self._collect_unstructured_data_async(state))

            # ë³‘ë ¬ ì‹¤í–‰
            results = await asyncio.gather(*tasks, return_exceptions=True)

            # ê²°ê³¼ í†µí•©
            structured_contexts = []
            unstructured_contexts = []

            for result in results:
                if isinstance(result, Exception):
                    logger.warning(f"ë³‘ë ¬ ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {result}")
                    continue

                if result.get("type") == "structured":
                    structured_contexts.extend(result.get("contexts", []))
                elif result.get("type") == "unstructured":
                    unstructured_contexts.extend(result.get("contexts", []))

            # ìƒíƒœì— í†µí•©
            state["contexts"].extend(structured_contexts)
            state["contexts"].extend(unstructured_contexts)

            state["execution_log"].append(f"âœ… ë³‘ë ¬ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: êµ¬ì¡°í™”({len(structured_contexts)}) + ë¹„êµ¬ì¡°í™”({len(unstructured_contexts)})")

        except Exception as e:
            logger.error(f"ë³‘ë ¬ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            state["execution_log"].append(f"âŒ ë³‘ë ¬ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

        return state

    async def _collect_structured_data_async(self, state: LangGraphReportState) -> Dict[str, Any]:
        """êµ¬ì¡°í™”ëœ ë°ì´í„° ë¹„ë™ê¸° ìˆ˜ì§‘"""
        contexts = []

        try:
            # Neo4j ë°ì´í„° ìˆ˜ì§‘
            graph_context = await self.report_service.fetch_context(
                query=state["query"],
                lookback_days=state["lookback_days"],
                domain=state.get("domain"),
                graph_limit=100
            )

            # ê·¸ë˜í”„ ë°ì´í„°ë¥¼ ContextItemìœ¼ë¡œ ë³€í™˜
            for row in graph_context.graph_rows:
                context_item = ContextItem(
                    source="neo4j",
                    type=self._determine_graph_type(row),
                    content=row,
                    confidence=0.8,
                    relevance=self._calculate_graph_relevance(row, state["query"]),
                    timestamp=str(asyncio.get_event_loop().time())
                )
                contexts.append(context_item)

            # ì£¼ê°€ ë°ì´í„° ìˆ˜ì§‘ (ìƒì¥ì‚¬ì˜ ê²½ìš°)
            await self._collect_stock_data(state, contexts)

            return {"type": "structured", "contexts": contexts}

        except Exception as e:
            logger.error(f"êµ¬ì¡°í™”ëœ ë°ì´í„° ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            return {"type": "structured", "contexts": []}

    async def _collect_unstructured_data_async(self, state: LangGraphReportState) -> Dict[str, Any]:
        """ë¹„êµ¬ì¡°í™”ëœ ë°ì´í„° ë¹„ë™ê¸° ìˆ˜ì§‘"""
        contexts = []

        try:
            # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‚¬ìš© (Ollama ì„ë² ë”© + í‚¤ì›Œë“œ)
            news_hits = await self._langgraph_hybrid_search(
                query=state["query"],
                lookback_days=state["lookback_days"],
                size=50
            )

            # ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ContextItemìœ¼ë¡œ ë³€í™˜
            for hit in news_hits:
                source_data = hit.get("_source", {})
                context_item = ContextItem(
                    source="opensearch",
                    type="news",
                    content=source_data,
                    confidence=min(hit.get("_score", 0) / 10.0, 1.0),
                    relevance=self._calculate_news_relevance(source_data, state["query"]),
                    timestamp=str(asyncio.get_event_loop().time())
                )
                contexts.append(context_item)

            return {"type": "unstructured", "contexts": contexts}

        except Exception as e:
            logger.error(f"ë¹„êµ¬ì¡°í™”ëœ ë°ì´í„° ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            return {"type": "unstructured", "contexts": []}

    async def _collect_structured_data(self, state: LangGraphReportState) -> LangGraphReportState:
        """2ë‹¨ê³„: êµ¬ì¡°í™”ëœ ë°ì´í„° ìˆ˜ì§‘ (Neo4j, Stock)"""

        state["execution_log"].append("ğŸ“Š êµ¬ì¡°í™”ëœ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")

        try:
            # Neo4j ë°ì´í„° ìˆ˜ì§‘
            graph_context = await self.report_service.fetch_context(
                query=state["query"],
                lookback_days=state["lookback_days"],
                domain=state.get("domain"),
                graph_limit=100
            )

            # ê·¸ë˜í”„ ë°ì´í„°ë¥¼ ContextItemìœ¼ë¡œ ë³€í™˜
            for row in graph_context.graph_rows:
                context_item = ContextItem(
                    source="neo4j",
                    type=self._determine_graph_type(row),
                    content=row,
                    confidence=0.9,  # êµ¬ì¡°í™”ëœ ë°ì´í„°ëŠ” ë†’ì€ ì‹ ë¢°ë„
                    relevance=self._calculate_relevance(row, state["query"]),
                    timestamp=str(asyncio.get_event_loop().time())
                )
                state["contexts"].append(context_item)

            # ì£¼ê°€ ë°ì´í„° ìˆ˜ì§‘ (ì¢…ëª©ì´ ìˆëŠ” ê²½ìš°)
            if "symbol" in state and state["symbol"]:
                try:
                    stock_data = await self.stock.get_price(state["symbol"])
                    stock_context = ContextItem(
                        source="stock",
                        type="price_data",
                        content=stock_data,
                        confidence=0.95,
                        relevance=0.8,
                        timestamp=str(asyncio.get_event_loop().time())
                    )
                    state["contexts"].append(stock_context)
                except Exception as e:
                    logger.warning(f"ì£¼ê°€ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

            state["execution_log"].append(f"âœ… êµ¬ì¡°í™”ëœ ë°ì´í„° {len([c for c in state['contexts'] if c.source in ['neo4j', 'stock']])}ê°œ ìˆ˜ì§‘")

        except Exception as e:
            logger.error(f"êµ¬ì¡°í™”ëœ ë°ì´í„° ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            state["execution_log"].append(f"âŒ êµ¬ì¡°í™”ëœ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

        return state

    async def _collect_unstructured_data(self, state: LangGraphReportState) -> LangGraphReportState:
        """3ë‹¨ê³„: ë¹„êµ¬ì¡°í™”ëœ ë°ì´í„° ìˆ˜ì§‘ (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰)"""

        state["execution_log"].append("ğŸ“° í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê¸°ë°˜ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")

        try:
            # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‚¬ìš© (Ollama ì„ë² ë”© + í‚¤ì›Œë“œ)
            news_hits = await self._langgraph_hybrid_search(
                query=state["query"],
                lookback_days=state["lookback_days"],
                size=50
            )
            state["execution_log"].append(f"âœ… í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì™„ë£Œ: {len(news_hits)}ê±´")

            # ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ContextItemìœ¼ë¡œ ë³€í™˜
            for hit in news_hits:
                source_data = hit.get("_source", {})
                context_item = ContextItem(
                    source="opensearch",
                    type="news",
                    content=source_data,
                    confidence=min(hit.get("_score", 0) / 10.0, 1.0),
                    relevance=self._calculate_news_relevance(source_data, state["query"]),
                    timestamp=str(asyncio.get_event_loop().time())
                )
                state["contexts"].append(context_item)

            # ì‹¬í™” ë¶„ì„ì´ í•„ìš”í•œ ê²½ìš° ì¶”ê°€ ê²€ìƒ‰
            if state["analysis_depth"] in [AnalysisDepth.DEEP, AnalysisDepth.COMPREHENSIVE]:
                # í™•ì¥ëœ í‚¤ì›Œë“œë¡œ ì¶”ê°€ ê²€ìƒ‰
                extended_queries = await self._generate_extended_queries(state["query"])

                for ext_query in extended_queries:
                    try:
                        # í™•ì¥ ì¿¼ë¦¬ë„ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‚¬ìš©
                        ext_hits = await self._langgraph_hybrid_search(
                            query=ext_query,
                            lookback_days=state["lookback_days"],
                            size=20
                        )

                        for hit in ext_hits:
                            source_data = hit.get("_source", {})
                            context_item = ContextItem(
                                source="opensearch_extended",
                                type="news_extended",
                                content=source_data,
                                confidence=min(hit.get("_score", 0) / 15.0, 0.8),  # í™•ì¥ ê²€ìƒ‰ì€ ì‹ ë¢°ë„ ì•½ê°„ ë‚®ìŒ
                                relevance=self._calculate_news_relevance(source_data, ext_query),
                                timestamp=str(asyncio.get_event_loop().time())
                            )
                            state["contexts"].append(context_item)
                    except Exception as e:
                        logger.warning(f"í™•ì¥ ê²€ìƒ‰ ì‹¤íŒ¨ ({ext_query}): {e}")

            state["execution_log"].append(f"âœ… ë¹„êµ¬ì¡°í™”ëœ ë°ì´í„° {len([c for c in state['contexts'] if c.source.startswith('opensearch')])}ê°œ ìˆ˜ì§‘")

        except Exception as e:
            logger.error(f"ë¹„êµ¬ì¡°í™”ëœ ë°ì´í„° ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            state["execution_log"].append(f"âŒ ë¹„êµ¬ì¡°í™”ëœ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

        return state

    async def _cross_validate_contexts(self, state: LangGraphReportState) -> LangGraphReportState:
        """4ë‹¨ê³„: ì»¨í…ìŠ¤íŠ¸ ê°„ êµì°¨ ê²€ì¦ ë° í•„í„°ë§"""

        state["execution_log"].append("ğŸ”— ì»¨í…ìŠ¤íŠ¸ êµì°¨ ê²€ì¦ ì‹œì‘")

        # ì¤‘ë³µ ì œê±°
        unique_contexts = []
        seen_contents = set()

        for context in state["contexts"]:
            # ì»¨í…ì¸  í•´ì‹œ ìƒì„± (ë‹¨ìˆœí™”)
            content_hash = hash(str(context.content))

            if content_hash not in seen_contents:
                seen_contents.add(content_hash)
                unique_contexts.append(context)

        # ì‹ ë¢°ë„ ë° ê´€ë ¨ì„± ê¸°ë°˜ í•„í„°ë§
        high_quality_contexts = [
            ctx for ctx in unique_contexts
            if ctx.confidence > 0.3 and ctx.relevance > 0.2
        ]

        # ìƒìœ„ ì»¨í…ìŠ¤íŠ¸ë§Œ ìœ ì§€ (ë¶„ì„ ê¹Šì´ì— ë”°ë¼)
        max_contexts = {
            AnalysisDepth.SHALLOW: 20,
            AnalysisDepth.STANDARD: 40,
            AnalysisDepth.DEEP: 80,
            AnalysisDepth.COMPREHENSIVE: 150
        }

        # í’ˆì§ˆ ì ìˆ˜ë¡œ ì •ë ¬ í›„ ìƒìœ„ Nê°œ ì„ íƒ
        sorted_contexts = sorted(
            high_quality_contexts,
            key=lambda x: x.confidence * x.relevance,
            reverse=True
        )

        state["contexts"] = sorted_contexts[:max_contexts[state["analysis_depth"]]]
        state["execution_log"].append(f"âœ… {len(state['contexts'])}ê°œ ê³ í’ˆì§ˆ ì»¨í…ìŠ¤íŠ¸ ì„ ë³„")

        return state

    async def _generate_insights(self, state: LangGraphReportState) -> LangGraphReportState:
        """5ë‹¨ê³„: LLM ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""

        state["execution_log"].append("ğŸ’¡ ì¸ì‚¬ì´íŠ¸ ìƒì„± ì‹œì‘")

        # ì»¨í…ìŠ¤íŠ¸ë¥¼ ìœ í˜•ë³„ë¡œ ê·¸ë£¹í•‘
        context_groups = {}
        for ctx in state["contexts"]:
            if ctx.type not in context_groups:
                context_groups[ctx.type] = []
            context_groups[ctx.type].append(ctx)

        insights = []

        for ctx_type, contexts in context_groups.items():
            if len(contexts) < 2:  # ìµœì†Œ 2ê°œ ì´ìƒ ìˆì–´ì•¼ ì¸ì‚¬ì´íŠ¸ ìƒì„±
                continue

            # ìƒì¥ì‚¬ íŠ¹í™” ì¸ì‚¬ì´íŠ¸ ìƒì„±
            data_summary = self._summarize_context_data(contexts[:3])  # ìƒìœ„ 3ê°œë§Œ
            insight_prompt = self._generate_sector_specific_insight_prompt(ctx_type, state['query'], data_summary)

            try:
                response = await self._llm_invoke(insight_prompt)

                insight = {
                    "type": ctx_type,
                    "content": response,
                    "source_count": len(contexts),
                    "confidence": sum(ctx.confidence for ctx in contexts) / len(contexts),
                    "timestamp": str(asyncio.get_event_loop().time())
                }
                insights.append(insight)

            except Exception as e:
                logger.warning(f"ì¸ì‚¬ì´íŠ¸ ìƒì„± ì‹¤íŒ¨ ({ctx_type}): {e}")

        state["insights"] = insights
        state["execution_log"].append(f"âœ… {len(insights)}ê°œ ì¸ì‚¬ì´íŠ¸ ìƒì„±")

        return state

    async def _analyze_relationships(self, state: LangGraphReportState) -> LangGraphReportState:
        """6ë‹¨ê³„: ì»¨í…ìŠ¤íŠ¸ ê°„ ê´€ê³„ ë¶„ì„"""

        state["execution_log"].append("ğŸ”— ê´€ê³„ ë¶„ì„ ì‹œì‘")

        relationships = []

        # ì»¨í…ìŠ¤íŠ¸ ìœ í˜•ë³„ ë¶„ë¥˜ (ëª¨ë“  ìƒì¥ì‚¬ ëŒ€ì‘)
        news_contexts = [ctx for ctx in state["contexts"] if ctx.type.startswith("news")]
        company_contexts = [ctx for ctx in state["contexts"] if ctx.type == "company"]
        financial_contexts = [ctx for ctx in state["contexts"] if ctx.type in ["financial", "investment", "stock"]]
        business_contexts = [ctx for ctx in state["contexts"] if ctx.type in ["contract", "deal", "announcement"]]

        # 1. ë‰´ìŠ¤-ê¸°ì—… ê´€ê³„ ë¶„ì„
        if news_contexts and company_contexts:
            await self._analyze_news_company_relationship(state, news_contexts, company_contexts, relationships)

        # 2. ì¬ë¬´-ë‰´ìŠ¤ ê´€ê³„ ë¶„ì„
        if financial_contexts and news_contexts:
            await self._analyze_financial_news_relationship(state, financial_contexts, news_contexts, relationships)

        # 3. ë¹„ì¦ˆë‹ˆìŠ¤ ì´ë²¤íŠ¸-ë‰´ìŠ¤ ê´€ê³„ ë¶„ì„
        if business_contexts and news_contexts:
            await self._analyze_business_news_relationship(state, business_contexts, news_contexts, relationships)

        state["relationships"] = relationships
        state["execution_log"].append(f"âœ… {len(relationships)}ê°œ ê´€ê³„ ë¶„ì„ ì™„ë£Œ")

        return state

    async def _analyze_news_company_relationship(self, state, news_contexts, company_contexts, relationships):
        """ë‰´ìŠ¤-ê¸°ì—… ê´€ê³„ ë¶„ì„"""
        try:
            relationship_prompt = f"""
            ë‹¤ìŒ ë‰´ìŠ¤ì™€ ê¸°ì—… ì •ë³´ ê°„ì˜ ê´€ê³„ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:

            ë‰´ìŠ¤ ë°ì´í„° (ìµœì‹  5ê°œ):
            {json.dumps([ctx.content for ctx in news_contexts[:5]], ensure_ascii=False, indent=2)}

            ê¸°ì—… ë°ì´í„°:
            {json.dumps([ctx.content for ctx in company_contexts[:3]], ensure_ascii=False, indent=2)}

            ë‹¤ìŒì„ ë¶„ì„í•´ì£¼ì„¸ìš”:
            1. ë‰´ìŠ¤ê°€ ê¸°ì—…ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ (ê¸ì •/ë¶€ì •/ì¤‘ë¦½)
            2. ê¸°ì—… ê°€ì¹˜ ë° ì£¼ê°€ì— ëŒ€í•œ ì‹œì‚¬ì 
            3. ì‚°ì—… ë‚´ ê²½ìŸë ¥ ë³€í™”
            4. í–¥í›„ ì˜ˆìƒë˜ëŠ” ê¸°ì—… ì „ëµ ë³€í™”

            ìƒì¥ì‚¬ íˆ¬ì ê´€ì ì—ì„œ êµ¬ì²´ì ì¸ ë¶„ì„ì„ í•´ì£¼ì„¸ìš”.
            """

            response = await self._llm_invoke(relationship_prompt)

            relationship = {
                "type": "news_company_correlation",
                "analysis": response,
                "confidence": 0.8,
                "timestamp": str(asyncio.get_event_loop().time())
            }
            relationships.append(relationship)

        except Exception as e:
            logger.warning(f"ë‰´ìŠ¤-ê¸°ì—… ê´€ê³„ ë¶„ì„ ì‹¤íŒ¨: {e}")

    async def _analyze_financial_news_relationship(self, state, financial_contexts, news_contexts, relationships):
        """ì¬ë¬´-ë‰´ìŠ¤ ê´€ê³„ ë¶„ì„"""
        try:
            relationship_prompt = f"""
            ë‹¤ìŒ ì¬ë¬´ ì •ë³´ì™€ ë‰´ìŠ¤ ê°„ì˜ ê´€ê³„ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:

            ì¬ë¬´ ë°ì´í„°:
            {json.dumps([ctx.content for ctx in financial_contexts[:3]], ensure_ascii=False, indent=2)}

            ê´€ë ¨ ë‰´ìŠ¤ (ìµœì‹  3ê°œ):
            {json.dumps([ctx.content for ctx in news_contexts[:3]], ensure_ascii=False, indent=2)}

            ë‹¤ìŒì„ ë¶„ì„í•´ì£¼ì„¸ìš”:
            1. ì¬ë¬´ ì„±ê³¼ì™€ ë‰´ìŠ¤ ì´ë²¤íŠ¸ ê°„ì˜ ì¸ê³¼ê´€ê³„
            2. ì¬ë¬´ ì§€í‘œ ë³€í™”ê°€ ì‹œì¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥
            3. íˆ¬ìì ê´€ì ì—ì„œì˜ ë¦¬ìŠ¤í¬/ê¸°íšŒ ìš”ì†Œ
            4. ì¬ë¬´ ê±´ì „ì„±ì— ëŒ€í•œ ì¢…í•© í‰ê°€

            íˆ¬ì ì˜ì‚¬ê²°ì •ì— ë„ì›€ì´ ë˜ëŠ” ë¶„ì„ì„ í•´ì£¼ì„¸ìš”.
            """

            response = await self._llm_invoke(relationship_prompt)

            relationship = {
                "type": "financial_news_correlation",
                "analysis": response,
                "confidence": 0.7,
                "timestamp": str(asyncio.get_event_loop().time())
            }
            relationships.append(relationship)

        except Exception as e:
            logger.warning(f"ì¬ë¬´-ë‰´ìŠ¤ ê´€ê³„ ë¶„ì„ ì‹¤íŒ¨: {e}")

    async def _analyze_business_news_relationship(self, state, business_contexts, news_contexts, relationships):
        """ë¹„ì¦ˆë‹ˆìŠ¤ ì´ë²¤íŠ¸-ë‰´ìŠ¤ ê´€ê³„ ë¶„ì„"""
        try:
            relationship_prompt = f"""
            ë‹¤ìŒ ë¹„ì¦ˆë‹ˆìŠ¤ ì´ë²¤íŠ¸ì™€ ë‰´ìŠ¤ ê°„ì˜ ê´€ê³„ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:

            ë¹„ì¦ˆë‹ˆìŠ¤ ì´ë²¤íŠ¸:
            {json.dumps([ctx.content for ctx in business_contexts[:3]], ensure_ascii=False, indent=2)}

            ê´€ë ¨ ë‰´ìŠ¤:
            {json.dumps([ctx.content for ctx in news_contexts[:3]], ensure_ascii=False, indent=2)}

            ë‹¤ìŒì„ ë¶„ì„í•´ì£¼ì„¸ìš”:
            1. ë¹„ì¦ˆë‹ˆìŠ¤ ì´ë²¤íŠ¸ê°€ ê¸°ì—… ì„±ì¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥
            2. ì‹œì¥ í¬ì§€ì…˜ ë³€í™” ë° ê²½ìŸ ìš°ìœ„
            3. ë§¤ì¶œ ë° ìˆ˜ìµì„±ì— ëŒ€í•œ ì˜í–¥ ì˜ˆì¸¡
            4. ì¥ê¸°ì  ì‚¬ì—… ì „ëµ ê´€ì ì—ì„œì˜ ì˜ë¯¸

            ê¸°ì—… ë¶„ì„ ê´€ì ì—ì„œ ì¢…í•©ì ì¸ í‰ê°€ë¥¼ í•´ì£¼ì„¸ìš”.
            """

            response = await self._llm_invoke(relationship_prompt)

            relationship = {
                "type": "business_news_correlation",
                "analysis": response,
                "confidence": 0.75,
                "timestamp": str(asyncio.get_event_loop().time())
            }
            relationships.append(relationship)

        except Exception as e:
            logger.warning(f"ë¹„ì¦ˆë‹ˆìŠ¤-ë‰´ìŠ¤ ê´€ê³„ ë¶„ì„ ì‹¤íŒ¨: {e}")

    async def _synthesize_report(self, state: LangGraphReportState) -> LangGraphReportState:
        """7ë‹¨ê³„: ì¢…í•© ë¦¬í¬íŠ¸ í•©ì„±"""

        state["execution_log"].append("ğŸ“ ë¦¬í¬íŠ¸ í•©ì„± ì‹œì‘")

        # ì„¹ì…˜ë³„ ë¦¬í¬íŠ¸ ìƒì„±
        sections = {}

        # 1. ê°œìš” ì„¹ì…˜
        overview_prompt = f"""
        ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ '{state['query']}'ì— ëŒ€í•œ ê°œìš”ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:

        ìˆ˜ì§‘ëœ ë°ì´í„°:
        - ì´ ì»¨í…ìŠ¤íŠ¸: {len(state['contexts'])}ê°œ
        - ìƒì„±ëœ ì¸ì‚¬ì´íŠ¸: {len(state['insights'])}ê°œ
        - ê´€ê³„ ë¶„ì„: {len(state['relationships'])}ê°œ

        ë¶„ì„ ê¹Šì´: {state['analysis_depth'].value}

        ê°„ê²°í•˜ë©´ì„œë„ í¬ê´„ì ì¸ ê°œìš”ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
        """

        try:
            overview_response = await self._llm_invoke(overview_prompt)
            sections["overview"] = overview_response
        except Exception as e:
            sections["overview"] = f"ê°œìš” ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"

        # 2. í•µì‹¬ ë°œê²¬ì‚¬í•­
        if state["insights"]:
            key_findings = "\n\n".join([
                f"**{insight['type']} ë¶„ì„**\n{insight['content']}"
                for insight in state["insights"]
            ])
            sections["key_findings"] = key_findings

        # 3. ê´€ê³„ ë¶„ì„
        if state["relationships"]:
            relationship_analysis = "\n\n".join([
                f"**{rel['type']}**\n{rel['analysis']}"
                for rel in state["relationships"]
            ])
            sections["relationships"] = relationship_analysis

        # 4. ë°ì´í„° ìš”ì•½
        data_summary = self._generate_data_summary(state["contexts"])
        sections["data_summary"] = data_summary

        # 5. ìµœì¢… ë¦¬í¬íŠ¸ í•©ì„±
        final_report_prompt = f"""
        ë‹¤ìŒ ì„¹ì…˜ë“¤ì„ ì¢…í•©í•˜ì—¬ ì „ë¬¸ì ì¸ ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:

        ì§ˆì˜: {state['query']}

        ì„¹ì…˜ë³„ ë‚´ìš©:

        ## ê°œìš”
        {sections.get('overview', 'N/A')}

        ## í•µì‹¬ ë°œê²¬ì‚¬í•­
        {sections.get('key_findings', 'N/A')}

        ## ê´€ê³„ ë¶„ì„
        {sections.get('relationships', 'N/A')}

        ## ë°ì´í„° ìš”ì•½
        {sections.get('data_summary', 'N/A')}

        ë‹¤ìŒ êµ¬ì¡°ë¡œ ì „ë¬¸ì ì¸ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:
        1. ìš”ì•½ (Executive Summary)
        2. ìƒì„¸ ë¶„ì„
        3. ì‹œì‚¬ì  ë° ì „ë§
        4. ê¶Œì¥ì‚¬í•­

        ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ê³ , ë¹„ì¦ˆë‹ˆìŠ¤ ì˜ì‚¬ê²°ì •ì— ë„ì›€ì´ ë˜ëŠ” ì‹¤ìš©ì ì¸ ë‚´ìš©ìœ¼ë¡œ êµ¬ì„±í•´ì£¼ì„¸ìš”.
        """

        try:
            final_response = await self._llm_invoke(final_report_prompt)
            state["final_report"] = final_response
            state["report_sections"] = sections
        except Exception as e:
            state["final_report"] = f"ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
            state["report_sections"] = sections

        state["execution_log"].append("âœ… ë¦¬í¬íŠ¸ í•©ì„± ì™„ë£Œ")

        return state

    async def _quality_check(self, state: LangGraphReportState) -> LangGraphReportState:
        """8ë‹¨ê³„: ê°œì„ ëœ í’ˆì§ˆ ê²€ì¦"""

        state["execution_log"].append("ğŸ¯ í’ˆì§ˆ ê²€ì¦ ì‹œì‘")

        # ìƒì¥ì‚¬ ë¶„ì„ì— íŠ¹í™”ëœ í’ˆì§ˆ í‰ê°€ ìš”ì†Œë“¤
        quality_factors = {}

        # 1. ë°ì´í„° ë‹¤ì–‘ì„± (ìƒì¥ì‚¬ ë¶„ì„ì— í•„ìš”í•œ ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤)
        context_types = set(ctx.type for ctx in state["contexts"])
        expected_types = {"company", "news", "financial", "stock"}
        type_coverage = len(context_types.intersection(expected_types)) / len(expected_types)
        quality_factors["data_diversity"] = type_coverage

        # 2. íˆ¬ì ê´€ë ¨ ì»¨í…ì¸  í’ˆì§ˆ
        investment_keywords = ["ì£¼ê°€", "íˆ¬ì", "ì¬ë¬´", "ìˆ˜ìµ", "ì„±ì¥", "ë°°ë‹¹", "ë°¸ë¥˜ì—ì´ì…˜", "ë¦¬ìŠ¤í¬"]
        content_quality = self._evaluate_investment_content_quality(state["final_report"], state["query"], investment_keywords)
        quality_factors["investment_content_quality"] = content_quality

        # 3. ìƒì¥ì‚¬ ë¶„ì„ êµ¬ì¡°ì  ì™„ì„±ë„
        required_sections = ["ê¸°ì—…ë¶„ì„", "ì¬ë¬´ë¶„ì„", "íˆ¬ìí¬ì¸íŠ¸", "ë¦¬ìŠ¤í¬", "ì „ë§"]
        structure_score = sum(1 for section in required_sections
                            if any(keyword in state["final_report"] for keyword in [section, section.lower()]))
        quality_factors["analysis_structure"] = structure_score / len(required_sections)

        # 4. ì •ëŸ‰ì  ë¶„ì„ í¬í•¨ë„
        quantitative_keywords = ["ë§¤ì¶œ", "ì˜ì—…ì´ìµ", "ìˆœì´ìµ", "ROE", "PER", "PBR", "ë¶€ì±„ë¹„ìœ¨", "%"]
        quant_score = sum(1 for keyword in quantitative_keywords if keyword in state["final_report"])
        quality_factors["quantitative_analysis"] = min(quant_score / len(quantitative_keywords), 1.0)

        # 5. ê´€ê³„ ë¶„ì„ í’ˆì§ˆ (ìƒì¥ì‚¬ëŠ” ë‹¤ì–‘í•œ ì´í•´ê´€ê³„ì ë¶„ì„ì´ ì¤‘ìš”)
        quality_factors["relationship_quality"] = min(len(state["relationships"]) / 2.0, 1.0)

        # 6. ì¸ì‚¬ì´íŠ¸ ê¹Šì´ (ìƒì¥ì‚¬ëŠ” íˆ¬ì ê´€ì ì˜ ê¹Šì€ ì¸ì‚¬ì´íŠ¸ í•„ìš”)
        insight_depth = self._evaluate_insight_depth(state["insights"])
        quality_factors["insight_depth"] = insight_depth

        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (ìƒì¥ì‚¬ ë¶„ì„ì— ë§ê²Œ ì¡°ì •)
        weights = {
            "data_diversity": 0.2,           # ë°ì´í„° ë‹¤ì–‘ì„±
            "investment_content_quality": 0.3,  # íˆ¬ì ê´€ë ¨ ë‚´ìš© í’ˆì§ˆ
            "analysis_structure": 0.2,       # ë¶„ì„ êµ¬ì¡°
            "quantitative_analysis": 0.1,    # ì •ëŸ‰ì  ë¶„ì„
            "relationship_quality": 0.1,     # ê´€ê³„ ë¶„ì„
            "insight_depth": 0.1            # ì¸ì‚¬ì´íŠ¸ ê¹Šì´
        }

        quality_score = sum(score * weights[factor] for factor, score in quality_factors.items())
        state["quality_score"] = quality_score

        # í’ˆì§ˆ ë“±ê¸‰ ê²°ì •
        if quality_score >= 0.75:
            quality_level = ReportQuality.EXCELLENT
        elif quality_score >= 0.55:
            quality_level = ReportQuality.GOOD
        elif quality_score >= 0.35:
            quality_level = ReportQuality.ACCEPTABLE
        else:
            quality_level = ReportQuality.POOR

        state["quality_level"] = quality_level
        state["execution_log"].append(f"âœ… í’ˆì§ˆ ì ìˆ˜: {quality_score:.2f} ({quality_level.value})")
        state["execution_log"].append(f"   ì„¸ë¶€ ì ìˆ˜: {quality_factors}")

        return state

    async def _enhance_report(self, state: LangGraphReportState) -> LangGraphReportState:
        """9ë‹¨ê³„: ë¦¬í¬íŠ¸ ê°œì„ """

        retry_count = state.get("retry_count", 0)
        quality_score = state.get("quality_score", 0.0)

        # quality_level ì•ˆì „ ì²´í¬
        quality_level = state.get("quality_level", ReportQuality.POOR)
        if not isinstance(quality_level, ReportQuality):
            quality_level = ReportQuality.POOR
            state["quality_level"] = quality_level

        state["execution_log"].append(f"ğŸ”§ ë¦¬í¬íŠ¸ ê°œì„  ì‹œì‘ (ì¬ì‹œë„ {retry_count}íšŒì°¨)")

        try:
            # í’ˆì§ˆ ë¬¸ì œ ì§„ë‹¨
            issues = []
            if len(state.get("final_report", "")) < 300:
                issues.append("ë¦¬í¬íŠ¸ ê¸¸ì´ ë¶€ì¡±")
            if len(state.get("insights", [])) < 1:
                issues.append("ì¸ì‚¬ì´íŠ¸ ë¶€ì¡±")
            if quality_score < 0.4:
                issues.append("ì „ë°˜ì  í’ˆì§ˆ ì €í•˜")

            # ê°œì„  ì „ëµ ê²°ì •
            enhancement_strategy = self._determine_enhancement_strategy(issues)

            # ë§ì¶¤í˜• ê°œì„  í”„ë¡¬í”„íŠ¸ ìƒì„±
            if enhancement_strategy == "expand_content":
                enhancement_prompt = f"""
                ë‹¤ìŒ ë¦¬í¬íŠ¸ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤. ë‚´ìš©ì„ í™•ì¥í•˜ê³  ë³´ì™„í•´ì£¼ì„¸ìš”:

                í˜„ì¬ ë¦¬í¬íŠ¸:
                {state['final_report']}

                ì‚¬ìš© ê°€ëŠ¥í•œ ì¶”ê°€ ë°ì´í„°:
                - ìˆ˜ì§‘ëœ ì»¨í…ìŠ¤íŠ¸: {len(state.get('contexts', []))}ê°œ
                - ìƒì„±ëœ ì¸ì‚¬ì´íŠ¸: {len(state.get('insights', []))}ê°œ

                ë‹¤ìŒì„ í¬í•¨í•˜ì—¬ ë¦¬í¬íŠ¸ë¥¼ í™•ì¥í•´ì£¼ì„¸ìš”:
                1. ìƒì„¸í•œ ë¶„ì„ ë‚´ìš©
                2. êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ì™€ ê·¼ê±°
                3. ì‹œì¥ ì˜í–¥ ë¶„ì„
                4. í–¥í›„ ì „ë§ê³¼ ê¶Œì¥ì‚¬í•­

                ìµœì†Œ 800ì ì´ìƒì˜ ì „ë¬¸ì ì¸ ë¦¬í¬íŠ¸ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
                """

            elif enhancement_strategy == "add_insights":
                # ì¸ì‚¬ì´íŠ¸ ë¶€ì¡± ì‹œ ì¶”ê°€ ìƒì„±
                available_contexts = [ctx for ctx in state.get("contexts", []) if ctx.confidence > 0.5]
                context_summary = self._summarize_context_data(available_contexts[:5])

                enhancement_prompt = f"""
                í˜„ì¬ ë¦¬í¬íŠ¸ì— ë¶„ì„ì  ì¸ì‚¬ì´íŠ¸ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ë‹¤ìŒ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¦¬í¬íŠ¸ë¥¼ ê°œì„ í•´ì£¼ì„¸ìš”:

                í˜„ì¬ ë¦¬í¬íŠ¸:
                {state['final_report']}

                ì¶”ê°€ ë°ì´í„°:
                {context_summary}

                ë‹¤ìŒ ê´€ì ì—ì„œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”:
                1. í•µì‹¬ ë°œê²¬ì‚¬í•­ê³¼ ê·¸ ì˜ë¯¸
                2. ë¹„ì¦ˆë‹ˆìŠ¤ ì˜í–¥ë„ ë¶„ì„
                3. ê²½ìŸì‚¬ ëŒ€ë¹„ ìœ„ì¹˜
                4. íˆ¬ì ë° ì‚¬ì—… ê¸°íšŒ
                5. ë¦¬ìŠ¤í¬ ìš”ì†Œì™€ ëŒ€ì‘ë°©ì•ˆ

                ë¶„ì„ì ì´ê³  ì‹¤ìš©ì ì¸ ì¸ì‚¬ì´íŠ¸ê°€ í’ë¶€í•œ ë¦¬í¬íŠ¸ë¡œ ê°œì„ í•´ì£¼ì„¸ìš”.
                """

            else:  # general_improvement
                enhancement_prompt = f"""
                ë‹¤ìŒ ë¦¬í¬íŠ¸ì˜ ì „ë°˜ì ì¸ í’ˆì§ˆì„ ê°œì„ í•´ì£¼ì„¸ìš”:

                í˜„ì¬ ë¦¬í¬íŠ¸:
                {state['final_report']}

                í’ˆì§ˆ ë¬¸ì œì : {', '.join(issues)}

                ê°œì„  ìš”êµ¬ì‚¬í•­:
                1. ë…¼ë¦¬ì  êµ¬ì¡° ê°•í™” (ë„ì…-ë³¸ë¡ -ê²°ë¡ )
                2. êµ¬ì²´ì  ë°ì´í„°ì™€ ìˆ˜ì¹˜ ë³´ê°•
                3. ì „ë¬¸ì„±ê³¼ ì‹ ë¢°ì„± í–¥ìƒ
                4. ì½ê¸° ì‰¬ìš´ ë¬¸ì²´ì™€ í˜•ì‹
                5. ì‹¤í–‰ ê°€ëŠ¥í•œ ê¶Œì¥ì‚¬í•­

                ì „ë¬¸ì ì´ê³  ì™„ì„±ë„ ë†’ì€ ë¹„ì¦ˆë‹ˆìŠ¤ ë¦¬í¬íŠ¸ë¡œ ì¬ì‘ì„±í•´ì£¼ì„¸ìš”.
                """

            # LLMì„ í†µí•œ ë¦¬í¬íŠ¸ ê°œì„ 
            enhanced_response = await self._llm_invoke(enhancement_prompt)

            # ê°œì„  ê²°ê³¼ ê²€ì¦
            if len(enhanced_response) > len(state.get("final_report", "")):
                state["final_report"] = enhanced_response
                state["execution_log"].append(f"âœ… ë¦¬í¬íŠ¸ ê°œì„  ì™„ë£Œ ({enhancement_strategy})")
            else:
                state["execution_log"].append("âš ï¸ ê°œì„  íš¨ê³¼ ë¯¸ë¯¸, ê¸°ì¡´ ë¦¬í¬íŠ¸ ìœ ì§€")

        except Exception as e:
            logger.error(f"[LangGraph] ë¦¬í¬íŠ¸ ê°œì„  ì‹¤íŒ¨: {e}")
            state["execution_log"].append(f"âŒ ë¦¬í¬íŠ¸ ê°œì„  ì‹¤íŒ¨: {e}")

        return state

    def _determine_enhancement_strategy(self, issues: List[str]) -> str:
        """ê°œì„  ì „ëµ ê²°ì •"""
        if "ë¦¬í¬íŠ¸ ê¸¸ì´ ë¶€ì¡±" in issues:
            return "expand_content"
        elif "ì¸ì‚¬ì´íŠ¸ ë¶€ì¡±" in issues:
            return "add_insights"
        else:
            return "general_improvement"

    def _should_enhance_report(self, state: LangGraphReportState) -> str:
        """ê°œì„ ëœ í’ˆì§ˆ ê²€ì¦ í›„ ê°œì„  ì—¬ë¶€ ê²°ì •"""

        # í’ˆì§ˆ ë ˆë²¨ì´ ì—†ê±°ë‚˜ ì˜ëª»ëœ ê°’ì´ë©´ POORë¡œ ì„¤ì •
        if "quality_level" not in state or not isinstance(state.get("quality_level"), ReportQuality):
            state["quality_level"] = ReportQuality.POOR
            logger.warning("[LangGraph] quality_levelì´ ëˆ„ë½ë˜ì—ˆê±°ë‚˜ ì˜ëª»ëœ ê°’ì…ë‹ˆë‹¤. POORë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
            return "enhance"

        quality_level = state["quality_level"]
        retry_count = state.get("retry_count", 0)
        quality_score = state.get("quality_score", 0.0)

        # ì¬ì‹œë„ íšŸìˆ˜ ì œí•œ (ìµœëŒ€ 3íšŒ)
        if retry_count >= 3:
            logger.warning(f"[LangGraph] ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼: {retry_count}íšŒ")
            state["execution_log"].append("âš ï¸ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼, í˜„ì¬ ê²°ê³¼ë¡œ ì™„ë£Œ")
            return "complete"

        # í’ˆì§ˆ ê¸°ì¤€ì— ë”°ë¥¸ ì¬ì‹œë„ ê²°ì •
        should_retry = False

        # POOR í’ˆì§ˆì€ í•­ìƒ ì¬ì‹œë„
        if quality_level == ReportQuality.POOR:
            should_retry = True

        # ACCEPTABLEë„ ì ìˆ˜ê°€ ë„ˆë¬´ ë‚®ìœ¼ë©´ ì¬ì‹œë„
        elif quality_level == ReportQuality.ACCEPTABLE and quality_score < 0.45:
            should_retry = True

        # ë¦¬í¬íŠ¸ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ì¬ì‹œë„
        elif len(state.get("final_report", "")) < 300:
            should_retry = True
            state["execution_log"].append("âš ï¸ ë¦¬í¬íŠ¸ê°€ ë„ˆë¬´ ì§§ì•„ ì¬ì‹œë„")

        # ì¸ì‚¬ì´íŠ¸ê°€ ë¶€ì¡±í•˜ë©´ ì¬ì‹œë„
        elif len(state.get("insights", [])) < 1:
            should_retry = True
            state["execution_log"].append("âš ï¸ ì¸ì‚¬ì´íŠ¸ ë¶€ì¡±ìœ¼ë¡œ ì¬ì‹œë„")

        if should_retry:
            state["retry_count"] = retry_count + 1
            state["execution_log"].append(f"ğŸ”„ í’ˆì§ˆ ê°œì„  ì¬ì‹œë„ {state['retry_count']}íšŒì°¨ (ì ìˆ˜: {quality_score:.2f})")
            return "enhance"

        # í’ˆì§ˆì´ ì¶©ë¶„í•˜ë©´ ì™„ë£Œ
        state["execution_log"].append(f"âœ… í’ˆì§ˆ ê¸°ì¤€ ì¶©ì¡±, ë¦¬í¬íŠ¸ ì™„ë£Œ (ì ìˆ˜: {quality_score:.2f})")
        return "complete"

    # ========== í—¬í¼ ë©”ì„œë“œë“¤ ==========

    async def _llm_invoke(self, prompt: str) -> str:
        """LLM í˜¸ì¶œì„ ìœ„í•œ ë¹„ë™ê¸° ë˜í¼"""
        try:
            # OllamaLLMì€ ë™ê¸° í˜¸ì¶œë§Œ ì§€ì›í•˜ë¯€ë¡œ ë¹„ë™ê¸°ë¡œ ë˜í•‘
            import anyio
            response = await anyio.to_thread.run_sync(self.llm.invoke, prompt)
            return response
        except Exception as e:
            logger.error(f"[LangGraph] LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return "LLM ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def _safe_parse_keywords(self, content: str) -> List[str]:
        """LLM ì‘ë‹µì—ì„œ í‚¤ì›Œë“œ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ"""
        try:
            # JSON ë°°ì—´ í˜•íƒœ íŒŒì‹± ì‹œë„
            if '[' in content and ']' in content:
                start = content.find('[')
                end = content.rfind(']') + 1
                keywords_json = content[start:end]
                return json.loads(keywords_json)

            # ì½¤ë§ˆ êµ¬ë¶„ í…ìŠ¤íŠ¸ íŒŒì‹±
            keywords = [k.strip().strip('"\'') for k in content.split(',')]
            return keywords[:5]  # ìµœëŒ€ 5ê°œ
        except Exception:
            # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’
            return ["í‚¤ì›Œë“œ"]

    def _safe_parse_complexity(self, content: str) -> str:
        """LLM ì‘ë‹µì—ì„œ ë³µì¡ë„ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ"""
        content_lower = content.lower().strip()
        valid_complexities = ["shallow", "standard", "deep", "comprehensive"]

        for complexity in valid_complexities:
            if complexity in content_lower:
                return complexity

        return "standard"  # ê¸°ë³¸ê°’

    def _evaluate_content_quality(self, report: str, query: str) -> float:
        """ë¦¬í¬íŠ¸ ì»¨í…ì¸  í’ˆì§ˆ í‰ê°€"""
        if not report or len(report) < 100:
            return 0.0

        query_words = set(query.lower().split())
        report_words = set(report.lower().split())

        # í‚¤ì›Œë“œ í¬í•¨ë¥ 
        keyword_coverage = len(query_words.intersection(report_words)) / len(query_words)

        # ë¬¸ì¥ êµ¬ì¡° í‰ê°€ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
        sentences = report.split('.')
        avg_sentence_length = sum(len(s.split()) for s in sentences) / max(len(sentences), 1)

        # ì ì ˆí•œ ë¬¸ì¥ ê¸¸ì´ (10-30 ë‹¨ì–´)
        length_score = 1.0 if 10 <= avg_sentence_length <= 30 else 0.5

        return (keyword_coverage * 0.7 + length_score * 0.3)

    def _summarize_context_data(self, contexts: List[ContextItem]) -> str:
        """ì»¨í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ìš”ì•½í•˜ì—¬ í”„ë¡¬í”„íŠ¸ìš©ìœ¼ë¡œ ë³€í™˜"""
        summaries = []
        for i, ctx in enumerate(contexts[:3], 1):
            content = ctx.content
            if isinstance(content, dict):
                title = content.get('title', content.get('name', f'{ctx.type} {i}'))
                summary = f"{i}. {title}"
                if 'amount' in content:
                    summary += f" (ê¸ˆì•¡: {content['amount']})"
                if 'date' in content:
                    summary += f" ({content['date']})"
            else:
                summary = f"{i}. {str(content)[:100]}..."
            summaries.append(summary)
        return "\n".join(summaries)

    def _determine_graph_type(self, row: Dict[str, Any]) -> str:
        """ê·¸ë˜í”„ ë°ì´í„° íƒ€ì… ê²°ì • (ëª¨ë“  ìƒì¥ì‚¬ ëŒ€ì‘)"""
        labels = row.get("labels", [])

        # ê¸°ì—… ê´€ë ¨
        if "Company" in labels:
            return "company"
        elif "Industry" in labels or "Sector" in labels:
            return "industry"
        elif "Stock" in labels or "Security" in labels:
            return "stock"

        # ì¬ë¬´/ê³„ì•½ ê´€ë ¨
        elif "Contract" in labels or "Deal" in labels:
            return "contract"
        elif "Financial" in labels or "Revenue" in labels:
            return "financial"
        elif "Investment" in labels or "Funding" in labels:
            return "investment"

        # ë‰´ìŠ¤/ì´ë²¤íŠ¸ ê´€ë ¨
        elif "Event" in labels:
            return "event"
        elif "News" in labels or "Article" in labels:
            return "news"
        elif "Announcement" in labels or "Disclosure" in labels:
            return "announcement"

        # ê¸°íƒ€ ë¹„ì¦ˆë‹ˆìŠ¤ ì—”í„°í‹°
        elif "Product" in labels or "Service" in labels:
            return "product"
        elif "Person" in labels or "Executive" in labels:
            return "person"
        elif "Location" in labels or "Region" in labels:
            return "location"
        elif "Technology" in labels or "Patent" in labels:
            return "technology"

        # ê·œì œ/ì •ì±… ê´€ë ¨
        elif "Regulation" in labels or "Policy" in labels:
            return "regulation"
        elif "ESG" in labels or "Sustainability" in labels:
            return "esg"

        else:
            return "entity"

    def _calculate_relevance(self, row: Dict[str, Any], query: str) -> float:
        """ê·¸ë˜í”„ ë°ì´í„° ê´€ë ¨ì„± ê³„ì‚°"""
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­ ê¸°ë°˜ ê´€ë ¨ì„± ê³„ì‚°
        query_words = query.lower().split()
        content_text = json.dumps(row, ensure_ascii=False).lower()

        matches = sum(1 for word in query_words if word in content_text)
        return min(matches / len(query_words), 1.0)

    def _calculate_news_relevance(self, source_data: Dict[str, Any], query: str) -> float:
        """ë‰´ìŠ¤ ë°ì´í„° ê´€ë ¨ì„± ê³„ì‚°"""
        query_words = query.lower().split()
        title = source_data.get("title", "").lower()
        content = source_data.get("content", "").lower()

        title_matches = sum(1 for word in query_words if word in title)
        content_matches = sum(1 for word in query_words if word in content)

        # ì œëª© ë§¤ì¹˜ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜
        relevance = (title_matches * 2 + content_matches) / (len(query_words) * 3)
        return min(relevance, 1.0)

    async def _generate_extended_queries(self, original_query: str) -> List[str]:
        """í™•ì¥ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±"""
        expand_prompt = f"""
        ë‹¤ìŒ ì›ë³¸ ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ í™•ì¥ ê²€ìƒ‰ í‚¤ì›Œë“œ 3ê°œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:

        ì›ë³¸ ì¿¼ë¦¬: {original_query}

        ì—°ê´€ì„± ìˆëŠ” í‚¤ì›Œë“œë“¤ì„ JSON ë°°ì—´ë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”:
        ["í™•ì¥í‚¤ì›Œë“œ1", "í™•ì¥í‚¤ì›Œë“œ2", "í™•ì¥í‚¤ì›Œë“œ3"]
        """

        try:
            response = await self._llm_invoke(expand_prompt)
            extended_queries = json.loads(response)
            return extended_queries[:3]  # ìµœëŒ€ 3ê°œë§Œ
        except Exception:
            # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ í™•ì¥ í‚¤ì›Œë“œ ë°˜í™˜
            return [f"{original_query} ë¶„ì„", f"{original_query} ì „ë§", f"{original_query} ë™í–¥"]

    def _generate_data_summary(self, contexts: List[ContextItem]) -> str:
        """ë°ì´í„° ìš”ì•½ ìƒì„±"""
        summary_lines = [
            f"ğŸ“Š **ì´ ìˆ˜ì§‘ ë°ì´í„°**: {len(contexts)}ê°œ",
            "",
            "**ë°ì´í„° ì¶œì²˜ë³„ ë¶„í¬:**"
        ]

        # ì¶œì²˜ë³„ í†µê³„
        source_counts = {}
        for ctx in contexts:
            source_counts[ctx.source] = source_counts.get(ctx.source, 0) + 1

        for source, count in source_counts.items():
            summary_lines.append(f"- {source}: {count}ê°œ")

        summary_lines.extend([
            "",
            "**ë°ì´í„° í’ˆì§ˆ:**",
            f"- í‰ê·  ì‹ ë¢°ë„: {(sum(ctx.confidence for ctx in contexts) / len(contexts)) if len(contexts) else 0 :.2f}",
            f"- í‰ê·  ê´€ë ¨ì„±: {(sum(ctx.relevance for ctx in contexts) / len(contexts)) if len(contexts) else 0 :.2f}"
        ])

        return "\n".join(summary_lines)

    def _initialize_state(
        self,
        query: str,
        domain: Optional[str] = None,
        lookback_days: int = 180,
        analysis_depth: str = "standard",
        symbol: Optional[str] = None
    ) -> LangGraphReportState:
        """ì•ˆì „í•œ ìƒíƒœ ì´ˆê¸°í™”"""

        # ì…ë ¥ ê²€ì¦ ë° ì •ê·œí™”
        query = query.strip() if query else ""
        if not query:
            raise ValueError("ì¿¼ë¦¬ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")

        domain = domain.strip() if domain else None
        lookback_days = max(1, min(lookback_days, 365))  # 1~365ì¼ ë²”ìœ„ ì œí•œ

        # ë¶„ì„ ê¹Šì´ ê²€ì¦
        try:
            depth_enum = AnalysisDepth(analysis_depth)
        except ValueError:
            logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” ë¶„ì„ ê¹Šì´: {analysis_depth}, ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •")
            depth_enum = AnalysisDepth.STANDARD

        # ê¸°ë³¸ ìƒíƒœ ìƒì„±
        state = LangGraphReportState(
            # ì…ë ¥ ì •ë³´
            query=query,
            domain=domain,
            lookback_days=lookback_days,
            analysis_depth=depth_enum,

            # ìˆ˜ì§‘ëœ ì»¨í…ìŠ¤íŠ¸
            contexts=[],

            # ë¶„ì„ ê²°ê³¼
            insights=[],
            relationships=[],

            # ë¦¬í¬íŠ¸ ìƒì„±
            report_sections={},
            final_report="",

            # í’ˆì§ˆ ê´€ë¦¬
            quality_score=0.0,
            quality_level=ReportQuality.POOR,
            retry_count=0,

            # ë©”íƒ€ë°ì´í„°
            execution_log=[f"ğŸš€ LangGraph ì›Œí¬í”Œë¡œìš° ì‹œì‘ - ì¿¼ë¦¬: {query}"],
            processing_time=0.0
        )

        # ì„ íƒì  íŒŒë¼ë¯¸í„° ì¶”ê°€
        if symbol and symbol.strip():
            state["symbol"] = symbol.strip()

        return state

    # ========== ê³µê°œ API ==========

    async def generate_langgraph_report(
        self,
        query: str,
        *,
        domain: Optional[str] = None,
        lookback_days: int = 180,
        analysis_depth: str = "standard",
        symbol: Optional[str] = None
    ) -> Dict[str, Any]:
        """LangGraph ê¸°ë°˜ ê³ ê¸‰ ë¦¬í¬íŠ¸ ìƒì„±"""

        import time
        start_time = time.time()

        try:
            # ì•ˆì „í•œ ìƒíƒœ ì´ˆê¸°í™”
            initial_state = self._initialize_state(
                query=query,
                domain=domain,
                lookback_days=lookback_days,
                analysis_depth=analysis_depth,
                symbol=symbol
            )

            logger.info(f"[LangGraph] ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘ - ì¿¼ë¦¬: {query}, ê¹Šì´: {analysis_depth}")

            # LangGraph ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
            final_state = await self.workflow.ainvoke(initial_state)

            # ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
            final_state["processing_time"] = time.time() - start_time

            # í’ˆì§ˆ ë ˆë²¨ ì•ˆì „ ì ‘ê·¼
            quality_level = "poor"  # ê¸°ë³¸ê°’ì„ poorë¡œ ë³€ê²½
            if "quality_level" in final_state:
                if isinstance(final_state["quality_level"], ReportQuality):
                    quality_level = final_state["quality_level"].value
                else:
                    # ë¬¸ìì—´ì´ë‚˜ ë‹¤ë¥¸ íƒ€ì…ì¸ ê²½ìš°
                    quality_level = str(final_state["quality_level"]).lower()
                    # ìœ íš¨í•œ ê°’ì¸ì§€ í™•ì¸
                    if quality_level not in ["poor", "acceptable", "good", "excellent"]:
                        quality_level = "poor"

            logger.info(f"[LangGraph] ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ - í’ˆì§ˆì ìˆ˜: {final_state['quality_score']:.2f}, ì²˜ë¦¬ì‹œê°„: {final_state['processing_time']:.2f}ì´ˆ")

            return {
                "markdown": final_state.get("final_report", "ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."),
                "quality_score": final_state.get("quality_score", 0.0),
                "quality_level": quality_level,
                "contexts_count": len(final_state.get("contexts", [])),
                "insights_count": len(final_state.get("insights", [])),
                "relationships_count": len(final_state.get("relationships", [])),
                "processing_time": final_state.get("processing_time", 0.0),
                "retry_count": final_state.get("retry_count", 0),
                "execution_log": final_state.get("execution_log", []),
                "sections": final_state.get("report_sections", {}),
                "type": "langgraph_enhanced",
                "meta": {
                    "query": query,
                    "domain": domain,
                    "lookback_days": lookback_days,
                    "analysis_depth": analysis_depth,
                    "confidence": final_state.get("quality_score", 0.0) * 100,
                    "coverage": min(len(final_state.get("contexts", [])) / 50 * 100, 100),
                    "search_time": final_state.get("processing_time", 0.0)
                }
            }

        except ValueError as e:
            logger.error(f"ì…ë ¥ ê²€ì¦ ì˜¤ë¥˜: {e}")
            return {
                "markdown": f"# ì…ë ¥ ì˜¤ë¥˜\n\n{str(e)}",
                "quality_score": 0.0,
                "quality_level": "poor",
                "error": str(e),
                "processing_time": time.time() - start_time,
                "type": "langgraph_validation_error"
            }
        except Exception as e:
            logger.error(f"LangGraph ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            logger.error(traceback.format_exc())
            return {
                "markdown": f"# ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨\n\nì‹œìŠ¤í…œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "quality_score": 0.0,
                "quality_level": "poor",
                "error": str(e),
                "processing_time": time.time() - start_time,
                "type": "langgraph_system_error"
            }

    # ========== í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì§€ì› ë©”ì„œë“œ ==========

    async def _langgraph_hybrid_search(
        self,
        query: str,
        lookback_days: int = 180,
        size: int = 50
    ) -> List[Dict[str, Any]]:
        """LangGraph ì „ìš© í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (í•œë²ˆì˜ ì¿¼ë¦¬ë¡œ í‚¤ì›Œë“œ + ë²¡í„°)"""

        try:
            from datetime import datetime, timedelta
            cutoff_date = (datetime.now() - timedelta(days=lookback_days)).strftime("%Y-%m-%d")

            # ì„ë² ë”©ì´ ìˆìœ¼ë©´ í•˜ì´ë¸Œë¦¬ë“œ, ì—†ìœ¼ë©´ í‚¤ì›Œë“œë§Œ
            if self.embedding_client:
                try:
                    # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
                    query_embedding = await self.embedding_client.encode(query)

                    # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê²€ìƒ‰ (ì„ë² ë”©ì€ ë‚˜ì¤‘ì— í›„ì²˜ë¦¬ë¡œ)
                    hybrid_query = {
                        "size": size * 2,  # ë²¡í„° í•„í„°ë§ì„ ìœ„í•´ ë” ë§ì´ ê°€ì ¸ì˜´
                        "query": {
                            "bool": {
                                "should": [
                                    {
                                        "multi_match": {
                                            "query": query,
                                            "fields": ["title^3", "text^2", "content"],
                                            "type": "best_fields",
                                            "fuzziness": "AUTO"
                                        }
                                    },
                                    {
                                        "match_phrase": {
                                            "title": {
                                                "query": query,
                                                "boost": 2.0
                                            }
                                        }
                                    }
                                ],
                                "filter": [
                                    {
                                        "range": {
                                            "created_datetime": {
                                                "gte": cutoff_date
                                            }
                                        }
                                    }
                                ]
                            }
                        },
                        "_source": ["text", "title", "url", "created_datetime", "metadata"]
                    }

                    result = await self.opensearch.search(
                        index=settings.news_bulk_index,
                        query=hybrid_query,
                        size=size * 2
                    )
                    hits = result.get("hits", {}).get("hits", [])

                    # ë²¡í„° ìœ ì‚¬ë„ë¡œ ì¬ì •ë ¬
                    if hits:
                        scored_hits = []
                        for hit in hits:
                            source = hit.get("_source", {})
                            doc_text = f"{source.get('title', '')} {source.get('text', source.get('content', ''))}"

                            if doc_text.strip():
                                try:
                                    doc_embedding = await self.embedding_client.encode(doc_text[:500])
                                    similarity = self._calculate_cosine_similarity(query_embedding, doc_embedding)

                                    # í‚¤ì›Œë“œ ì ìˆ˜ì™€ ë²¡í„° ìœ ì‚¬ë„ ê²°í•©
                                    keyword_score = hit.get("_score", 0)
                                    combined_score = (keyword_score * 0.4) + (similarity * 10)  # ë²¡í„°ì— ë†’ì€ ê°€ì¤‘ì¹˜

                                    hit["_vector_similarity"] = similarity
                                    hit["_combined_score"] = combined_score
                                    scored_hits.append(hit)

                                except Exception as e:
                                    logger.warning(f"ë¬¸ì„œ ì„ë² ë”© ì‹¤íŒ¨: {e}")
                                    hit["_combined_score"] = hit.get("_score", 0)
                                    scored_hits.append(hit)

                        # ê²°í•© ì ìˆ˜ë¡œ ì •ë ¬ í›„ ìƒìœ„ Nê°œ ë°˜í™˜
                        scored_hits.sort(key=lambda x: x.get("_combined_score", 0), reverse=True)
                        hits = scored_hits[:size]

                    logger.info(f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì™„ë£Œ: {len(hits)}ê±´")
                    return hits

                except Exception as e:
                    logger.warning(f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤íŒ¨, í‚¤ì›Œë“œë§Œ ì‚¬ìš©: {e}")

            # í‚¤ì›Œë“œ ê²€ìƒ‰ë§Œ
            keyword_query = {
                "size": size,
                "query": {
                    "bool": {
                        "should": [
                            {
                                "multi_match": {
                                    "query": query,
                                    "fields": ["title^3", "text^2", "content"],
                                    "type": "best_fields",
                                    "fuzziness": "AUTO"
                                }
                            }
                        ],
                        "filter": [
                            {
                                "range": {
                                    "created_datetime": {
                                        "gte": cutoff_date
                                    }
                                }
                            }
                        ]
                    }
                },
                "_source": ["text", "title", "url", "created_datetime", "metadata"]
            }

            result = await self.opensearch.search(
                index=settings.news_bulk_index,
                query=keyword_query,
                size=size
            )
            hits = result.get("hits", {}).get("hits", [])
            logger.info(f"í‚¤ì›Œë“œ ê²€ìƒ‰ ì™„ë£Œ: {len(hits)}ê±´")
            return hits

        except Exception as e:
            logger.error(f"ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    def _calculate_cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """ë‘ ë²¡í„° ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        try:
            import numpy as np

            # numpy ë°°ì—´ë¡œ ë³€í™˜
            a = np.array(vec1)
            b = np.array(vec2)

            # ë²¡í„° í¬ê¸°ê°€ ë‹¤ë¥´ë©´ 0 ë°˜í™˜
            if len(a) != len(b):
                return 0.0

            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            dot_product = np.dot(a, b)
            norm_a = np.linalg.norm(a)
            norm_b = np.linalg.norm(b)

            # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
            if norm_a == 0 or norm_b == 0:
                return 0.0

            similarity = dot_product / (norm_a * norm_b)

            # -1 ~ 1 ë²”ìœ„ë¥¼ 0 ~ 1 ë²”ìœ„ë¡œ ì •ê·œí™”
            return (similarity + 1.0) / 2.0

        except Exception as e:
            logger.warning(f"ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0

    def _calculate_graph_relevance(self, row: Dict[str, Any], query: str) -> float:
        """ê·¸ë˜í”„ ë°ì´í„°ì˜ ê´€ë ¨ì„± ê³„ì‚°"""
        try:
            relevance = 0.0
            query_lower = query.lower()

            # ë¼ë²¨ ê¸°ë°˜ ê´€ë ¨ì„±
            labels = row.get("labels", [])
            for label in labels:
                if any(keyword in label.lower() for keyword in query_lower.split()):
                    relevance += 0.3

            # ì†ì„± ê¸°ë°˜ ê´€ë ¨ì„±
            for key, value in row.items():
                if isinstance(value, str) and any(keyword in value.lower() for keyword in query_lower.split()):
                    relevance += 0.2

            # ê³„ì•½ ê¸ˆì•¡ì´ ìˆìœ¼ë©´ ê°€ì¤‘ì¹˜ ì¶”ê°€
            if "amount" in row and row.get("amount"):
                relevance += 0.1

            return min(relevance, 1.0)

        except Exception:
            return 0.5  # ê¸°ë³¸ê°’

    def _generate_sector_specific_insight_prompt(self, ctx_type: str, query: str, data_summary: str) -> str:
        """ì„¹í„°ë³„ ë§ì¶¤í˜• ì¸ì‚¬ì´íŠ¸ í”„ë¡¬í”„íŠ¸ ìƒì„±"""

        base_template = f"ì§ˆì˜ '{query}'ì— ëŒ€í•œ {ctx_type} ë°ì´í„° ë¶„ì„:\n\n{data_summary}\n\n"

        if ctx_type == "company":
            return base_template + """
ìƒì¥ì‚¬ ê¸°ì—… ë¶„ì„ ê´€ì ì—ì„œ ë‹¤ìŒ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:
1. [ê¸°ì—… ê²½ìŸë ¥] - ì‹œì¥ ë‚´ í¬ì§€ì…˜ ë° ì°¨ë³„í™” ìš”ì†Œ
2. [ì¬ë¬´ ê±´ì „ì„±] - ìˆ˜ìµì„±, ì„±ì¥ì„±, ì•ˆì •ì„± ì¢…í•© í‰ê°€
3. [íˆ¬ì ë§¤ë ¥ë„] - ì£¼ê°€ ì „ë§ ë° íˆ¬ì í¬ì¸íŠ¸
4. [ë¦¬ìŠ¤í¬ ìš”ì†Œ] - ì£¼ìš” ìš°ë ¤ì‚¬í•­ ë° ëŒ€ì‘ ì „ëµ

ê° í•­ëª©ì„ 2-3ë¬¸ì¥ìœ¼ë¡œ êµ¬ì²´ì ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”."""

        elif ctx_type in ["financial", "investment", "stock"]:
            return base_template + """
ì¬ë¬´/íˆ¬ì ë¶„ì„ ê´€ì ì—ì„œ ë‹¤ìŒ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:
1. [ìˆ˜ìµì„± ë¶„ì„] - ë§¤ì¶œ, ì˜ì—…ì´ìµ, ìˆœì´ìµ ì¶”ì„¸ ë° ë§ˆì§„ ë¶„ì„
2. [ì„±ì¥ì„± í‰ê°€] - ì „ë…„ ëŒ€ë¹„ ì„±ì¥ë¥  ë° ë¯¸ë˜ ì„±ì¥ ë™ë ¥
3. [ë°¸ë¥˜ì—ì´ì…˜] - ì£¼ê°€ ì ì •ì„± ë° íˆ¬ì ì‹œì  íŒë‹¨
4. [ë°°ë‹¹/ì£¼ì£¼í™˜ì›] - ë°°ë‹¹ ì •ì±… ë° ì£¼ì£¼ ì¹œí™” ì •ì±…

íˆ¬ìì ê´€ì ì—ì„œ ì‹¤ìš©ì ì¸ ë¶„ì„ì„ í•´ì£¼ì„¸ìš”."""

        elif ctx_type in ["news", "announcement"]:
            return base_template + """
ë‰´ìŠ¤/ê³µì‹œ ë¶„ì„ ê´€ì ì—ì„œ ë‹¤ìŒ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:
1. [ì‹œì¥ ë°˜ì‘] - ë‰´ìŠ¤ê°€ ì£¼ê°€ ë° ì‹œì¥ ì„¼í‹°ë¨¼íŠ¸ì— ë¯¸ì¹˜ëŠ” ì˜í–¥
2. [ì‚¬ì—… ì˜í–¥] - ê¸°ì—…ì˜ ì¤‘ì¥ê¸° ì‚¬ì—… ì „ëµì— ëŒ€í•œ ì‹œì‚¬ì 
3. [ê²½ìŸ í™˜ê²½] - ë™ì¢… ì—…ê³„ ë° ê²½ìŸì‚¬ ëŒ€ë¹„ ìƒëŒ€ì  ìœ„ì¹˜ ë³€í™”
4. [íˆ¬ì ì„íŒ©íŠ¸] - íˆ¬ìì í–‰ë™ ë³€í™” ë° ì£¼ëª©í•´ì•¼ í•  í¬ì¸íŠ¸

ì‹œì¥ ë¶„ì„ê°€ ê´€ì ì—ì„œ ì „ë¬¸ì ì¸ í•´ì„ì„ í•´ì£¼ì„¸ìš”."""

        elif ctx_type in ["industry", "sector"]:
            return base_template + """
ì‚°ì—…/ì„¹í„° ë¶„ì„ ê´€ì ì—ì„œ ë‹¤ìŒ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:
1. [ì‚°ì—… íŠ¸ë Œë“œ] - ì—…ê³„ ì „ë°˜ì˜ ì„±ì¥ ë™ë ¥ ë° ë³€í™” ìš”ì¸
2. [ê²½ìŸ êµ¬ë„] - ì£¼ìš” í”Œë ˆì´ì–´ë“¤ì˜ ì‹œì¥ ì ìœ ìœ¨ ë° ê²½ìŸ ì „ëµ
3. [ê·œì œ í™˜ê²½] - ì •ì±… ë³€í™”ê°€ ì—…ê³„ì— ë¯¸ì¹˜ëŠ” ì˜í–¥
4. [íˆ¬ì ê¸°íšŒ] - ì„¹í„° ë‚´ ìœ ë§ ì¢…ëª© ë° íˆ¬ì í…Œë§ˆ

ì„¹í„° ì• ë„ë¦¬ìŠ¤íŠ¸ ê´€ì ì—ì„œ ì¢…í•©ì ì¸ ë¶„ì„ì„ í•´ì£¼ì„¸ìš”."""

        elif ctx_type in ["technology", "product"]:
            return base_template + """
ê¸°ìˆ /ì œí’ˆ ë¶„ì„ ê´€ì ì—ì„œ ë‹¤ìŒ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:
1. [ê¸°ìˆ  í˜ì‹ ] - ì‹ ê¸°ìˆ  ë„ì… ë° íŠ¹í—ˆ í¬íŠ¸í´ë¦¬ì˜¤ ê°•í™” íš¨ê³¼
2. [ì‹œì¥ í™•ì¥] - ì‹ ì œí’ˆ/ì„œë¹„ìŠ¤ë¥¼ í†µí•œ ì‹œì¥ í™•ëŒ€ ê°€ëŠ¥ì„±
3. [ìˆ˜ìµ ê¸°ì—¬] - ê¸°ìˆ /ì œí’ˆì´ ë§¤ì¶œ ë° ìˆ˜ìµì„±ì— ë¯¸ì¹˜ëŠ” ì˜í–¥
4. [ê²½ìŸ ìš°ìœ„] - ê¸°ìˆ ë ¥ ê¸°ë°˜ ì§€ì†ê°€ëŠ¥í•œ ê²½ìŸ ìš°ìœ„ êµ¬ì¶•

ê¸°ìˆ  ë¶„ì„ ì „ë¬¸ê°€ ê´€ì ì—ì„œ í‰ê°€í•´ì£¼ì„¸ìš”."""

        else:
            # ê¸°ë³¸ í…œí”Œë¦¿
            return base_template + """
ë‹¤ìŒ ê´€ì ì—ì„œ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:
1. [ì£¼ìš” ë°œê²¬ì‚¬í•­] - ë°ì´í„°ì—ì„œ ë‚˜íƒ€ë‚˜ëŠ” í•µì‹¬ íŠ¸ë Œë“œ
2. [ë¹„ì¦ˆë‹ˆìŠ¤ ì˜í–¥] - ê¸°ì—… ìš´ì˜ ë° ì „ëµì— ë¯¸ì¹˜ëŠ” ì˜í–¥
3. [ì‹œì¥ ì˜ë¯¸] - ì£¼ì‹ì‹œì¥ ë° íˆ¬ììë“¤ì—ê²Œ ì£¼ëŠ” ì‹œì‚¬ì 
4. [í–¥í›„ ì „ë§] - ì˜ˆìƒë˜ëŠ” ë³€í™” ë° ì£¼ëª©í•  í¬ì¸íŠ¸

ì „ë¬¸ ì• ë„ë¦¬ìŠ¤íŠ¸ ê´€ì ì—ì„œ ë¶„ì„í•´ì£¼ì„¸ìš”."""

    async def _collect_stock_data(self, state: LangGraphReportState, contexts: List[ContextItem]):
        """ì£¼ê°€ ë° ì¬ë¬´ ë°ì´í„° ìˆ˜ì§‘"""
        try:
            # ê¸°ì—…ëª… ë˜ëŠ” ì‹¬ë³¼ì—ì„œ ì£¼ê°€ ë°ì´í„° ìˆ˜ì§‘
            company_names = []
            symbols = []

            # ì»¨í…ìŠ¤íŠ¸ì—ì„œ ê¸°ì—… ì •ë³´ ì¶”ì¶œ
            for ctx in contexts:
                if ctx.type == "company":
                    content = ctx.content
                    if isinstance(content, dict):
                        if "name" in content:
                            company_names.append(content["name"])
                        if "symbol" in content or "ticker" in content:
                            symbol = content.get("symbol") or content.get("ticker")
                            if symbol:
                                symbols.append(symbol)

            # ì¿¼ë¦¬ì—ì„œë„ ì‹¬ë³¼ ì¶”ì¶œ ì‹œë„
            if state.get("symbol"):
                symbols.append(state["symbol"])

            # ì£¼ê°€ ë°ì´í„° ìˆ˜ì§‘
            for symbol in symbols[:3]:  # ìµœëŒ€ 3ê°œ ì‹¬ë³¼
                try:
                    # StockMCPë¥¼ í†µí•œ ì£¼ê°€ ë°ì´í„° ìˆ˜ì§‘
                    stock_data = await self._fetch_stock_info(symbol)

                    if stock_data:
                        stock_context = ContextItem(
                            source="stock_api",
                            type="stock",
                            content=stock_data,
                            confidence=0.9,
                            relevance=0.8,
                            timestamp=str(asyncio.get_event_loop().time())
                        )
                        contexts.append(stock_context)

                except Exception as e:
                    logger.warning(f"ì£¼ê°€ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨ ({symbol}): {e}")

        except Exception as e:
            logger.warning(f"ì£¼ê°€ ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")

    async def _fetch_stock_info(self, symbol: str) -> Dict[str, Any]:
        """ì£¼ì‹ ì •ë³´ ì¡°íšŒ"""
        try:
            # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ ë”ë¯¸ ë°ì´í„°ë¥¼ ë°˜í™˜
            # ì‹¤ì œë¡œëŠ” ì™¸ë¶€ API (Yahoo Finance, Alpha Vantage ë“±) ì—°ë™
            return {
                "symbol": symbol,
                "current_price": "ì¡°íšŒ í•„ìš”",
                "price_change": "ì¡°íšŒ í•„ìš”",
                "price_change_percent": "ì¡°íšŒ í•„ìš”",
                "volume": "ì¡°íšŒ í•„ìš”",
                "market_cap": "ì¡°íšŒ í•„ìš”",
                "pe_ratio": "ì¡°íšŒ í•„ìš”",
                "dividend_yield": "ì¡°íšŒ í•„ìš”",
                "52_week_high": "ì¡°íšŒ í•„ìš”",
                "52_week_low": "ì¡°íšŒ í•„ìš”",
                "note": "ì‹¤ì œ API ì—°ë™ ì‹œ ì‹¤ì‹œê°„ ë°ì´í„° ì œê³µ"
            }

        except Exception as e:
            logger.error(f"ì£¼ì‹ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨ ({symbol}): {e}")
            return None

    def _evaluate_investment_content_quality(self, report: str, query: str, investment_keywords: List[str]) -> float:
        """íˆ¬ì ê´€ë ¨ ì»¨í…ì¸  í’ˆì§ˆ í‰ê°€"""
        try:
            if not report or len(report) < 100:
                return 0.0

            # íˆ¬ì í‚¤ì›Œë“œ í¬í•¨ ì •ë„
            keyword_count = sum(1 for keyword in investment_keywords if keyword in report)
            keyword_score = min(keyword_count / len(investment_keywords), 1.0)

            # ì¿¼ë¦¬ ê´€ë ¨ì„±
            query_words = set(query.lower().split())
            report_words = set(report.lower().split())
            relevance_score = len(query_words.intersection(report_words)) / len(query_words)

            # íˆ¬ì ë¶„ì„ ë¬¸êµ¬ í™•ì¸
            analysis_phrases = ["íˆ¬ì í¬ì¸íŠ¸", "ë¦¬ìŠ¤í¬ ìš”ì¸", "ëª©í‘œì£¼ê°€", "íˆ¬ìì˜ê²¬", "ì¬ë¬´ ë¶„ì„"]
            phrase_score = sum(1 for phrase in analysis_phrases if phrase in report) / len(analysis_phrases)

            # ê°€ì¤‘ í‰ê· 
            return (keyword_score * 0.4 + relevance_score * 0.3 + phrase_score * 0.3)

        except Exception:
            return 0.5

    def _evaluate_insight_depth(self, insights: List[Dict[str, Any]]) -> float:
        """ì¸ì‚¬ì´íŠ¸ ê¹Šì´ í‰ê°€"""
        try:
            if not insights:
                return 0.0

            depth_score = 0.0
            total_insights = len(insights)

            for insight in insights:
                content = insight.get("content", "")
                if isinstance(content, str):
                    # ë¶„ì„ ê¹Šì´ ì§€í‘œë“¤
                    depth_indicators = [
                        "ë¶„ì„", "ì˜í–¥", "ì „ë§", "ì˜ˆìƒ", "í‰ê°€",
                        "ë¹„êµ", "ë³€í™”", "ì„±ì¥", "ìœ„í—˜", "ê¸°íšŒ"
                    ]

                    indicator_count = sum(1 for indicator in depth_indicators if indicator in content)

                    # ë¬¸ì¥ ê¸¸ì´ë„ ê³ ë ¤ (ë” ê¸´ ì„¤ëª… = ë” ê¹Šì€ ë¶„ì„)
                    sentence_length_score = min(len(content) / 200, 1.0)

                    insight_depth = (indicator_count / len(depth_indicators)) * 0.7 + sentence_length_score * 0.3
                    depth_score += insight_depth

            return min(depth_score / total_insights, 1.0)

        except Exception:
            return 0.5