# src/ontology_chat/services/langgraph_report_service.py
"""
LangGraph ê¸°ë°˜ ê³ ê¸‰ ì»¨í…ìŠ¤íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ë¦¬í¬íŠ¸ ìƒì„± ì„œë¹„ìŠ¤

ì£¼ìš” íŠ¹ì§•:
1. ë‹¤ë‹¨ê³„ ì •ë³´ ìˆ˜ì§‘ ë° ê²€ì¦
2. ì»¨í…ìŠ¤íŠ¸ ê°„ ê´€ê³„ ë¶„ì„
3. ë™ì  ë¶„ì„ ê¹Šì´ ì¡°ì ˆ
4. í’ˆì§ˆ ê¸°ë°˜ ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜
5. êµ¬ì¡°í™”ëœ ì¸ì‚¬ì´íŠ¸ ìƒì„±
"""

from __future__ import annotations
import asyncio
from typing import Any, Dict, List, Optional, TypedDict, Annotated
from dataclasses import dataclass
from enum import Enum
import json

from langgraph.graph import StateGraph, END
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
# from langchain_openai import ChatOpenAI
from langchain_ollama import OllamaLLM

from api.logging import setup_logging
from api.adapters.mcp_neo4j import Neo4jMCP
from api.adapters.mcp_opensearch import OpenSearchMCP
from api.adapters.mcp_stock import StockMCP
from api.adapters.ollama_embedding import OllamaEmbeddingMCP
from api.services.report_service import ReportService
from api.config import settings
import traceback 
from icecream import ic
logger = setup_logging()

# ========== ìƒíƒœ ì •ì˜ ==========

class AnalysisDepth(Enum):
    SHALLOW = "shallow"      # ê¸°ë³¸ ì •ë³´ë§Œ
    STANDARD = "standard"    # ì¼ë°˜ì  ë¶„ì„
    DEEP = "deep"           # ì‹¬í™” ë¶„ì„
    COMPREHENSIVE = "comprehensive"  # ì¢…í•© ë¶„ì„

class ReportQuality(Enum):
    POOR = "poor"           # ì¬ì‹œë„ í•„ìš”
    ACCEPTABLE = "acceptable"  # ìµœì†Œ ê¸°ì¤€ ì¶©ì¡±
    GOOD = "good"           # ì–‘í˜¸
    EXCELLENT = "excellent"    # ìš°ìˆ˜

@dataclass
class ContextItem:
    """ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ë‹¨ìœ„"""
    source: str          # ì¶œì²˜ (neo4j, opensearch, stock, llm)
    type: str           # ìœ í˜• (news, contract, company, analysis)
    content: Dict[str, Any]  # ì‹¤ì œ ë°ì´í„°
    confidence: float   # ì‹ ë¢°ë„ (0.0-1.0)
    relevance: float    # ê´€ë ¨ì„± (0.0-1.0)
    timestamp: str      # ìˆ˜ì§‘ ì‹œê°„

class LangGraphReportState(TypedDict):
    """LangGraph ìƒíƒœ ì •ì˜ (ê³ í’ˆì§ˆ ë³´ê³ ì„œìš© í™•ì¥)"""
    # ì…ë ¥ ì •ë³´
    query: str
    domain: Optional[str]
    lookback_days: int
    analysis_depth: AnalysisDepth

    # Phase 1: ë¶„ì„ ê³„íš (NEW)
    analysis_plan: Optional[Dict[str, Any]]  # ë¶„ì„ ì „ëµ ë° ë°ì´í„° ìš”êµ¬ì‚¬í•­

    # Phase 2: ìˆ˜ì§‘ëœ ì»¨í…ìŠ¤íŠ¸
    contexts: List[ContextItem]

    # Phase 4: ë¶„ì„ ê²°ê³¼ (ë¶„ë¦¬ ë³µì›)
    insights: List[Dict[str, Any]]
    relationships: List[Dict[str, Any]]
    deep_reasoning: Optional[Dict[str, Any]]  # ì‹¬í™” ì¶”ë¡  (NEW)

    # Phase 5: ë¦¬í¬íŠ¸ ìƒì„±
    report_sections: Dict[str, str]
    final_report: str

    # Phase 6: í’ˆì§ˆ ê´€ë¦¬
    quality_score: float
    quality_level: ReportQuality
    retry_count: int

    # ë©”íƒ€ë°ì´í„°
    execution_log: List[str]
    processing_time: float

# ========== LangGraph ë…¸ë“œ í•¨ìˆ˜ë“¤ ==========

class LangGraphReportEngine:
    """LangGraph ê¸°ë°˜ ë¦¬í¬íŠ¸ ìƒì„± ì—”ì§„"""

    def __init__(self):
        self.neo4j = Neo4jMCP()
        self.opensearch = OpenSearchMCP()
        self.stock = StockMCP()
        self.report_service = ReportService()

        # Ollama ì„ë² ë”© í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ì§ì ‘ ë²¡í„° ìœ ì‚¬ë„ ê³„ì‚°ìš©)
        try:
            self.embedding_client = OllamaEmbeddingMCP()
            logger.info(f"[LangGraph] Ollama ì„ë² ë”© í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"Ollama ì„ë² ë”© ì´ˆê¸°í™” ì‹¤íŒ¨, í‚¤ì›Œë“œ ê²€ìƒ‰ë§Œ ì‚¬ìš©: {e}")
            self.embedding_client = None

        # LLM ì´ˆê¸°í™” (Deep Analysisìš© ê³ í’ˆì§ˆ ëª¨ë¸)
        self.llm = OllamaLLM(
            model=settings.ollama_report_model,  # ë³€ê²½: ollama_model â†’ ollama_report_model
            base_url=settings.get_ollama_base_url(),
            temperature=0.1,
            num_predict=4000
        )
        logger.info(f"[LangGraph] LLM ì´ˆê¸°í™” ì™„ë£Œ (Deep Analysis): {settings.ollama_report_model} @ {settings.get_ollama_base_url()}")

        # LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„±
        self.workflow = self._build_workflow()

    def _build_workflow(self) -> StateGraph:
        """LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„± (ê³ í’ˆì§ˆ ë³´ê³ ì„œìš© í™•ì¥ ë²„ì „)"""

        workflow = StateGraph(LangGraphReportState)

        # Phase 1: ì´í•´ ë° ê³„íš
        workflow.add_node("analyze_query", self._analyze_query)
        workflow.add_node("plan_analysis", self._plan_analysis)  # NEW

        # Phase 2: ë°ì´í„° ìˆ˜ì§‘
        workflow.add_node("collect_parallel_data", self._collect_parallel_data)

        # Phase 2.5: Context Engineering (NEW)
        workflow.add_node("apply_context_engineering", self._apply_context_engineering)

        # Phase 3: ê²€ì¦ ë° í•„í„°ë§
        workflow.add_node("cross_validate_contexts", self._cross_validate_contexts)

        # Phase 4: ë¶„ì„ (ë¶„ë¦¬ ë³µì› - ê³ í’ˆì§ˆ ìš°ì„ )
        workflow.add_node("generate_insights", self._generate_insights)  # ë³µì›
        workflow.add_node("analyze_relationships", self._analyze_relationships)  # ë³µì›
        workflow.add_node("deep_reasoning", self._deep_reasoning)  # NEW

        # Phase 5: í•©ì„±
        workflow.add_node("synthesize_report", self._synthesize_report)  # ë³µì›

        # Phase 6: í’ˆì§ˆ ê´€ë¦¬
        workflow.add_node("quality_check", self._quality_check)
        workflow.add_node("enhance_report", self._enhance_report)

        # ì›Œí¬í”Œë¡œìš° ì—°ê²° (ê³ í’ˆì§ˆ ì„ í˜• íŒŒì´í”„ë¼ì¸)
        workflow.set_entry_point("analyze_query")

        # Phase 1: ì´í•´ â†’ ê³„íš
        workflow.add_edge("analyze_query", "plan_analysis")

        # Phase 2: ê³„íš â†’ ìˆ˜ì§‘
        workflow.add_edge("plan_analysis", "collect_parallel_data")

        # Phase 2.5: ìˆ˜ì§‘ â†’ Context Engineering
        workflow.add_edge("collect_parallel_data", "apply_context_engineering")

        # Phase 3: Context Engineering â†’ ê²€ì¦
        workflow.add_edge("apply_context_engineering", "cross_validate_contexts")

        # Phase 4: ê²€ì¦ â†’ ì¸ì‚¬ì´íŠ¸ â†’ ê´€ê³„ â†’ ì¶”ë¡ 
        workflow.add_edge("cross_validate_contexts", "generate_insights")
        workflow.add_edge("generate_insights", "analyze_relationships")
        workflow.add_edge("analyze_relationships", "deep_reasoning")

        # Phase 5: ì¶”ë¡  â†’ ë³´ê³ ì„œ ì‘ì„±
        workflow.add_edge("deep_reasoning", "synthesize_report")

        # Phase 6: ë³´ê³ ì„œ â†’ í’ˆì§ˆê²€ì‚¬
        workflow.add_edge("synthesize_report", "quality_check")

        # ì¡°ê±´ë¶€ ë¶„ê¸°: í’ˆì§ˆ ë‚®ìœ¼ë©´ ê°œì„ , ë†’ìœ¼ë©´ ì™„ë£Œ
        workflow.add_conditional_edges(
            "quality_check",
            self._should_enhance_report,
            {
                "enhance": "enhance_report",
                "complete": END
            }
        )
        workflow.add_edge("enhance_report", "quality_check")

        return workflow.compile()

    async def _analyze_query(self, state: LangGraphReportState) -> LangGraphReportState:
        """1ë‹¨ê³„: í†µí•© ì¿¼ë¦¬ ë¶„ì„ (ê³ ë„í™”ëœ ë‹¨ì¼ í”„ë¡¬í”„íŠ¸)"""
        import time
        start_time = time.time()

        state["execution_log"].append("ğŸ” í†µí•© ì¿¼ë¦¬ ë¶„ì„ ì‹œì‘")
        logger.info(f"[LangGraph-1] í†µí•© ì¿¼ë¦¬ ë¶„ì„ ì‹œì‘: {state['query']}")

        try:
            # í†µí•© ë¶„ì„ í”„ë¡¬í”„íŠ¸ (2íšŒ â†’ 1íšŒ)
            unified_prompt = f"""ë‹¹ì‹ ì€ ê¸ˆìœµ ì‹œì¥ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì§ˆì˜ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”.

ì§ˆì˜: "{state['query']}"

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì •í™•íˆ ì‘ë‹µí•˜ì„¸ìš” (ë‹¤ë¥¸ ì„¤ëª… ì—†ì´ JSONë§Œ):
{{
  "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", "í‚¤ì›Œë“œ3"],
  "entities": {{
    "companies": ["íšŒì‚¬ëª…1", "íšŒì‚¬ëª…2"],
    "products": [],
    "sectors": []
  }},
  "complexity": "shallow",
  "focus_areas": ["ë¶„ì„ ì´ˆì  1", "ì´ˆì  2"],
  "requirements": {{
    "ì‹œê³„ì—´_ë¶„ì„": false,
    "ë¹„êµ_ë¶„ì„": false,
    "ì¬ë¬´_ë¶„ì„": false
  }}
}}

ë¶„ì„ ì§€ì¹¨:
- keywords: íˆ¬ìì ê´€ì ì˜ í•µì‹¬ í‚¤ì›Œë“œ 3-5ê°œ
- entities: íšŒì‚¬ëª…, ì œí’ˆëª…, ì‚°ì—… ë¶„ë¥˜
- complexity: "shallow"(ë‹¨ìˆœ ì¡°íšŒ), "standard"(ì¼ë°˜ ë¶„ì„), "deep"(ì‹¬ì¸µ ë¶„ì„), "comprehensive"(ë¹„êµ/ì „ëµ ë¶„ì„)
- focus_areas: ì§ˆì˜ì˜ í•µì‹¬ ë¶„ì„ ì˜ì—­
- requirements: í•„ìš”í•œ ë¶„ì„ ìœ í˜• íŒë‹¨
"""

            response = await self._llm_invoke(unified_prompt)

            # JSON íŒŒì‹±
            import re
            # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                analysis = json.loads(json_match.group())

                keywords = analysis.get("keywords", [state['query']])
                complexity = analysis.get("complexity", "standard")
                focus_areas = analysis.get("focus_areas", [state['query']])
                entities = analysis.get("entities", {})
                requirements = analysis.get("requirements", {})

                state["analysis_depth"] = AnalysisDepth(complexity)
                state["query_analysis"] = {
                    "keywords": keywords,
                    "complexity": complexity,
                    "focus_areas": focus_areas,
                    "entities": entities,
                    "requirements": requirements
                }

                state["execution_log"].append(
                    f"âœ… í†µí•© ë¶„ì„ ì™„ë£Œ: {complexity}, í‚¤ì›Œë“œ {len(keywords)}ê°œ, "
                    f"ê¸°ì—… {len(entities.get('companies', []))}ê°œ"
                )

            else:
                raise ValueError("JSON íŒŒì‹± ì‹¤íŒ¨")

        except Exception as e:
            logger.warning(f"í†µí•© ì¿¼ë¦¬ ë¶„ì„ ì˜¤ë¥˜: {e}, í´ë°± ëª¨ë“œ")
            # í´ë°±: ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ ë¶„ì„
            state["analysis_depth"] = AnalysisDepth.STANDARD
            state["query_analysis"] = {
                "keywords": [state['query']],
                "complexity": "standard",
                "focus_areas": [state['query']],
                "entities": {"companies": [], "products": [], "sectors": []},
                "requirements": {}
            }
            state["execution_log"].append("âš ï¸ í´ë°±: ê¸°ë³¸ ë¶„ì„ ëª¨ë“œ")

        elapsed = time.time() - start_time
        logger.info(f"[LangGraph-1] í†µí•© ì¿¼ë¦¬ ë¶„ì„ ì™„ë£Œ: {elapsed:.3f}ì´ˆ")
        return state

    async def _plan_analysis(self, state: LangGraphReportState) -> LangGraphReportState:
        """1.5ë‹¨ê³„: ë¶„ì„ ì „ëµ ìˆ˜ë¦½ (NEW - ê³ í’ˆì§ˆ ë³´ê³ ì„œìš©)

        ëª©ì : ì–´ë–¤ ë°ì´í„°ë¥¼ ì–´ë–»ê²Œ ë¶„ì„í• ì§€ ëª…í™•í•œ ê³„íš ìˆ˜ë¦½
        """
        import time
        start_time = time.time()

        state["execution_log"].append("ğŸ“‹ ë¶„ì„ ì „ëµ ìˆ˜ë¦½ ì‹œì‘")
        logger.info(f"[LangGraph-1.5] ë¶„ì„ ì „ëµ ìˆ˜ë¦½ ì‹œì‘")

        try:
            query_analysis = state.get("query_analysis", {})
            entities = query_analysis.get("entities", {})
            focus_areas = query_analysis.get("focus_areas", [])

            # ë¶„ì„ ì „ëµ í”„ë¡¬í”„íŠ¸
            strategy_prompt = f"""ë‹¹ì‹ ì€ ê¸ˆìœµ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì§ˆì˜ì— ëŒ€í•œ ì¢…í•©ì ì¸ ë¶„ì„ ì „ëµì„ ìˆ˜ë¦½í•˜ì„¸ìš”.

ì§ˆì˜: "{state['query']}"
ê°ì§€ëœ ì—”í‹°í‹°: {entities}
ì´ˆì  ì˜ì—­: {focus_areas}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ë¶„ì„ ê³„íšì„ ì‘ì„±í•˜ì„¸ìš”:
{{
  "primary_focus": ["ì£¼ìš” ë¶„ì„ ëª©í‘œ1", "ëª©í‘œ2"],
  "comparison_axes": ["ë¹„êµ ê¸°ì¤€1", "ê¸°ì¤€2"],
  "required_data_types": ["í•„ìš”í•œ ë°ì´í„° ìœ í˜•"],
  "expected_insights": ["ì˜ˆìƒë˜ëŠ” ì¸ì‚¬ì´íŠ¸"],
  "analysis_approach": {{
    "quantitative": ["ìˆ˜ì¹˜ ë¶„ì„ í•­ëª©"],
    "qualitative": ["ì •ì„± ë¶„ì„ í•­ëª©"],
    "temporal": ["ì‹œê³„ì—´ ë¶„ì„ í•„ìš” ì—¬ë¶€"]
  }},
  "key_questions": ["ë‹µí•´ì•¼ í•  í•µì‹¬ ì§ˆë¬¸ë“¤"]
}}

JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”:
"""

            response = await self._llm_invoke(strategy_prompt)

            # JSON íŒŒì‹±
            import json
            import re
            json_match = re.search(r'\{[\s\S]*\}', response)
            if json_match:
                analysis_plan = json.loads(json_match.group(0))
                state["analysis_plan"] = analysis_plan

                state["execution_log"].append(
                    f"âœ… ë¶„ì„ ì „ëµ ìˆ˜ë¦½ ì™„ë£Œ: "
                    f"{len(analysis_plan.get('primary_focus', []))}ê°œ ëª©í‘œ, "
                    f"{len(analysis_plan.get('key_questions', []))}ê°œ í•µì‹¬ ì§ˆë¬¸"
                )

                logger.info(f"[LangGraph-1.5] ë¶„ì„ ê³„íš: {analysis_plan.get('primary_focus', [])}")
            else:
                # í´ë°±
                state["analysis_plan"] = {
                    "primary_focus": focus_areas[:3],
                    "comparison_axes": ["ì‹œì¥ í¬ì§€ì…˜", "ì„±ì¥ì„±"],
                    "required_data_types": ["ë‰´ìŠ¤", "ì¬ë¬´"],
                    "expected_insights": ["ê²½ìŸ êµ¬ë„", "íˆ¬ì ì „ë§"],
                    "key_questions": [state['query']]
                }
                state["execution_log"].append("âš ï¸ ê¸°ë³¸ ë¶„ì„ ê³„íš ì ìš©")

        except Exception as e:
            logger.error(f"[LangGraph-1.5] ë¶„ì„ ì „ëµ ìˆ˜ë¦½ ì‹¤íŒ¨: {e}")
            # ìµœì†Œ ê³„íš
            state["analysis_plan"] = {
                "primary_focus": [state['query']],
                "key_questions": [state['query']]
            }
            state["execution_log"].append(f"âš ï¸ ë¶„ì„ ê³„íš ì‹¤íŒ¨, ìµœì†Œ ëª¨ë“œ: {e}")

        elapsed = time.time() - start_time
        logger.info(f"[LangGraph-1.5] ë¶„ì„ ì „ëµ ìˆ˜ë¦½ ì™„ë£Œ: {elapsed:.3f}ì´ˆ")
        return state

    async def _collect_parallel_data(self, state: LangGraphReportState) -> LangGraphReportState:
        """2ë‹¨ê³„: ë³‘ë ¬ ë°ì´í„° ìˆ˜ì§‘ (ì„±ëŠ¥ ìµœì í™”)"""
        import time
        start_time = time.time()

        state["execution_log"].append("ğŸš€ ë³‘ë ¬ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
        logger.info(f"[LangGraph-2] ë³‘ë ¬ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")

        try:
            # ë³‘ë ¬ë¡œ ì‹¤í–‰í•  ì‘ì—…ë“¤
            tasks = []

            # 1. êµ¬ì¡°í™”ëœ ë°ì´í„° ìˆ˜ì§‘ (Neo4j, Stock)
            tasks.append(self._collect_structured_data_async(state))

            # 2. ë¹„êµ¬ì¡°í™”ëœ ë°ì´í„° ìˆ˜ì§‘ (OpenSearch)
            tasks.append(self._collect_unstructured_data_async(state))

            # ë³‘ë ¬ ì‹¤í–‰
            results = await asyncio.gather(*tasks, return_exceptions=True)

            # ê²°ê³¼ í†µí•©
            structured_contexts = []
            unstructured_contexts = []

            for result in results:
                if isinstance(result, Exception):
                    logger.warning(f"ë³‘ë ¬ ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {result}")
                    continue

                if result.get("type") == "structured":
                    structured_contexts.extend(result.get("contexts", []))
                elif result.get("type") == "unstructured":
                    unstructured_contexts.extend(result.get("contexts", []))

            # ìƒíƒœì— í†µí•©
            state["contexts"].extend(structured_contexts)
            state["contexts"].extend(unstructured_contexts)

            state["execution_log"].append(f"âœ… ë³‘ë ¬ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: êµ¬ì¡°í™”({len(structured_contexts)}) + ë¹„êµ¬ì¡°í™”({len(unstructured_contexts)})")

        except Exception as e:
            logger.error(f"ë³‘ë ¬ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            state["execution_log"].append(f"âŒ ë³‘ë ¬ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

        elapsed = time.time() - start_time
        logger.info(f"[LangGraph-2] ë³‘ë ¬ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {elapsed:.3f}ì´ˆ, ì»¨í…ìŠ¤íŠ¸ {len(state['contexts'])}ê°œ")
        return state

    async def _apply_context_engineering(self, state: LangGraphReportState) -> LangGraphReportState:
        """2.5ë‹¨ê³„: Advanced Context Engineering - í”„ë¡œë•ì…˜ê¸‰ ì»¨í…ìŠ¤íŠ¸ ìµœì í™”

        Best Practices (AWS/Google/Towards Data Science 2025):
        1. Relevance Cascading - ë‹¨ê³„ì  í•„í„°ë§ (broad â†’ specific)
        2. Semantic Similarity - BGE-M3 ì„ë² ë”© ê¸°ë°˜ ì˜ë¯¸ í‰ê°€
        3. Diversity Optimization - ì¤‘ë³µ ì œê±° ë° ì •ë³´ ë‹¤ì–‘ì„± í™•ë³´
        4. Metadata Filtering - ì¶œì²˜/ì‹œê°„/ì‹ ë¢°ë„ ê¸°ë°˜ ìš°ì„ ìˆœìœ„
        5. Context Sequencing - ì •ë³´ ì „ë‹¬ ìˆœì„œ ìµœì í™”
        6. Reranking & Pruning - ìµœì¢… í’ˆì§ˆ ì„ ë³„
        """
        import time
        from datetime import datetime, timedelta
        start_time = time.time()

        state["execution_log"].append("ğŸ¯ Advanced Context Engineering ì‹œì‘")
        logger.info(f"[LangGraph-2.5] Context Engineering ì‹œì‘: {len(state['contexts'])}ê°œ ì»¨í…ìŠ¤íŠ¸")

        try:
            from api.services.semantic_similarity import get_semantic_filter

            # === Phase 1: Relevance Cascading (ë‹¨ê³„ì  í•„í„°ë§) ===
            contexts_as_dicts = self._prepare_contexts_for_engineering(state["contexts"])
            initial_count = len(contexts_as_dicts)

            # Step 1.1: Source-based filtering (broad)
            source_filtered = self._filter_by_source_priority(contexts_as_dicts)
            logger.info(f"[LangGraph-2.5] Source filtering: {len(contexts_as_dicts)} â†’ {len(source_filtered)}")

            # Step 1.2: Recency filtering
            recency_filtered = self._filter_by_recency(source_filtered, state.get("lookback_days", 180))
            logger.info(f"[LangGraph-2.5] Recency filtering: {len(source_filtered)} â†’ {len(recency_filtered)}")

            # Step 1.3: Confidence filtering
            confidence_filtered = self._filter_by_confidence(recency_filtered, threshold=0.3)
            logger.info(f"[LangGraph-2.5] Confidence filtering: {len(recency_filtered)} â†’ {len(confidence_filtered)}")

            state["execution_log"].append(f"ğŸ“Š Cascading: {initial_count} â†’ {len(confidence_filtered)}ê°œ")

            # === Phase 2: Semantic Similarity (ì˜ë¯¸ì  ìœ ì‚¬ë„) ===
            semantic_filter = get_semantic_filter()
            semantic_filtered = await semantic_filter.filter_by_similarity(
                query=state["query"],
                documents=confidence_filtered,
                top_k=50,
                diversity_mode=True,
                fast_mode=False
            )

            state["execution_log"].append(f"âœ… Semantic Filtering: {len(confidence_filtered)} â†’ {len(semantic_filtered)}ê°œ")
            logger.info(f"[LangGraph-2.5] Semantic filtering: {len(confidence_filtered)} â†’ {len(semantic_filtered)}")

            # === Phase 3: Diversity Optimization (ë‹¤ì–‘ì„± ìµœì í™”) ===
            diversity_score = semantic_filter.calculate_semantic_diversity(semantic_filtered)
            state["execution_log"].append(f"ğŸ“Š Diversity Score: {diversity_score:.2f}")
            logger.info(f"[LangGraph-2.5] Diversity score: {diversity_score:.2f}")

            # === Phase 4: Metadata-Enhanced Reranking (ë©”íƒ€ë°ì´í„° ê°•í™” ì¬ì •ë ¬) ===
            metadata_reranked = self._rerank_with_metadata(
                semantic_filtered,
                state["query"],
                state.get("analysis_plan", {})
            )

            # Semantic reranking ì¶”ê°€
            reranked_contexts = await semantic_filter.rerank_by_semantic_relevance(
                query=state["query"],
                documents=metadata_reranked,
                combine_with_original=True
            )

            logger.info(f"[LangGraph-2.5] Metadata+Semantic reranking ì™„ë£Œ")

            # === Phase 5: Context Sequencing (ì •ë³´ ì „ë‹¬ ìˆœì„œ ìµœì í™”) ===
            sequenced_contexts = self._sequence_contexts_for_reasoning(reranked_contexts, state["query"])
            state["execution_log"].append(f"ğŸ”„ Context Sequencing ì™„ë£Œ")
            logger.info(f"[LangGraph-2.5] Context sequencing: {len(sequenced_contexts)}ê°œ")

            # === Phase 6: Final Pruning (ìµœì¢… ì„ ë³„) ===
            final_contexts = sequenced_contexts[:30]  # Top 30

            # ContextItemìœ¼ë¡œ ë³€í™˜
            engineered_contexts = []
            for idx, ctx_dict in enumerate(final_contexts):
                context_item = ContextItem(
                    source=ctx_dict.get("source", "unknown"),
                    type=ctx_dict.get("type", "unknown"),
                    content=ctx_dict.get("metadata", {}),
                    confidence=ctx_dict.get("confidence", 0.5),
                    relevance=ctx_dict.get("combined_score", ctx_dict.get("semantic_score", 0.5)),
                    timestamp=ctx_dict.get("timestamp", "")
                )
                engineered_contexts.append(context_item)

            state["contexts"] = engineered_contexts
            state["execution_log"].append(
                f"âœ… Context Engineering ì™„ë£Œ: {initial_count} â†’ {len(engineered_contexts)}ê°œ "
                f"(ë‹¤ì–‘ì„±: {diversity_score:.2f})"
            )

        except Exception as e:
            logger.error(f"[LangGraph-2.5] Context Engineering ì‹¤íŒ¨: {e}")
            logger.error(traceback.format_exc())
            state["execution_log"].append(f"âš ï¸ Context Engineering ê±´ë„ˆë›°ê¸°: {str(e)[:100]}")
            # Fallback: ê¸°ë³¸ í•„í„°ë§ë§Œ ì ìš©
            state["contexts"] = state["contexts"][:30]

        elapsed = time.time() - start_time
        logger.info(f"[LangGraph-2.5] Context Engineering ì™„ë£Œ: {elapsed:.3f}ì´ˆ, ìµœì¢… {len(state['contexts'])}ê°œ")
        return state

    async def _collect_structured_data_async(self, state: LangGraphReportState) -> Dict[str, Any]:
        """êµ¬ì¡°í™”ëœ ë°ì´í„° ë¹„ë™ê¸° ìˆ˜ì§‘"""
        contexts = []

        try:
            # Neo4j ë°ì´í„° ìˆ˜ì§‘
            graph_context = await self.report_service.fetch_context(
                query=state["query"],
                lookback_days=state["lookback_days"],
                domain=state.get("domain"),
                graph_limit=100
            )

            # ê·¸ë˜í”„ ë°ì´í„°ë¥¼ ContextItemìœ¼ë¡œ ë³€í™˜
            for row in graph_context.graph_rows:
                context_item = ContextItem(
                    source="neo4j",
                    type=self._determine_graph_type(row),
                    content=row,
                    confidence=0.8,
                    relevance=self._calculate_graph_relevance(row, state["query"]),
                    timestamp=str(asyncio.get_event_loop().time())
                )
                contexts.append(context_item)

            # ì£¼ê°€ ë°ì´í„° ìˆ˜ì§‘ (ìƒì¥ì‚¬ì˜ ê²½ìš°)
            await self._collect_stock_data(state, contexts)

            return {"type": "structured", "contexts": contexts}

        except Exception as e:
            logger.error(f"êµ¬ì¡°í™”ëœ ë°ì´í„° ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            return {"type": "structured", "contexts": []}

    async def _collect_unstructured_data_async(self, state: LangGraphReportState) -> Dict[str, Any]:
        """ë¹„êµ¬ì¡°í™”ëœ ë°ì´í„° ë¹„ë™ê¸° ìˆ˜ì§‘"""
        contexts = []

        try:
            # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‚¬ìš© (Ollama ì„ë² ë”© + í‚¤ì›Œë“œ)
            news_hits = await self._langgraph_hybrid_search(
                query=state["query"],
                lookback_days=state["lookback_days"],
                size=50
            )

            # ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ContextItemìœ¼ë¡œ ë³€í™˜
            for hit in news_hits:
                source_data = hit.get("_source", {})
                context_item = ContextItem(
                    source="opensearch",
                    type="news",
                    content=source_data,
                    confidence=min(hit.get("_score", 0) / 10.0, 1.0),
                    relevance=self._calculate_news_relevance(source_data, state["query"]),
                    timestamp=str(asyncio.get_event_loop().time())
                )
                contexts.append(context_item)

            return {"type": "unstructured", "contexts": contexts}

        except Exception as e:
            logger.error(f"ë¹„êµ¬ì¡°í™”ëœ ë°ì´í„° ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            return {"type": "unstructured", "contexts": []}

    async def _collect_structured_data(self, state: LangGraphReportState) -> LangGraphReportState:
        """2ë‹¨ê³„: êµ¬ì¡°í™”ëœ ë°ì´í„° ìˆ˜ì§‘ (Neo4j, Stock)"""

        state["execution_log"].append("ğŸ“Š êµ¬ì¡°í™”ëœ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")

        try:
            # Neo4j ë°ì´í„° ìˆ˜ì§‘
            graph_context = await self.report_service.fetch_context(
                query=state["query"],
                lookback_days=state["lookback_days"],
                domain=state.get("domain"),
                graph_limit=100
            )

            # ê·¸ë˜í”„ ë°ì´í„°ë¥¼ ContextItemìœ¼ë¡œ ë³€í™˜
            for row in graph_context.graph_rows:
                context_item = ContextItem(
                    source="neo4j",
                    type=self._determine_graph_type(row),
                    content=row,
                    confidence=0.9,  # êµ¬ì¡°í™”ëœ ë°ì´í„°ëŠ” ë†’ì€ ì‹ ë¢°ë„
                    relevance=self._calculate_relevance(row, state["query"]),
                    timestamp=str(asyncio.get_event_loop().time())
                )
                state["contexts"].append(context_item)

            # ì£¼ê°€ ë°ì´í„° ìˆ˜ì§‘ (ì¢…ëª©ì´ ìˆëŠ” ê²½ìš°)
            if "symbol" in state and state["symbol"]:
                try:
                    stock_data = await self.stock.get_price(state["symbol"])
                    stock_context = ContextItem(
                        source="stock",
                        type="price_data",
                        content=stock_data,
                        confidence=0.95,
                        relevance=0.8,
                        timestamp=str(asyncio.get_event_loop().time())
                    )
                    state["contexts"].append(stock_context)
                except Exception as e:
                    logger.warning(f"ì£¼ê°€ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

            state["execution_log"].append(f"âœ… êµ¬ì¡°í™”ëœ ë°ì´í„° {len([c for c in state['contexts'] if c.source in ['neo4j', 'stock']])}ê°œ ìˆ˜ì§‘")

        except Exception as e:
            logger.error(f"êµ¬ì¡°í™”ëœ ë°ì´í„° ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            state["execution_log"].append(f"âŒ êµ¬ì¡°í™”ëœ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

        return state

    async def _collect_unstructured_data(self, state: LangGraphReportState) -> LangGraphReportState:
        """3ë‹¨ê³„: ë¹„êµ¬ì¡°í™”ëœ ë°ì´í„° ìˆ˜ì§‘ (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰)"""

        state["execution_log"].append("ğŸ“° í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê¸°ë°˜ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")

        try:
            # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‚¬ìš© (Ollama ì„ë² ë”© + í‚¤ì›Œë“œ)
            news_hits = await self._langgraph_hybrid_search(
                query=state["query"],
                lookback_days=state["lookback_days"],
                size=50
            )
            state["execution_log"].append(f"âœ… í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì™„ë£Œ: {len(news_hits)}ê±´")

            # ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ContextItemìœ¼ë¡œ ë³€í™˜
            for hit in news_hits:
                source_data = hit.get("_source", {})
                context_item = ContextItem(
                    source="opensearch",
                    type="news",
                    content=source_data,
                    confidence=min(hit.get("_score", 0) / 10.0, 1.0),
                    relevance=self._calculate_news_relevance(source_data, state["query"]),
                    timestamp=str(asyncio.get_event_loop().time())
                )
                state["contexts"].append(context_item)

            # ì‹¬í™” ë¶„ì„ì´ í•„ìš”í•œ ê²½ìš° ì¶”ê°€ ê²€ìƒ‰
            if state["analysis_depth"] in [AnalysisDepth.DEEP, AnalysisDepth.COMPREHENSIVE]:
                # í™•ì¥ëœ í‚¤ì›Œë“œë¡œ ì¶”ê°€ ê²€ìƒ‰
                extended_queries = await self._generate_extended_queries(state["query"])

                for ext_query in extended_queries:
                    try:
                        # í™•ì¥ ì¿¼ë¦¬ë„ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‚¬ìš©
                        ext_hits = await self._langgraph_hybrid_search(
                            query=ext_query,
                            lookback_days=state["lookback_days"],
                            size=20
                        )

                        for hit in ext_hits:
                            source_data = hit.get("_source", {})
                            context_item = ContextItem(
                                source="opensearch_extended",
                                type="news_extended",
                                content=source_data,
                                confidence=min(hit.get("_score", 0) / 15.0, 0.8),  # í™•ì¥ ê²€ìƒ‰ì€ ì‹ ë¢°ë„ ì•½ê°„ ë‚®ìŒ
                                relevance=self._calculate_news_relevance(source_data, ext_query),
                                timestamp=str(asyncio.get_event_loop().time())
                            )
                            state["contexts"].append(context_item)
                    except Exception as e:
                        logger.warning(f"í™•ì¥ ê²€ìƒ‰ ì‹¤íŒ¨ ({ext_query}): {e}")

            state["execution_log"].append(f"âœ… ë¹„êµ¬ì¡°í™”ëœ ë°ì´í„° {len([c for c in state['contexts'] if c.source.startswith('opensearch')])}ê°œ ìˆ˜ì§‘")

        except Exception as e:
            logger.error(f"ë¹„êµ¬ì¡°í™”ëœ ë°ì´í„° ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            state["execution_log"].append(f"âŒ ë¹„êµ¬ì¡°í™”ëœ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

        return state

    async def _cross_validate_contexts(self, state: LangGraphReportState) -> LangGraphReportState:
        """4ë‹¨ê³„: ì»¨í…ìŠ¤íŠ¸ ê°„ êµì°¨ ê²€ì¦ ë° í•„í„°ë§"""

        state["execution_log"].append("ğŸ”— ì»¨í…ìŠ¤íŠ¸ êµì°¨ ê²€ì¦ ì‹œì‘")

        # ì¤‘ë³µ ì œê±°
        unique_contexts = []
        seen_contents = set()

        for context in state["contexts"]:
            # ì»¨í…ì¸  í•´ì‹œ ìƒì„± (ë‹¨ìˆœí™”)
            content_hash = hash(str(context.content))

            if content_hash not in seen_contents:
                seen_contents.add(content_hash)
                unique_contexts.append(context)

        # ì‹ ë¢°ë„ ë° ê´€ë ¨ì„± ê¸°ë°˜ í•„í„°ë§
        high_quality_contexts = [
            ctx for ctx in unique_contexts
            if ctx.confidence > 0.3 and ctx.relevance > 0.2
        ]

        # ìƒìœ„ ì»¨í…ìŠ¤íŠ¸ë§Œ ìœ ì§€ (ë¶„ì„ ê¹Šì´ì— ë”°ë¼)
        max_contexts = {
            AnalysisDepth.SHALLOW: 20,
            AnalysisDepth.STANDARD: 40,
            AnalysisDepth.DEEP: 80,
            AnalysisDepth.COMPREHENSIVE: 150
        }

        # í’ˆì§ˆ ì ìˆ˜ë¡œ ì •ë ¬ í›„ ìƒìœ„ Nê°œ ì„ íƒ
        sorted_contexts = sorted(
            high_quality_contexts,
            key=lambda x: x.confidence * x.relevance,
            reverse=True
        )

        state["contexts"] = sorted_contexts[:max_contexts[state["analysis_depth"]]]
        state["execution_log"].append(f"âœ… {len(state['contexts'])}ê°œ ê³ í’ˆì§ˆ ì»¨í…ìŠ¤íŠ¸ ì„ ë³„")

        return state

    async def _generate_insights(self, state: LangGraphReportState) -> LangGraphReportState:
        """4ë‹¨ê³„: ì¸ì‚¬ì´íŠ¸ ìƒì„± (ë³µì› - ê³ í’ˆì§ˆ ìš°ì„ )

        ëª©ì : ìˆ˜ì§‘ëœ ë°ì´í„°ì—ì„œ ì˜ë¯¸ ìˆëŠ” ë°œê²¬ì‚¬í•­ ë„ì¶œ
        """
        import time
        start_time = time.time()

        state["execution_log"].append("ğŸ’¡ ì¸ì‚¬ì´íŠ¸ ìƒì„± ì‹œì‘")
        logger.info(f"[LangGraph-4] ì¸ì‚¬ì´íŠ¸ ìƒì„± ì‹œì‘")

        try:
            contexts_summary = self._prepare_comprehensive_context_summary(state["contexts"])
            analysis_plan = state.get("analysis_plan", {})
            key_questions = analysis_plan.get("key_questions", [state['query']])

            # ì¸ì‚¬ì´íŠ¸ ìƒì„± í”„ë¡¬í”„íŠ¸ (ê³ í’ˆì§ˆ ìš°ì„ )
            insights_prompt = f"""ê¸ˆìœµ ì• ë„ë¦¬ìŠ¤íŠ¸ë¡œì„œ ë‹¤ìŒ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•˜ì„¸ìš”.

**ì§ˆì˜**: {state['query']}
**ë¶„ì„ ëª©í‘œ**: {analysis_plan.get('primary_focus', [])}
**í•µì‹¬ ì§ˆë¬¸**: {key_questions}

**ë°ì´í„°**:
{contexts_summary}

ë‹¤ìŒ JSON ë°°ì—´ í˜•ì‹ìœ¼ë¡œ ì¸ì‚¬ì´íŠ¸ë¥¼ ìƒì„±í•˜ì„¸ìš” (3-5ê°œ):
[
  {{
    "title": "ì¸ì‚¬ì´íŠ¸ ì œëª©",
    "type": "quantitative|qualitative|temporal|comparative",
    "finding": "ë°œê²¬ì‚¬í•­ ì„¤ëª… (êµ¬ì²´ì  ìˆ˜ì¹˜ í¬í•¨)",
    "evidence": ["ê·¼ê±°1", "ê·¼ê±°2"],
    "significance": "íˆ¬ìì ê´€ì ì—ì„œì˜ ì˜ë¯¸",
    "confidence": 0.0-1.0
  }}
]

JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”:
"""

            response = await self._llm_invoke(insights_prompt)

            # JSON íŒŒì‹±
            import json, re
            json_match = re.search(r'\[[\s\S]*\]', response)
            if json_match:
                insights = json.loads(json_match.group(0))
                state["insights"] = insights
                state["execution_log"].append(f"âœ… {len(insights)}ê°œ ì¸ì‚¬ì´íŠ¸ ìƒì„±")
                logger.info(f"[LangGraph-4] {len(insights)}ê°œ ì¸ì‚¬ì´íŠ¸ ìƒì„± ì™„ë£Œ")
            else:
                # í´ë°±: í…ìŠ¤íŠ¸ì—ì„œ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ
                state["insights"] = [{
                    "title": "ê¸°ë³¸ ë¶„ì„",
                    "type": "comprehensive",
                    "finding": response[:500] if response else "ë°ì´í„° ë¶€ì¡±",
                    "evidence": [f"{len(state['contexts'])}ê°œ ë°ì´í„° ì†ŒìŠ¤"],
                    "significance": "ì¢…í•© ë¶„ì„ ê²°ê³¼",
                    "confidence": 0.7
                }]
                state["execution_log"].append("âš ï¸ í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ")

        except Exception as e:
            logger.error(f"[LangGraph-4] ì¸ì‚¬ì´íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            state["insights"] = []
            state["execution_log"].append(f"âŒ ì¸ì‚¬ì´íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")

        elapsed = time.time() - start_time
        logger.info(f"[LangGraph-4] ì¸ì‚¬ì´íŠ¸ ìƒì„± ì™„ë£Œ: {elapsed:.3f}ì´ˆ")
        return state

    async def _analyze_relationships(self, state: LangGraphReportState) -> LangGraphReportState:
        """5ë‹¨ê³„: ê´€ê³„ ë¶„ì„ (ë³µì› - ê³ í’ˆì§ˆ ìš°ì„ )

        ëª©ì : ì—”í‹°í‹° ê°„ ì—°ê²°ì„± ë° ì˜í–¥ ê´€ê³„ íŒŒì•…
        """
        import time
        start_time = time.time()

        state["execution_log"].append("ğŸ”— ê´€ê³„ ë¶„ì„ ì‹œì‘")
        logger.info(f"[LangGraph-5] ê´€ê³„ ë¶„ì„ ì‹œì‘")

        try:
            query_analysis = state.get("query_analysis", {})
            entities = query_analysis.get("entities", {})
            insights = state.get("insights", [])

            # ê´€ê³„ ë¶„ì„ í”„ë¡¬í”„íŠ¸
            relationships_prompt = f"""ê¸ˆìœµ ì• ë„ë¦¬ìŠ¤íŠ¸ë¡œì„œ ë‹¤ìŒ ì—”í‹°í‹°ë“¤ ê°„ì˜ ê´€ê³„ë¥¼ ë¶„ì„í•˜ì„¸ìš”.

**ì§ˆì˜**: {state['query']}
**ì—”í‹°í‹°**: {entities}
**ë„ì¶œëœ ì¸ì‚¬ì´íŠ¸**: {[ins.get('title') for ins in insights[:3]]}

ë‹¤ìŒ ê´€ê³„ë“¤ì„ ë¶„ì„í•˜ì„¸ìš”:
1. **ê²½ìŸ ê´€ê³„**: ì‹œì¥ ë‚´ ê²½ìŸ êµ¬ë„ ë° ìƒëŒ€ì  í¬ì§€ì…˜
2. **ê³µê¸‰ë§ ê´€ê³„**: ìƒí•˜ë¥˜ ì˜ì¡´ì„± ë° íŒŒíŠ¸ë„ˆì‹­
3. **ì´ë²¤íŠ¸ ì˜í–¥**: ì£¼ìš” ì´ë²¤íŠ¸ê°€ ì—”í‹°í‹°ì— ë¯¸ì¹˜ëŠ” ì˜í–¥
4. **ì‹œì¥ ì—­í•™**: ì‹œì¥ íŠ¸ë Œë“œì™€ ê¸°ì—… ì „ëµì˜ ê´€ê³„

JSON ë°°ì—´ë¡œ ì¶œë ¥í•˜ì„¸ìš”:
[
  {{
    "type": "competition|supply_chain|event_impact|market_dynamics",
    "entities": ["ì—”í‹°í‹°1", "ì—”í‹°í‹°2"],
    "relationship": "ê´€ê³„ ì„¤ëª…",
    "strength": "strong|moderate|weak",
    "impact": "ê¸ì •ì /ë¶€ì •ì  ì˜í–¥ ì„¤ëª…",
    "confidence": 0.0-1.0
  }}
]

JSONë§Œ ì¶œë ¥:
"""

            response = await self._llm_invoke(relationships_prompt)

            # JSON íŒŒì‹±
            import json, re
            json_match = re.search(r'\[[\s\S]*\]', response)
            if json_match:
                relationships = json.loads(json_match.group(0))
                state["relationships"] = relationships
                state["execution_log"].append(f"âœ… {len(relationships)}ê°œ ê´€ê³„ ë¶„ì„ ì™„ë£Œ")
                logger.info(f"[LangGraph-5] {len(relationships)}ê°œ ê´€ê³„ ë¶„ì„ ì™„ë£Œ")
            else:
                # í´ë°±
                state["relationships"] = [{
                    "type": "comprehensive",
                    "entities": list(entities.get('companies', []))[:2],
                    "relationship": response[:300] if response else "ê´€ê³„ ë¶„ì„ ë°ì´í„° ë¶€ì¡±",
                    "strength": "moderate",
                    "impact": "ë¶„ì„ í•„ìš”",
                    "confidence": 0.6
                }]
                state["execution_log"].append("âš ï¸ ê¸°ë³¸ ê´€ê³„ ë¶„ì„ ì ìš©")

        except Exception as e:
            logger.error(f"[LangGraph-5] ê´€ê³„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            state["relationships"] = []
            state["execution_log"].append(f"âŒ ê´€ê³„ ë¶„ì„ ì‹¤íŒ¨: {e}")

        elapsed = time.time() - start_time
        logger.info(f"[LangGraph-5] ê´€ê³„ ë¶„ì„ ì™„ë£Œ: {elapsed:.3f}ì´ˆ")
        return state

    async def _deep_reasoning(self, state: LangGraphReportState) -> LangGraphReportState:
        """6ë‹¨ê³„: ì‹¬í™” ì¶”ë¡  (NEW - ê³ í’ˆì§ˆ ìš°ì„ )

        ëª©ì : Why, How, What-if ë¶„ì„ì„ í†µí•œ ê¹Šì´ ìˆëŠ” í†µì°°
        """
        import time
        start_time = time.time()

        state["execution_log"].append("ğŸ§  ì‹¬í™” ì¶”ë¡  ì‹œì‘")
        logger.info(f"[LangGraph-6] ì‹¬í™” ì¶”ë¡  ì‹œì‘")

        try:
            insights = state.get("insights", [])
            relationships = state.get("relationships", [])
            analysis_plan = state.get("analysis_plan", {})

            # ì‹¬í™” ì¶”ë¡  í”„ë¡¬í”„íŠ¸
            reasoning_prompt = f"""ê¸ˆìœµ ì „ë¬¸ê°€ë¡œì„œ ë‹¤ìŒ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹¬ì¸µ ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ì„¸ìš”.

**ì§ˆì˜**: {state['query']}
**ì¸ì‚¬ì´íŠ¸**: {[ins.get('title') for ins in insights]}
**ê´€ê³„**: {[rel.get('type') for rel in relationships]}

ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”:

1. **Why (ì›ì¸)**: ì™œ ì´ëŸ¬í•œ í˜„ìƒì´ ë°œìƒí–ˆëŠ”ê°€?
2. **How (ë©”ì»¤ë‹ˆì¦˜)**: ì–´ë–¤ ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ ì‘ë™í•˜ëŠ”ê°€?
3. **What-if (ì‹œë‚˜ë¦¬ì˜¤)**: í–¥í›„ ì˜ˆìƒ ì‹œë‚˜ë¦¬ì˜¤ëŠ”?
4. **So What (ì˜ë¯¸)**: íˆ¬ììì—ê²Œ ì£¼ëŠ” ì‹¤ì§ˆì  ì˜ë¯¸ëŠ”?

JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”:
{{
  "why": {{
    "causes": ["ì›ì¸1", "ì›ì¸2"],
    "analysis": "ì›ì¸ ë¶„ì„"
  }},
  "how": {{
    "mechanisms": ["ë©”ì»¤ë‹ˆì¦˜1", "ë©”ì»¤ë‹ˆì¦˜2"],
    "analysis": "ë©”ì»¤ë‹ˆì¦˜ ì„¤ëª…"
  }},
  "what_if": {{
    "scenarios": [
      {{"scenario": "ì‹œë‚˜ë¦¬ì˜¤ ëª…", "probability": "high|medium|low", "impact": "ì„¤ëª…"}}
    ]
  }},
  "so_what": {{
    "investor_implications": "íˆ¬ì ì˜ë¯¸",
    "actionable_insights": ["ì‹¤í–‰ ê°€ëŠ¥í•œ ì¸ì‚¬ì´íŠ¸"]
  }}
}}

JSONë§Œ ì¶œë ¥:
"""

            response = await self._llm_invoke(reasoning_prompt)

            # JSON íŒŒì‹± (ê°•í™”ëœ ë¡œì§)
            import json, re

            # 1ì°¨ ì‹œë„: ê°€ì¥ í° JSON ê°ì²´ ì¶”ì¶œ
            json_pattern = r'\{(?:[^{}]|(?:\{(?:[^{}]|(?:\{[^{}]*\}))*\}))*\}'
            json_matches = re.findall(json_pattern, response, re.DOTALL)

            deep_reasoning = None
            parse_error = None

            # ëª¨ë“  ë§¤ì¹˜ëœ JSONì— ëŒ€í•´ íŒŒì‹± ì‹œë„ (í° ê²ƒë¶€í„°)
            for json_str in sorted(json_matches, key=len, reverse=True):
                try:
                    parsed = json.loads(json_str)
                    # í•„ìˆ˜ í‚¤ ê²€ì¦
                    if isinstance(parsed, dict) and any(k in parsed for k in ["why", "how", "what_if", "so_what"]):
                        deep_reasoning = parsed
                        logger.info(f"[LangGraph-6] JSON íŒŒì‹± ì„±ê³µ ({len(json_str)}ì)")
                        break
                except json.JSONDecodeError as je:
                    parse_error = str(je)
                    continue

            if deep_reasoning:
                state["deep_reasoning"] = deep_reasoning
                state["execution_log"].append("âœ… ì‹¬í™” ì¶”ë¡  ì™„ë£Œ (Why/How/What-if/So-what)")
                logger.info(f"[LangGraph-6] ì‹¬í™” ì¶”ë¡  ì™„ë£Œ")
            else:
                # í´ë°±: êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ íŒŒì‹± ì‹œë„
                logger.warning(f"[LangGraph-6] JSON íŒŒì‹± ì‹¤íŒ¨, í´ë°± ëª¨ë“œ: {parse_error}")
                state["deep_reasoning"] = {
                    "why": {"causes": ["LLM ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨"], "analysis": response[:300] if response else ""},
                    "how": {"mechanisms": [], "analysis": ""},
                    "what_if": {"scenarios": []},
                    "so_what": {"investor_implications": "ì¶”ê°€ ë¶„ì„ í•„ìš”", "actionable_insights": []}
                }
                state["execution_log"].append(f"âš ï¸ ê¸°ë³¸ ì¶”ë¡  ëª¨ë“œ (íŒŒì‹± ì˜¤ë¥˜: {parse_error})")

        except Exception as e:
            logger.error(f"[LangGraph-6] ì‹¬í™” ì¶”ë¡  ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(traceback.format_exc())
            state["deep_reasoning"] = {
                "why": {"causes": ["ì‹¬í™” ì¶”ë¡  ì˜¤ë¥˜"], "analysis": str(e)[:200]},
                "so_what": {"investor_implications": "ì˜¤ë¥˜ë¡œ ì¸í•œ ì¶”ë¡  ë¶ˆê°€", "actionable_insights": []}
            }
            state["execution_log"].append(f"âŒ ì‹¬í™” ì¶”ë¡  ì‹¤íŒ¨: {str(e)[:100]}")

        elapsed = time.time() - start_time
        logger.info(f"[LangGraph-6] ì‹¬í™” ì¶”ë¡  ì™„ë£Œ: {elapsed:.3f}ì´ˆ")
        return state

    async def _synthesize_report(self, state: LangGraphReportState) -> LangGraphReportState:
        """7ë‹¨ê³„: ë³´ê³ ì„œ í•©ì„± (ë³µì› - ê³ í’ˆì§ˆ ìš°ì„ )

        ëª©ì : ëª¨ë“  ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ì™„ê²°ëœ ë³´ê³ ì„œ ì‘ì„±
        """
        import time
        start_time = time.time()

        state["execution_log"].append("ğŸ“ ë³´ê³ ì„œ í•©ì„± ì‹œì‘")
        logger.info(f"[LangGraph-7] ë³´ê³ ì„œ í•©ì„± ì‹œì‘")

        try:
            insights = state.get("insights", [])
            relationships = state.get("relationships", [])
            deep_reasoning = state.get("deep_reasoning", {})
            analysis_plan = state.get("analysis_plan", {})

            # ì¸ì‚¬ì´íŠ¸ ìš”ì•½
            insights_summary = "\n".join([
                f"- **{ins.get('title')}**: {ins.get('finding', '')[:150]}"
                for ins in insights[:5]
            ])

            # ê´€ê³„ ìš”ì•½
            relationships_summary = "\n".join([
                f"- {rel.get('type')}: {rel.get('relationship', '')[:100]}"
                for rel in relationships[:3]
            ])

            # ì¶”ë¡  ìš”ì•½
            reasoning_summary = ""
            if deep_reasoning:
                causes = deep_reasoning.get('why', {}).get('causes', [])
                implications = deep_reasoning.get('so_what', {}).get('investor_implications', '')
                reasoning_summary = f"\n\n**ì›ì¸**: {', '.join(causes[:3])}\n**íˆ¬ì ì˜ë¯¸**: {implications[:200]}"

            # ë³´ê³ ì„œ í•©ì„± í”„ë¡¬í”„íŠ¸
            synthesis_prompt = f"""ê¸ˆìœµ ì• ë„ë¦¬ìŠ¤íŠ¸ë¡œì„œ ë‹¤ìŒ ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ì™„ê²°ëœ íˆ¬ì ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.

**ì§ˆì˜**: {state['query']}

**ë¶„ì„ ê²°ê³¼**:

### ì¸ì‚¬ì´íŠ¸
{insights_summary}

### ê´€ê³„ ë¶„ì„
{relationships_summary}

### ì‹¬í™” ì¶”ë¡ 
{reasoning_summary}

ë‹¤ìŒ êµ¬ì¡°ë¡œ Markdown ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”:

# Executive Summary
- í•µì‹¬ ë°œê²¬ì‚¬í•­ 3-4ê°œ (ë°ì´í„° ê¸°ë°˜)

# Market Analysis
- ì‹œì¥ ìƒí™© ë° ì£¼ìš” ë™í–¥
- ê²½ìŸ êµ¬ë„

# Key Insights
ê° ì¸ì‚¬ì´íŠ¸ë³„ë¡œ:
- ì œëª© ë° ë°œê²¬ì‚¬í•­
- ê·¼ê±° ë°ì´í„°
- íˆ¬ì ê´€ì  ì˜ë¯¸

# Relationship & Competitive Analysis
- ì—”í‹°í‹° ê°„ ê´€ê³„
- ì‹œì¥ í¬ì§€ì…˜
- ê³µê¸‰ë§ ì—­í•™

# Deep Reasoning
- í˜„ìƒì˜ ì›ì¸ (Why)
- ì‘ë™ ë©”ì»¤ë‹ˆì¦˜ (How)
- ì˜ˆìƒ ì‹œë‚˜ë¦¬ì˜¤ (What-if)

# Investment Perspective
- ë‹¨ê¸°/ì¤‘ê¸° ì „ë§
- ì´‰ë§¤ ë° ë¦¬ìŠ¤í¬
- êµ¬ì²´ì  ê¶Œì¥ì‚¬í•­

**ì‘ì„± ì›ì¹™**:
- ëª¨ë“  ì£¼ì¥ì— ë°ì´í„° ê·¼ê±° ëª…ì‹œ
- êµ¬ì²´ì  ìˆ˜ì¹˜ í¬í•¨
- ì‹¤í–‰ ê°€ëŠ¥í•œ ê¶Œê³ 
- ì „ë¬¸ì ì´ê³  ê°„ê²°í•œ ë¬¸ì²´

ë°”ë¡œ ì‹œì‘:
"""

            response = await self._llm_invoke(synthesis_prompt)
            state["final_report"] = response

            state["report_sections"] = {
                "executive_summary": "ì™„ë£Œ",
                "insights": "ì™„ë£Œ",
                "relationships": "ì™„ë£Œ",
                "reasoning": "ì™„ë£Œ",
                "investment": "ì™„ë£Œ"
            }

            state["execution_log"].append("âœ… ë³´ê³ ì„œ í•©ì„± ì™„ë£Œ (ê³ í’ˆì§ˆ ë²„ì „)")
            logger.info(f"[LangGraph-7] ë³´ê³ ì„œ í•©ì„± ì™„ë£Œ")

        except Exception as e:
            logger.error(f"[LangGraph-7] ë³´ê³ ì„œ í•©ì„± ì‹¤íŒ¨: {e}")
            # í´ë°±: ê¸°ë³¸ ë³´ê³ ì„œ
            state["final_report"] = f"""# {state['query']} ë¶„ì„ ë³´ê³ ì„œ

## Executive Summary
{len(state.get('insights', []))}ê°œ ì¸ì‚¬ì´íŠ¸, {len(state.get('relationships', []))}ê°œ ê´€ê³„ ë¶„ì„ ì™„ë£Œ

## Key Insights
{insights_summary if insights_summary else 'ë°ì´í„° ë¶„ì„ ì¤‘...'}

## Analysis
ìƒì„¸ ë¶„ì„ì€ ê°œë³„ ì„¹ì…˜ì„ ì°¸ì¡°í•˜ì„¸ìš”.

**ì˜¤ë¥˜**: {str(e)}
"""
            state["execution_log"].append(f"âŒ ë³´ê³ ì„œ í•©ì„± ì‹¤íŒ¨, ê¸°ë³¸ í…œí”Œë¦¿ ì‚¬ìš©: {e}")

        elapsed = time.time() - start_time
        logger.info(f"[LangGraph-7] ë³´ê³ ì„œ í•©ì„± ì™„ë£Œ: {elapsed:.3f}ì´ˆ")
        return state

    def _prepare_comprehensive_context_summary(self, contexts: List[ContextItem]) -> str:
        """í†µí•© ë¶„ì„ì„ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ ìš”ì•½ ìƒì„± (ìµœì í™”: ê°„ê²°í•˜ê²Œ)"""

        if not contexts:
            return "ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

        # ìœ í˜•ë³„ë¡œ ê·¸ë£¹í•‘
        context_groups = {}
        for ctx in contexts:
            if ctx.type not in context_groups:
                context_groups[ctx.type] = []
            context_groups[ctx.type].append(ctx)

        summary_parts = [f"**ì´ ë°ì´í„°**: {len(contexts)}ê°œ\n"]

        # ê° ìœ í˜•ë³„ë¡œ ìƒìœ„ 2ê°œë§Œ í¬í•¨ (ê°„ê²°í™”)
        for ctx_type, ctx_list in context_groups.items():
            summary_parts.append(f"\n### {ctx_type.upper()} ({len(ctx_list)}ê°œ)")

            # ì‹ ë¢°ë„ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ 2ê°œë§Œ
            top_contexts = sorted(ctx_list, key=lambda x: x.confidence * x.relevance, reverse=True)[:2]

            for i, ctx in enumerate(top_contexts, 1):
                # í•µì‹¬ ì •ë³´ë§Œ ì¶”ì¶œ (title, summary ë“±)
                # ì•ˆì „í•œ ì ‘ê·¼: dictì™€ dataclass ëª¨ë‘ ì§€ì›
                try:
                    if isinstance(ctx, dict):
                        content = ctx.get('content', {})
                    else:
                        content = ctx.content

                    if isinstance(content, dict):
                        title = content.get("title", content.get("name", ""))
                        summary = content.get("summary", content.get("content", ""))[:200]  # 200ìë¡œ ì œí•œ
                        summary_parts.append(f"[{i}] {title[:100]} - {summary}")
                    else:
                        # JSONì´ ì•„ë‹Œ ê²½ìš° ê°„ë‹¨íˆ ì²˜ë¦¬
                        content_str = str(content)[:150]
                        summary_parts.append(f"[{i}] {content_str}")
                except Exception as e:
                    # ì˜¤ë¥˜ ë°œìƒì‹œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
                    summary_parts.append(f"[{i}] ë°ì´í„° íŒŒì‹± ì˜¤ë¥˜: {str(e)[:50]}")

        return "\n".join(summary_parts)

    async def _generate_insights_LEGACY_UNUSED(self, state: LangGraphReportState) -> LangGraphReportState:
        """ë ˆê±°ì‹œ: ì‚¬ìš© ì•ˆ í•¨ - ìƒˆë¡œìš´ ê³ í’ˆì§ˆ ë²„ì „ìœ¼ë¡œ ëŒ€ì²´ë¨ (625í–‰)"""
        import time
        start_time = time.time()

        state["execution_log"].append("ğŸ’¡ ì¸ì‚¬ì´íŠ¸ ìƒì„± ì‹œì‘")
        logger.info(f"[LangGraph-5] ì¸ì‚¬ì´íŠ¸ ìƒì„± ì‹œì‘")

        # ì»¨í…ìŠ¤íŠ¸ë¥¼ ìœ í˜•ë³„ë¡œ ê·¸ë£¹í•‘
        context_groups = {}
        for ctx in state["contexts"]:
            if ctx.type not in context_groups:
                context_groups[ctx.type] = []
            context_groups[ctx.type].append(ctx)

        insights = []

        for ctx_type, contexts in context_groups.items():
            if len(contexts) < 2:  # ìµœì†Œ 2ê°œ ì´ìƒ ìˆì–´ì•¼ ì¸ì‚¬ì´íŠ¸ ìƒì„±
                continue

            # ìƒì¥ì‚¬ íŠ¹í™” ì¸ì‚¬ì´íŠ¸ ìƒì„±
            data_summary = self._summarize_context_data(contexts[:3])  # ìƒìœ„ 3ê°œë§Œ
            insight_prompt = self._generate_sector_specific_insight_prompt(ctx_type, state['query'], data_summary)

            try:
                response = await self._llm_invoke(insight_prompt)

                insight = {
                    "type": ctx_type,
                    "content": response,
                    "source_count": len(contexts),
                    "confidence": sum(ctx.confidence for ctx in contexts) / len(contexts),
                    "timestamp": str(asyncio.get_event_loop().time())
                }
                insights.append(insight)

            except Exception as e:
                logger.warning(f"ì¸ì‚¬ì´íŠ¸ ìƒì„± ì‹¤íŒ¨ ({ctx_type}): {e}")

        state["insights"] = insights
        state["execution_log"].append(f"âœ… {len(insights)}ê°œ ì¸ì‚¬ì´íŠ¸ ìƒì„±")

        elapsed = time.time() - start_time
        logger.info(f"[LangGraph-5] ì¸ì‚¬ì´íŠ¸ ìƒì„± ì™„ë£Œ: {elapsed:.3f}ì´ˆ, {len(insights)}ê°œ")
        return state

    async def _analyze_relationships_LEGACY_UNUSED(self, state: LangGraphReportState) -> LangGraphReportState:
        """ë ˆê±°ì‹œ: ì‚¬ìš© ì•ˆ í•¨ - ìƒˆë¡œìš´ ê³ í’ˆì§ˆ ë²„ì „ìœ¼ë¡œ ëŒ€ì²´ë¨ (697í–‰)"""

        state["execution_log"].append("ğŸ”— ê´€ê³„ ë¶„ì„ ì‹œì‘")

        relationships = []

        # ì»¨í…ìŠ¤íŠ¸ ìœ í˜•ë³„ ë¶„ë¥˜ (ëª¨ë“  ìƒì¥ì‚¬ ëŒ€ì‘)
        news_contexts = [ctx for ctx in state["contexts"] if ctx.type.startswith("news")]
        company_contexts = [ctx for ctx in state["contexts"] if ctx.type == "company"]
        financial_contexts = [ctx for ctx in state["contexts"] if ctx.type in ["financial", "investment", "stock"]]
        business_contexts = [ctx for ctx in state["contexts"] if ctx.type in ["contract", "deal", "announcement"]]

        # 1. ë‰´ìŠ¤-ê¸°ì—… ê´€ê³„ ë¶„ì„
        if news_contexts and company_contexts:
            await self._analyze_news_company_relationship(state, news_contexts, company_contexts, relationships)

        # 2. ì¬ë¬´-ë‰´ìŠ¤ ê´€ê³„ ë¶„ì„
        if financial_contexts and news_contexts:
            await self._analyze_financial_news_relationship(state, financial_contexts, news_contexts, relationships)

        # 3. ë¹„ì¦ˆë‹ˆìŠ¤ ì´ë²¤íŠ¸-ë‰´ìŠ¤ ê´€ê³„ ë¶„ì„
        if business_contexts and news_contexts:
            await self._analyze_business_news_relationship(state, business_contexts, news_contexts, relationships)

        state["relationships"] = relationships
        state["execution_log"].append(f"âœ… {len(relationships)}ê°œ ê´€ê³„ ë¶„ì„ ì™„ë£Œ")

        return state

    async def _analyze_news_company_relationship(self, state, news_contexts, company_contexts, relationships):
        """ë‰´ìŠ¤-ê¸°ì—… ê´€ê³„ ë¶„ì„"""
        try:
            relationship_prompt = f"""
            ë‹¤ìŒ ë‰´ìŠ¤ì™€ ê¸°ì—… ì •ë³´ ê°„ì˜ ê´€ê³„ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:

            ë‰´ìŠ¤ ë°ì´í„° (ìµœì‹  5ê°œ):
            {json.dumps([ctx.content for ctx in news_contexts[:5]], ensure_ascii=False, indent=2)}

            ê¸°ì—… ë°ì´í„°:
            {json.dumps([ctx.content for ctx in company_contexts[:3]], ensure_ascii=False, indent=2)}

            ë‹¤ìŒì„ ë¶„ì„í•´ì£¼ì„¸ìš”:
            1. ë‰´ìŠ¤ê°€ ê¸°ì—…ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ (ê¸ì •/ë¶€ì •/ì¤‘ë¦½)
            2. ê¸°ì—… ê°€ì¹˜ ë° ì£¼ê°€ì— ëŒ€í•œ ì‹œì‚¬ì 
            3. ì‚°ì—… ë‚´ ê²½ìŸë ¥ ë³€í™”
            4. í–¥í›„ ì˜ˆìƒë˜ëŠ” ê¸°ì—… ì „ëµ ë³€í™”

            ìƒì¥ì‚¬ íˆ¬ì ê´€ì ì—ì„œ êµ¬ì²´ì ì¸ ë¶„ì„ì„ í•´ì£¼ì„¸ìš”.
            """

            response = await self._llm_invoke(relationship_prompt)

            relationship = {
                "type": "news_company_correlation",
                "analysis": response,
                "confidence": 0.8,
                "timestamp": str(asyncio.get_event_loop().time())
            }
            relationships.append(relationship)

        except Exception as e:
            logger.warning(f"ë‰´ìŠ¤-ê¸°ì—… ê´€ê³„ ë¶„ì„ ì‹¤íŒ¨: {e}")

    async def _analyze_financial_news_relationship(self, state, financial_contexts, news_contexts, relationships):
        """ì¬ë¬´-ë‰´ìŠ¤ ê´€ê³„ ë¶„ì„"""
        try:
            relationship_prompt = f"""
            ë‹¤ìŒ ì¬ë¬´ ì •ë³´ì™€ ë‰´ìŠ¤ ê°„ì˜ ê´€ê³„ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:

            ì¬ë¬´ ë°ì´í„°:
            {json.dumps([ctx.content for ctx in financial_contexts[:3]], ensure_ascii=False, indent=2)}

            ê´€ë ¨ ë‰´ìŠ¤ (ìµœì‹  3ê°œ):
            {json.dumps([ctx.content for ctx in news_contexts[:3]], ensure_ascii=False, indent=2)}

            ë‹¤ìŒì„ ë¶„ì„í•´ì£¼ì„¸ìš”:
            1. ì¬ë¬´ ì„±ê³¼ì™€ ë‰´ìŠ¤ ì´ë²¤íŠ¸ ê°„ì˜ ì¸ê³¼ê´€ê³„
            2. ì¬ë¬´ ì§€í‘œ ë³€í™”ê°€ ì‹œì¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥
            3. íˆ¬ìì ê´€ì ì—ì„œì˜ ë¦¬ìŠ¤í¬/ê¸°íšŒ ìš”ì†Œ
            4. ì¬ë¬´ ê±´ì „ì„±ì— ëŒ€í•œ ì¢…í•© í‰ê°€

            íˆ¬ì ì˜ì‚¬ê²°ì •ì— ë„ì›€ì´ ë˜ëŠ” ë¶„ì„ì„ í•´ì£¼ì„¸ìš”.
            """

            response = await self._llm_invoke(relationship_prompt)

            relationship = {
                "type": "financial_news_correlation",
                "analysis": response,
                "confidence": 0.7,
                "timestamp": str(asyncio.get_event_loop().time())
            }
            relationships.append(relationship)

        except Exception as e:
            logger.warning(f"ì¬ë¬´-ë‰´ìŠ¤ ê´€ê³„ ë¶„ì„ ì‹¤íŒ¨: {e}")

    async def _analyze_business_news_relationship(self, state, business_contexts, news_contexts, relationships):
        """ë¹„ì¦ˆë‹ˆìŠ¤ ì´ë²¤íŠ¸-ë‰´ìŠ¤ ê´€ê³„ ë¶„ì„"""
        try:
            relationship_prompt = f"""
            ë‹¤ìŒ ë¹„ì¦ˆë‹ˆìŠ¤ ì´ë²¤íŠ¸ì™€ ë‰´ìŠ¤ ê°„ì˜ ê´€ê³„ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:

            ë¹„ì¦ˆë‹ˆìŠ¤ ì´ë²¤íŠ¸:
            {json.dumps([ctx.content for ctx in business_contexts[:3]], ensure_ascii=False, indent=2)}

            ê´€ë ¨ ë‰´ìŠ¤:
            {json.dumps([ctx.content for ctx in news_contexts[:3]], ensure_ascii=False, indent=2)}

            ë‹¤ìŒì„ ë¶„ì„í•´ì£¼ì„¸ìš”:
            1. ë¹„ì¦ˆë‹ˆìŠ¤ ì´ë²¤íŠ¸ê°€ ê¸°ì—… ì„±ì¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥
            2. ì‹œì¥ í¬ì§€ì…˜ ë³€í™” ë° ê²½ìŸ ìš°ìœ„
            3. ë§¤ì¶œ ë° ìˆ˜ìµì„±ì— ëŒ€í•œ ì˜í–¥ ì˜ˆì¸¡
            4. ì¥ê¸°ì  ì‚¬ì—… ì „ëµ ê´€ì ì—ì„œì˜ ì˜ë¯¸

            ê¸°ì—… ë¶„ì„ ê´€ì ì—ì„œ ì¢…í•©ì ì¸ í‰ê°€ë¥¼ í•´ì£¼ì„¸ìš”.
            """

            response = await self._llm_invoke(relationship_prompt)

            relationship = {
                "type": "business_news_correlation",
                "analysis": response,
                "confidence": 0.75,
                "timestamp": str(asyncio.get_event_loop().time())
            }
            relationships.append(relationship)

        except Exception as e:
            logger.warning(f"ë¹„ì¦ˆë‹ˆìŠ¤-ë‰´ìŠ¤ ê´€ê³„ ë¶„ì„ ì‹¤íŒ¨: {e}")

    async def _synthesize_report(self, state: LangGraphReportState) -> LangGraphReportState:
        """7ë‹¨ê³„: ì¢…í•© ë¦¬í¬íŠ¸ í•©ì„±"""

        state["execution_log"].append("ğŸ“ ë¦¬í¬íŠ¸ í•©ì„± ì‹œì‘")

        # ì„¹ì…˜ë³„ ë¦¬í¬íŠ¸ ìƒì„±
        sections = {}

        # 1. ê°œìš” ì„¹ì…˜
        overview_prompt = f"""
        ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ '{state['query']}'ì— ëŒ€í•œ ê°œìš”ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:

        ìˆ˜ì§‘ëœ ë°ì´í„°:
        - ì´ ì»¨í…ìŠ¤íŠ¸: {len(state['contexts'])}ê°œ
        - ìƒì„±ëœ ì¸ì‚¬ì´íŠ¸: {len(state['insights'])}ê°œ
        - ê´€ê³„ ë¶„ì„: {len(state['relationships'])}ê°œ

        ë¶„ì„ ê¹Šì´: {state['analysis_depth'].value}

        ê°„ê²°í•˜ë©´ì„œë„ í¬ê´„ì ì¸ ê°œìš”ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
        """

        try:
            overview_response = await self._llm_invoke(overview_prompt)
            sections["overview"] = overview_response
        except Exception as e:
            sections["overview"] = f"ê°œìš” ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"

        # 2. í•µì‹¬ ë°œê²¬ì‚¬í•­
        if state["insights"]:
            key_findings = "\n\n".join([
                f"**{insight.get('title', insight['type'])} ({insight['type']})**\n"
                f"{insight.get('finding', insight.get('content', ''))}\n"
                f"*ê·¼ê±°*: {', '.join(insight.get('evidence', []))}\n"
                f"*ì˜ë¯¸*: {insight.get('significance', '')}"
                for insight in state["insights"]
            ])
            sections["key_findings"] = key_findings

        # 3. ê´€ê³„ ë¶„ì„
        if state["relationships"]:
            relationship_analysis = "\n\n".join([
                f"**{rel['type']} ({rel.get('strength', 'moderate')})**\n"
                f"ì—”í‹°í‹°: {', '.join(rel.get('entities', []))}\n"
                f"{rel.get('relationship', rel.get('analysis', ''))}\n"
                f"*ì˜í–¥*: {rel.get('impact', '')}"
                for rel in state["relationships"]
            ])
            sections["relationships"] = relationship_analysis

        # 4. ë°ì´í„° ìš”ì•½
        data_summary = self._generate_data_summary(state["contexts"])
        sections["data_summary"] = data_summary

        # 5. ìµœì¢… ë¦¬í¬íŠ¸ í•©ì„±
        final_report_prompt = f"""
        ë‹¤ìŒ ì„¹ì…˜ë“¤ì„ ì¢…í•©í•˜ì—¬ ì „ë¬¸ì ì¸ ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:

        ì§ˆì˜: {state['query']}

        ì„¹ì…˜ë³„ ë‚´ìš©:

        ## ê°œìš”
        {sections.get('overview', 'N/A')}

        ## í•µì‹¬ ë°œê²¬ì‚¬í•­
        {sections.get('key_findings', 'N/A')}

        ## ê´€ê³„ ë¶„ì„
        {sections.get('relationships', 'N/A')}

        ## ë°ì´í„° ìš”ì•½
        {sections.get('data_summary', 'N/A')}

        ë‹¤ìŒ êµ¬ì¡°ë¡œ ì „ë¬¸ì ì¸ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:
        1. ìš”ì•½ (Executive Summary)
        2. ìƒì„¸ ë¶„ì„
        3. ì‹œì‚¬ì  ë° ì „ë§
        4. ê¶Œì¥ì‚¬í•­

        ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ê³ , ë¹„ì¦ˆë‹ˆìŠ¤ ì˜ì‚¬ê²°ì •ì— ë„ì›€ì´ ë˜ëŠ” ì‹¤ìš©ì ì¸ ë‚´ìš©ìœ¼ë¡œ êµ¬ì„±í•´ì£¼ì„¸ìš”.
        """

        try:
            final_response = await self._llm_invoke(final_report_prompt)
            state["final_report"] = final_response
            state["report_sections"] = sections
        except Exception as e:
            state["final_report"] = f"ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
            state["report_sections"] = sections

        state["execution_log"].append("âœ… ë¦¬í¬íŠ¸ í•©ì„± ì™„ë£Œ")

        return state

    async def _quality_check(self, state: LangGraphReportState) -> LangGraphReportState:
        """8ë‹¨ê³„: ê°œì„ ëœ í’ˆì§ˆ ê²€ì¦"""

        state["execution_log"].append("ğŸ¯ í’ˆì§ˆ ê²€ì¦ ì‹œì‘")

        # ìƒì¥ì‚¬ ë¶„ì„ì— íŠ¹í™”ëœ í’ˆì§ˆ í‰ê°€ ìš”ì†Œë“¤
        quality_factors = {}

        # 1. ë°ì´í„° ë‹¤ì–‘ì„± (ìƒì¥ì‚¬ ë¶„ì„ì— í•„ìš”í•œ ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤)
        context_types = set(ctx.type for ctx in state["contexts"])
        expected_types = {"company", "news", "financial", "stock"}
        type_coverage = len(context_types.intersection(expected_types)) / len(expected_types)
        quality_factors["data_diversity"] = type_coverage

        # 2. íˆ¬ì ê´€ë ¨ ì»¨í…ì¸  í’ˆì§ˆ
        investment_keywords = ["ì£¼ê°€", "íˆ¬ì", "ì¬ë¬´", "ìˆ˜ìµ", "ì„±ì¥", "ë°°ë‹¹", "ë°¸ë¥˜ì—ì´ì…˜", "ë¦¬ìŠ¤í¬"]
        content_quality = self._evaluate_investment_content_quality(state["final_report"], state["query"], investment_keywords)
        quality_factors["investment_content_quality"] = content_quality

        # 3. ìƒì¥ì‚¬ ë¶„ì„ êµ¬ì¡°ì  ì™„ì„±ë„
        required_sections = ["ê¸°ì—…ë¶„ì„", "ì¬ë¬´ë¶„ì„", "íˆ¬ìí¬ì¸íŠ¸", "ë¦¬ìŠ¤í¬", "ì „ë§"]
        structure_score = sum(1 for section in required_sections
                            if any(keyword in state["final_report"] for keyword in [section, section.lower()]))
        quality_factors["analysis_structure"] = structure_score / len(required_sections)

        # 4. ì •ëŸ‰ì  ë¶„ì„ í¬í•¨ë„
        quantitative_keywords = ["ë§¤ì¶œ", "ì˜ì—…ì´ìµ", "ìˆœì´ìµ", "ROE", "PER", "PBR", "ë¶€ì±„ë¹„ìœ¨", "%"]
        quant_score = sum(1 for keyword in quantitative_keywords if keyword in state["final_report"])
        quality_factors["quantitative_analysis"] = min(quant_score / len(quantitative_keywords), 1.0)

        # 5. ê´€ê³„ ë¶„ì„ í’ˆì§ˆ (ìƒì¥ì‚¬ëŠ” ë‹¤ì–‘í•œ ì´í•´ê´€ê³„ì ë¶„ì„ì´ ì¤‘ìš”)
        quality_factors["relationship_quality"] = min(len(state["relationships"]) / 2.0, 1.0)

        # 6. ì¸ì‚¬ì´íŠ¸ ê¹Šì´ (ìƒì¥ì‚¬ëŠ” íˆ¬ì ê´€ì ì˜ ê¹Šì€ ì¸ì‚¬ì´íŠ¸ í•„ìš”)
        insight_depth = self._evaluate_insight_depth(state["insights"])
        quality_factors["insight_depth"] = insight_depth

        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (ìƒì¥ì‚¬ ë¶„ì„ì— ë§ê²Œ ì¡°ì •)
        weights = {
            "data_diversity": 0.2,           # ë°ì´í„° ë‹¤ì–‘ì„±
            "investment_content_quality": 0.3,  # íˆ¬ì ê´€ë ¨ ë‚´ìš© í’ˆì§ˆ
            "analysis_structure": 0.2,       # ë¶„ì„ êµ¬ì¡°
            "quantitative_analysis": 0.1,    # ì •ëŸ‰ì  ë¶„ì„
            "relationship_quality": 0.1,     # ê´€ê³„ ë¶„ì„
            "insight_depth": 0.1            # ì¸ì‚¬ì´íŠ¸ ê¹Šì´
        }

        quality_score = sum(score * weights[factor] for factor, score in quality_factors.items())
        state["quality_score"] = quality_score

        # í’ˆì§ˆ ë“±ê¸‰ ê²°ì •
        if quality_score >= 0.75:
            quality_level = ReportQuality.EXCELLENT
        elif quality_score >= 0.55:
            quality_level = ReportQuality.GOOD
        elif quality_score >= 0.35:
            quality_level = ReportQuality.ACCEPTABLE
        else:
            quality_level = ReportQuality.POOR

        state["quality_level"] = quality_level
        state["execution_log"].append(f"âœ… í’ˆì§ˆ ì ìˆ˜: {quality_score:.2f} ({quality_level.value})")
        state["execution_log"].append(f"   ì„¸ë¶€ ì ìˆ˜: {quality_factors}")

        return state

    async def _enhance_report(self, state: LangGraphReportState) -> LangGraphReportState:
        """9ë‹¨ê³„: ë¦¬í¬íŠ¸ ê°œì„ """

        retry_count = state.get("retry_count", 0)
        quality_score = state.get("quality_score", 0.0)

        # quality_level ì•ˆì „ ì²´í¬
        quality_level = state.get("quality_level", ReportQuality.POOR)
        if not isinstance(quality_level, ReportQuality):
            quality_level = ReportQuality.POOR
            state["quality_level"] = quality_level

        state["execution_log"].append(f"ğŸ”§ ë¦¬í¬íŠ¸ ê°œì„  ì‹œì‘ (ì¬ì‹œë„ {retry_count}íšŒì°¨)")

        try:
            # í’ˆì§ˆ ë¬¸ì œ ì§„ë‹¨
            issues = []
            if len(state.get("final_report", "")) < 300:
                issues.append("ë¦¬í¬íŠ¸ ê¸¸ì´ ë¶€ì¡±")
            if len(state.get("insights", [])) < 1:
                issues.append("ì¸ì‚¬ì´íŠ¸ ë¶€ì¡±")
            if quality_score < 0.4:
                issues.append("ì „ë°˜ì  í’ˆì§ˆ ì €í•˜")

            # ê°œì„  ì „ëµ ê²°ì •
            enhancement_strategy = self._determine_enhancement_strategy(issues)

            # ë§ì¶¤í˜• ê°œì„  í”„ë¡¬í”„íŠ¸ ìƒì„±
            if enhancement_strategy == "expand_content":
                enhancement_prompt = f"""
                ë‹¤ìŒ ë¦¬í¬íŠ¸ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤. ë‚´ìš©ì„ í™•ì¥í•˜ê³  ë³´ì™„í•´ì£¼ì„¸ìš”:

                í˜„ì¬ ë¦¬í¬íŠ¸:
                {state['final_report']}

                ì‚¬ìš© ê°€ëŠ¥í•œ ì¶”ê°€ ë°ì´í„°:
                - ìˆ˜ì§‘ëœ ì»¨í…ìŠ¤íŠ¸: {len(state.get('contexts', []))}ê°œ
                - ìƒì„±ëœ ì¸ì‚¬ì´íŠ¸: {len(state.get('insights', []))}ê°œ

                ë‹¤ìŒì„ í¬í•¨í•˜ì—¬ ë¦¬í¬íŠ¸ë¥¼ í™•ì¥í•´ì£¼ì„¸ìš”:
                1. ìƒì„¸í•œ ë¶„ì„ ë‚´ìš©
                2. êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ì™€ ê·¼ê±°
                3. ì‹œì¥ ì˜í–¥ ë¶„ì„
                4. í–¥í›„ ì „ë§ê³¼ ê¶Œì¥ì‚¬í•­

                ìµœì†Œ 800ì ì´ìƒì˜ ì „ë¬¸ì ì¸ ë¦¬í¬íŠ¸ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
                """

            elif enhancement_strategy == "add_insights":
                # ì¸ì‚¬ì´íŠ¸ ë¶€ì¡± ì‹œ ì¶”ê°€ ìƒì„±
                available_contexts = [ctx for ctx in state.get("contexts", []) if ctx.confidence > 0.5]
                context_summary = self._summarize_context_data(available_contexts[:5])

                enhancement_prompt = f"""
                í˜„ì¬ ë¦¬í¬íŠ¸ì— ë¶„ì„ì  ì¸ì‚¬ì´íŠ¸ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ë‹¤ìŒ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¦¬í¬íŠ¸ë¥¼ ê°œì„ í•´ì£¼ì„¸ìš”:

                í˜„ì¬ ë¦¬í¬íŠ¸:
                {state['final_report']}

                ì¶”ê°€ ë°ì´í„°:
                {context_summary}

                ë‹¤ìŒ ê´€ì ì—ì„œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”:
                1. í•µì‹¬ ë°œê²¬ì‚¬í•­ê³¼ ê·¸ ì˜ë¯¸
                2. ë¹„ì¦ˆë‹ˆìŠ¤ ì˜í–¥ë„ ë¶„ì„
                3. ê²½ìŸì‚¬ ëŒ€ë¹„ ìœ„ì¹˜
                4. íˆ¬ì ë° ì‚¬ì—… ê¸°íšŒ
                5. ë¦¬ìŠ¤í¬ ìš”ì†Œì™€ ëŒ€ì‘ë°©ì•ˆ

                ë¶„ì„ì ì´ê³  ì‹¤ìš©ì ì¸ ì¸ì‚¬ì´íŠ¸ê°€ í’ë¶€í•œ ë¦¬í¬íŠ¸ë¡œ ê°œì„ í•´ì£¼ì„¸ìš”.
                """

            else:  # general_improvement
                enhancement_prompt = f"""
                ë‹¤ìŒ ë¦¬í¬íŠ¸ì˜ ì „ë°˜ì ì¸ í’ˆì§ˆì„ ê°œì„ í•´ì£¼ì„¸ìš”:

                í˜„ì¬ ë¦¬í¬íŠ¸:
                {state['final_report']}

                í’ˆì§ˆ ë¬¸ì œì : {', '.join(issues)}

                ê°œì„  ìš”êµ¬ì‚¬í•­:
                1. ë…¼ë¦¬ì  êµ¬ì¡° ê°•í™” (ë„ì…-ë³¸ë¡ -ê²°ë¡ )
                2. êµ¬ì²´ì  ë°ì´í„°ì™€ ìˆ˜ì¹˜ ë³´ê°•
                3. ì „ë¬¸ì„±ê³¼ ì‹ ë¢°ì„± í–¥ìƒ
                4. ì½ê¸° ì‰¬ìš´ ë¬¸ì²´ì™€ í˜•ì‹
                5. ì‹¤í–‰ ê°€ëŠ¥í•œ ê¶Œì¥ì‚¬í•­

                ì „ë¬¸ì ì´ê³  ì™„ì„±ë„ ë†’ì€ ë¹„ì¦ˆë‹ˆìŠ¤ ë¦¬í¬íŠ¸ë¡œ ì¬ì‘ì„±í•´ì£¼ì„¸ìš”.
                """

            # LLMì„ í†µí•œ ë¦¬í¬íŠ¸ ê°œì„ 
            enhanced_response = await self._llm_invoke(enhancement_prompt)

            # ê°œì„  ê²°ê³¼ ê²€ì¦
            if len(enhanced_response) > len(state.get("final_report", "")):
                state["final_report"] = enhanced_response
                state["execution_log"].append(f"âœ… ë¦¬í¬íŠ¸ ê°œì„  ì™„ë£Œ ({enhancement_strategy})")
            else:
                state["execution_log"].append("âš ï¸ ê°œì„  íš¨ê³¼ ë¯¸ë¯¸, ê¸°ì¡´ ë¦¬í¬íŠ¸ ìœ ì§€")

        except Exception as e:
            logger.error(f"[LangGraph] ë¦¬í¬íŠ¸ ê°œì„  ì‹¤íŒ¨: {e}")
            state["execution_log"].append(f"âŒ ë¦¬í¬íŠ¸ ê°œì„  ì‹¤íŒ¨: {e}")

        return state

    def _determine_enhancement_strategy(self, issues: List[str]) -> str:
        """ê°œì„  ì „ëµ ê²°ì •"""
        if "ë¦¬í¬íŠ¸ ê¸¸ì´ ë¶€ì¡±" in issues:
            return "expand_content"
        elif "ì¸ì‚¬ì´íŠ¸ ë¶€ì¡±" in issues:
            return "add_insights"
        else:
            return "general_improvement"

    # ========== Context Engineering Helper Methods ==========

    def _prepare_contexts_for_engineering(self, contexts: List[ContextItem]) -> List[Dict[str, Any]]:
        """ì»¨í…ìŠ¤íŠ¸ë¥¼ Context Engineeringìš© dict í˜•ì‹ìœ¼ë¡œ ë³€í™˜

        í•˜ì´ë¸Œë¦¬ë“œ ì „ëµ:
        1. ì‹ ê·œ ìŠ¤í‚¤ë§ˆ í•„ë“œ (quality_score, is_featured ë“±) í™œìš©
        2. í•„ë“œ ì—†ìœ¼ë©´ ê¸°ì¡´ ë°ì´í„°ë¡œ ìì²´ ê³„ì‚°
        """
        contexts_as_dicts = []
        for ctx in contexts:
            if isinstance(ctx, dict):
                ctx_dict = ctx
            else:
                # ContextItem dataclassë¥¼ dictë¡œ ë³€í™˜
                ctx_dict = {
                    "source": ctx.source,
                    "type": ctx.type,
                    "content": str(ctx.content.get("title", "")) + " " + str(ctx.content.get("summary", ""))[:500],
                    "text": str(ctx.content)[:1000],
                    "confidence": ctx.confidence,
                    "relevance": ctx.relevance,
                    "timestamp": ctx.timestamp,
                    "metadata": ctx.content,

                    # â­â­â­ ì‹ ê·œ ìŠ¤í‚¤ë§ˆ í•„ë“œ (ê¸ˆì¼ë¶€í„° ì±„ì›Œì§)
                    "quality_score": ctx.content.get("quality_score"),  # NULL ê°€ëŠ¥
                    "is_featured": ctx.content.get("is_featured", False),
                    "neo4j_synced": ctx.content.get("neo4j_synced", False),
                    "ontology_status": ctx.content.get("ontology_status"),
                    "neo4j_node_count": ctx.content.get("neo4j_node_count", 0),
                    "event_chain_id": ctx.content.get("event_chain_id"),
                }

            # í•„ë“œ ì—†ìœ¼ë©´ ìì²´ ê³„ì‚° (Fallback)
            if ctx_dict.get("quality_score") is None:
                ctx_dict["quality_score"] = self._calculate_content_quality(ctx_dict)

            contexts_as_dicts.append(ctx_dict)
        return contexts_as_dicts

    def _calculate_content_quality(self, ctx: Dict[str, Any]) -> float:
        """ì»¨í…ì¸  ìì²´ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (ì‹ ê·œ í•„ë“œ ì—†ì„ ë•Œ Fallback)

        ê¸°ì¡´ ë°ì´í„°ë§Œìœ¼ë¡œ í’ˆì§ˆ í‰ê°€:
        - ë‚´ìš© ê¸¸ì´ (40%)
        - ì •ë³´ ë°€ë„ (30%)
        - ì œëª© í’ˆì§ˆ (15%)
        - ìš”ì•½ ì¡´ì¬ (15%)
        """
        import re

        content = ctx.get("content", "")
        metadata = ctx.get("metadata", {})

        # 1. ë‚´ìš© ê¸¸ì´ ì ìˆ˜ (0.0-1.0)
        content_length = len(content)
        if content_length > 1000:
            length_score = 1.0
        elif content_length > 500:
            length_score = 0.8
        elif content_length > 200:
            length_score = 0.5
        else:
            length_score = 0.3

        # 2. ì •ë³´ ë°€ë„ ì ìˆ˜ (í‚¤ì›Œë“œ ë‹¤ì–‘ì„±)
        has_numbers = bool(re.search(r'\d+', content))
        has_percentage = bool(re.search(r'\d+%', content))
        has_money = bool(re.search(r'\d+ì–µ|\d+ì¡°|\$\d+', content))
        has_company = bool(re.search(r'ì‚¼ì„±|SK|LG|í˜„ëŒ€|í¬ìŠ¤ì½”', content))

        density_score = 0.0
        density_score += 0.25 if has_numbers else 0
        density_score += 0.25 if has_percentage else 0
        density_score += 0.25 if has_money else 0
        density_score += 0.25 if has_company else 0

        # 3. ì œëª© í’ˆì§ˆ
        title = metadata.get("title", "")
        title_length = len(title)
        title_quality = 1.0 if 10 < title_length < 100 else 0.5

        # 4. ìš”ì•½ ì¡´ì¬
        summary = metadata.get("summary", "")
        has_summary = 1.0 if len(summary) > 50 else 0.5

        # ìµœì¢… ì ìˆ˜ (0.0-1.0)
        quality_score = (
            length_score * 0.40 +
            density_score * 0.30 +
            title_quality * 0.15 +
            has_summary * 0.15
        )

        return round(quality_score, 2)

    def _filter_by_source_priority(self, contexts: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ì¶œì²˜ ìš°ì„ ìˆœìœ„ ê¸°ë°˜ í•„í„°ë§ (Cascading Step 1)

        í•˜ì´ë¸Œë¦¬ë“œ ì „ëµ:
        1. ê¸°ë³¸ ì¶œì²˜ ê°€ì¤‘ì¹˜ ì ìš©
        2. â­ quality_score ë°˜ì˜ (ìˆìœ¼ë©´)
        3. â­ is_featured ë³´ë„ˆìŠ¤ (ìˆìœ¼ë©´)
        4. â­ neo4j_synced ë³´ë„ˆìŠ¤ (ìˆìœ¼ë©´)
        """
        # ê¸°ë³¸ ì¶œì²˜ë³„ ìš°ì„ ìˆœìœ„ ê°€ì¤‘ì¹˜
        source_weights = {
            "neo4j": 1.3,
            "opensearch": 1.0,
            "stock": 0.8
        }

        for ctx in contexts:
            source = ctx.get("source", "unknown")
            base_weight = source_weights.get(source, 0.5)

            # â­ ì‹ ê·œ ìŠ¤í‚¤ë§ˆ í•„ë“œ í™œìš© (ê¸ˆì¼ë¶€í„° ì±„ì›Œì§)
            quality_score = ctx.get("quality_score", 0.5)  # ìì²´ ê³„ì‚° ë˜ëŠ” DB ê°’

            # â­ is_featured ë³´ë„ˆìŠ¤ (+0.3)
            featured_bonus = 0.3 if ctx.get("is_featured", False) else 0

            # â­ neo4j_synced ë³´ë„ˆìŠ¤ (+0.2)
            synced_bonus = 0.2 if ctx.get("neo4j_synced", False) else 0

            # ìµœì¢… ê°€ì¤‘ì¹˜ = ì¶œì²˜ * (í’ˆì§ˆ + ë³´ë„ˆìŠ¤)
            final_weight = base_weight * (quality_score + featured_bonus + synced_bonus)

            ctx["source_weight"] = final_weight
            ctx["confidence"] = min(ctx.get("confidence", 0.5) * final_weight, 1.0)

        # confidence ê¸°ì¤€ ì •ë ¬
        return sorted(contexts, key=lambda x: x.get("confidence", 0), reverse=True)

    def _filter_by_recency(self, contexts: List[Dict[str, Any]], lookback_days: int) -> List[Dict[str, Any]]:
        """ìµœì‹ ì„± ê¸°ë°˜ í•„í„°ë§ (Cascading Step 2)

        ìµœê·¼ ë°ì´í„°ì— ê°€ì¤‘ì¹˜ ë¶€ì—¬
        """
        from datetime import datetime, timedelta

        cutoff_date = datetime.now() - timedelta(days=lookback_days)
        filtered = []

        for ctx in contexts:
            # timestamp íŒŒì‹± ì‹œë„
            try:
                # timestampê°€ ë¬¸ìì—´ì¸ ê²½ìš° íŒŒì‹±
                ts_str = ctx.get("timestamp", "")
                if ts_str:
                    # Unix timestamp ë˜ëŠ” ISO í˜•ì‹ ì§€ì›
                    try:
                        ts = float(ts_str)
                        ctx_date = datetime.fromtimestamp(ts)
                    except ValueError:
                        ctx_date = datetime.fromisoformat(ts_str.replace('Z', '+00:00'))

                    # ìµœì‹ ì„± ì ìˆ˜ ê³„ì‚° (ìµœê·¼ì¼ìˆ˜ë¡ ë†’ìŒ)
                    days_old = (datetime.now() - ctx_date).days
                    recency_score = max(0, 1 - (days_old / lookback_days))
                    ctx["recency_score"] = recency_score
                else:
                    ctx["recency_score"] = 0.5  # ë‚ ì§œ ì •ë³´ ì—†ìœ¼ë©´ ì¤‘ê°„ ì ìˆ˜

                filtered.append(ctx)
            except Exception:
                ctx["recency_score"] = 0.5
                filtered.append(ctx)

        return filtered

    def _filter_by_confidence(self, contexts: List[Dict[str, Any]], threshold: float = 0.3) -> List[Dict[str, Any]]:
        """ì‹ ë¢°ë„ ê¸°ë°˜ í•„í„°ë§ (Cascading Step 3)"""
        return [ctx for ctx in contexts if ctx.get("confidence", 0) >= threshold]

    def _rerank_with_metadata(
        self,
        contexts: List[Dict[str, Any]],
        query: str,
        analysis_plan: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ì¬ì •ë ¬ (Phase 4)

        í•˜ì´ë¸Œë¦¬ë“œ ì „ëµ:
        1. ê¸°ë³¸ ë©”íƒ€ë°ì´í„° (source, recency, semantic)
        2. â­ ì‹ ê·œ ìŠ¤í‚¤ë§ˆ ë©”íƒ€ë°ì´í„° (quality_score, neo4j_node_count ë“±)
        """
        for ctx in contexts:
            # ê¸°ë³¸ ì ìˆ˜ë“¤ (50%)
            source_weight = ctx.get("source_weight", 1.0)
            recency_score = ctx.get("recency_score", 0.5)
            semantic_score = ctx.get("semantic_score", 0.5)

            base_score = (
                semantic_score * 0.30 +      # Semantic ê´€ë ¨ì„± 30%
                source_weight * 0.12 +       # ì¶œì²˜ ì‹ ë¢°ë„ 12%
                recency_score * 0.08         # ìµœì‹ ì„± 8%
            )

            # â­ ì‹ ê·œ ìŠ¤í‚¤ë§ˆ ë©”íƒ€ë°ì´í„° (30%)
            quality_score = ctx.get("quality_score", 0.5)  # ìì²´ ê³„ì‚° ë˜ëŠ” DB ê°’
            is_featured = ctx.get("is_featured", False)
            neo4j_synced = ctx.get("neo4j_synced", False)
            neo4j_node_count = ctx.get("neo4j_node_count", 0)

            # Neo4j ì—°ê²°ì„± ë³´ë„ˆìŠ¤ (ìµœëŒ€ 0.1)
            connectivity_bonus = min(neo4j_node_count / 10.0, 0.1)

            schema_score = (
                quality_score * 0.15 +                              # quality_score 15%
                (0.1 if is_featured else 0.0) +                    # is_featured 10%
                (0.05 if neo4j_synced else 0.0) +                  # neo4j_synced 5%
                connectivity_bonus                                  # connectivity ìµœëŒ€ 10%
            )

            # Analysis plan alignment (20%)
            plan_alignment = self._calculate_plan_alignment(ctx, analysis_plan)

            # ìµœì¢… ì ìˆ˜ = ê¸°ë³¸(50%) + ìŠ¤í‚¤ë§ˆ(30%) + ê³„íš(20%)
            metadata_score = base_score + schema_score + (plan_alignment * 0.20)

            ctx["metadata_score"] = round(metadata_score, 3)

        # ë©”íƒ€ë°ì´í„° ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
        return sorted(contexts, key=lambda x: x.get("metadata_score", 0), reverse=True)

    def _calculate_plan_alignment(self, context: Dict[str, Any], analysis_plan: Dict[str, Any]) -> float:
        """ì»¨í…ìŠ¤íŠ¸ê°€ ë¶„ì„ ê³„íšê³¼ ì–¼ë§ˆë‚˜ ì¼ì¹˜í•˜ëŠ”ì§€ ê³„ì‚°"""
        if not analysis_plan:
            return 0.5

        score = 0.5  # ê¸°ë³¸ ì ìˆ˜

        # Primary focus í‚¤ì›Œë“œ ë§¤ì¹­
        primary_focus = analysis_plan.get("primary_focus", [])
        ctx_text = str(context.get("content", "")).lower()

        for focus in primary_focus:
            if focus.lower() in ctx_text:
                score += 0.1

        # Required data types ë§¤ì¹­
        required_types = analysis_plan.get("required_data_types", [])
        ctx_type = context.get("type", "")

        if ctx_type in required_types:
            score += 0.2

        return min(score, 1.0)

    def _sequence_contexts_for_reasoning(
        self,
        contexts: List[Dict[str, Any]],
        query: str
    ) -> List[Dict[str, Any]]:
        """ì •ë³´ ì „ë‹¬ ìˆœì„œ ìµœì í™” (Phase 5)

        ì •ë³´ ì „ë‹¬ ìˆœì„œ:
        1. Overview/Definitions (ê°œìš”/ì •ì˜) - ë°°ê²½ ì´í•´
        2. Recent News (ìµœê·¼ ë‰´ìŠ¤) - í˜„ì¬ ìƒí™©
        3. Relationships/Analysis (ê´€ê³„/ë¶„ì„) - ì‹¬í™” ì´í•´
        4. Supporting Data (ë³´ì¡° ë°ì´í„°) - ì¶”ê°€ ê·¼ê±°
        """
        # ì»¨í…ìŠ¤íŠ¸ íƒ€ì…ë³„ ìš°ì„ ìˆœìœ„
        type_priority = {
            "company": 1,      # ê¸°ì—… ì •ë³´ - ê°€ì¥ ë¨¼ì €
            "news": 2,         # ë‰´ìŠ¤ - ë‘ ë²ˆì§¸
            "event": 2,        # ì´ë²¤íŠ¸ - ë‰´ìŠ¤ì™€ ë™ì¼
            "contract": 3,     # ê³„ì•½ - ì„¸ ë²ˆì§¸
            "stock": 4,        # ì£¼ê°€ - ë„¤ ë²ˆì§¸ (ë³´ì¡° ì •ë³´)
            "analysis": 3      # ë¶„ì„ - ì„¸ ë²ˆì§¸
        }

        # ê° ì»¨í…ìŠ¤íŠ¸ì— ì‹œí€€ìŠ¤ ì ìˆ˜ ë¶€ì—¬
        for ctx in contexts:
            ctx_type = ctx.get("type", "unknown")
            recency = ctx.get("recency_score", 0.5)

            # íƒ€ì… ìš°ì„ ìˆœìœ„
            type_score = 5 - type_priority.get(ctx_type, 5)  # ì—­ìˆœ (1ì´ ê°€ì¥ ë†’ìŒ)

            # Sequence score = íƒ€ì… ìš°ì„ ìˆœìœ„ + ìµœì‹ ì„± ë³´ë„ˆìŠ¤
            sequence_score = type_score + (recency * 0.3)

            ctx["sequence_score"] = sequence_score

        # Sequence score ê¸°ì¤€ ì •ë ¬ (ë†’ì„ìˆ˜ë¡ ë¨¼ì €)
        sequenced = sorted(contexts, key=lambda x: x.get("sequence_score", 0), reverse=True)

        # ê°™ì€ íƒ€ì… ë‚´ì—ì„œëŠ” semantic_scoreë¡œ ì¬ì •ë ¬
        final_sequence = []
        for type_name in ["company", "news", "contract", "stock", "analysis"]:
            type_contexts = [c for c in sequenced if c.get("type") == type_name]
            type_contexts.sort(key=lambda x: x.get("combined_score", x.get("semantic_score", 0)), reverse=True)
            final_sequence.extend(type_contexts)

        # íƒ€ì… ë¶„ë¥˜ë˜ì§€ ì•Šì€ ê²ƒë“¤ ì¶”ê°€
        unclassified = [c for c in sequenced if c.get("type", "unknown") not in ["company", "news", "contract", "stock", "analysis"]]
        final_sequence.extend(unclassified)

        return final_sequence

    # ========== End of Context Engineering Helpers ==========

    def _should_enhance_report(self, state: LangGraphReportState) -> str:
        """ê°œì„ ëœ í’ˆì§ˆ ê²€ì¦ í›„ ê°œì„  ì—¬ë¶€ ê²°ì •"""

        # í’ˆì§ˆ ë ˆë²¨ì´ ì—†ê±°ë‚˜ ì˜ëª»ëœ ê°’ì´ë©´ POORë¡œ ì„¤ì •
        if "quality_level" not in state or not isinstance(state.get("quality_level"), ReportQuality):
            state["quality_level"] = ReportQuality.POOR
            logger.warning("[LangGraph] quality_levelì´ ëˆ„ë½ë˜ì—ˆê±°ë‚˜ ì˜ëª»ëœ ê°’ì…ë‹ˆë‹¤. POORë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
            return "enhance"

        quality_level = state["quality_level"]
        retry_count = state.get("retry_count", 0)
        quality_score = state.get("quality_score", 0.0)

        # ì¬ì‹œë„ íšŸìˆ˜ ì œí•œ (ìµœëŒ€ 1íšŒë¡œ ì¶•ì†Œ - ì„±ëŠ¥ ìµœì í™”)
        if retry_count >= 1:
            logger.warning(f"[LangGraph] ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼: {retry_count}íšŒ")
            state["execution_log"].append("âš ï¸ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼, í˜„ì¬ ê²°ê³¼ë¡œ ì™„ë£Œ")
            return "complete"

        # í’ˆì§ˆ ê¸°ì¤€ì— ë”°ë¥¸ ì¬ì‹œë„ ê²°ì • (ì¡°ê±´ ê°•í™” - ì„±ëŠ¥ ìµœì í™”)
        should_retry = False

        # POOR í’ˆì§ˆì´ê³  ì ìˆ˜ê°€ ì •ë§ ë‚®ì„ ë•Œë§Œ ì¬ì‹œë„ (0.3 ì´í•˜)
        if quality_level == ReportQuality.POOR and quality_score < 0.3:
            should_retry = True

        # ACCEPTABLEì€ ì¬ì‹œë„í•˜ì§€ ì•ŠìŒ (ì„±ëŠ¥ ìµœì í™”)

        # ë¦¬í¬íŠ¸ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ì¬ì‹œë„
        elif len(state.get("final_report", "")) < 300:
            should_retry = True
            state["execution_log"].append("âš ï¸ ë¦¬í¬íŠ¸ê°€ ë„ˆë¬´ ì§§ì•„ ì¬ì‹œë„")

        # ì¸ì‚¬ì´íŠ¸ê°€ ë¶€ì¡±í•˜ë©´ ì¬ì‹œë„
        elif len(state.get("insights", [])) < 1:
            should_retry = True
            state["execution_log"].append("âš ï¸ ì¸ì‚¬ì´íŠ¸ ë¶€ì¡±ìœ¼ë¡œ ì¬ì‹œë„")

        if should_retry:
            state["retry_count"] = retry_count + 1
            state["execution_log"].append(f"ğŸ”„ í’ˆì§ˆ ê°œì„  ì¬ì‹œë„ {state['retry_count']}íšŒì°¨ (ì ìˆ˜: {quality_score:.2f})")
            return "enhance"

        # í’ˆì§ˆì´ ì¶©ë¶„í•˜ë©´ ì™„ë£Œ
        state["execution_log"].append(f"âœ… í’ˆì§ˆ ê¸°ì¤€ ì¶©ì¡±, ë¦¬í¬íŠ¸ ì™„ë£Œ (ì ìˆ˜: {quality_score:.2f})")
        return "complete"

    # ========== í—¬í¼ ë©”ì„œë“œë“¤ ==========

    async def _llm_invoke(self, prompt: str) -> str:
        """LLM í˜¸ì¶œì„ ìœ„í•œ ë¹„ë™ê¸° ë˜í¼"""
        try:
            # OllamaLLMì€ ë™ê¸° í˜¸ì¶œë§Œ ì§€ì›í•˜ë¯€ë¡œ ë¹„ë™ê¸°ë¡œ ë˜í•‘
            import anyio
            response = await anyio.to_thread.run_sync(self.llm.invoke, prompt)
            return response
        except Exception as e:
            logger.error(f"[LangGraph] LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return "LLM ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def _safe_parse_keywords(self, content: str) -> List[str]:
        """LLM ì‘ë‹µì—ì„œ í‚¤ì›Œë“œ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ"""
        try:
            # JSON ë°°ì—´ í˜•íƒœ íŒŒì‹± ì‹œë„
            if '[' in content and ']' in content:
                start = content.find('[')
                end = content.rfind(']') + 1
                keywords_json = content[start:end]
                return json.loads(keywords_json)

            # ì½¤ë§ˆ êµ¬ë¶„ í…ìŠ¤íŠ¸ íŒŒì‹±
            keywords = [k.strip().strip('"\'') for k in content.split(',')]
            return keywords[:5]  # ìµœëŒ€ 5ê°œ
        except Exception:
            # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’
            return ["í‚¤ì›Œë“œ"]

    def _safe_parse_complexity(self, content: str) -> str:
        """LLM ì‘ë‹µì—ì„œ ë³µì¡ë„ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ"""
        content_lower = content.lower().strip()
        valid_complexities = ["shallow", "standard", "deep", "comprehensive"]

        for complexity in valid_complexities:
            if complexity in content_lower:
                return complexity

        return "standard"  # ê¸°ë³¸ê°’

    def _evaluate_content_quality(self, report: str, query: str) -> float:
        """ë¦¬í¬íŠ¸ ì»¨í…ì¸  í’ˆì§ˆ í‰ê°€"""
        if not report or len(report) < 100:
            return 0.0

        query_words = set(query.lower().split())
        report_words = set(report.lower().split())

        # í‚¤ì›Œë“œ í¬í•¨ë¥ 
        keyword_coverage = len(query_words.intersection(report_words)) / len(query_words)

        # ë¬¸ì¥ êµ¬ì¡° í‰ê°€ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
        sentences = report.split('.')
        avg_sentence_length = sum(len(s.split()) for s in sentences) / max(len(sentences), 1)

        # ì ì ˆí•œ ë¬¸ì¥ ê¸¸ì´ (10-30 ë‹¨ì–´)
        length_score = 1.0 if 10 <= avg_sentence_length <= 30 else 0.5

        return (keyword_coverage * 0.7 + length_score * 0.3)

    def _summarize_context_data(self, contexts: List[ContextItem]) -> str:
        """ì»¨í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ìš”ì•½í•˜ì—¬ í”„ë¡¬í”„íŠ¸ìš©ìœ¼ë¡œ ë³€í™˜"""
        summaries = []
        for i, ctx in enumerate(contexts[:3], 1):
            content = ctx.content
            if isinstance(content, dict):
                title = content.get('title', content.get('name', f'{ctx.type} {i}'))
                summary = f"{i}. {title}"
                if 'amount' in content:
                    summary += f" (ê¸ˆì•¡: {content['amount']})"
                if 'date' in content:
                    summary += f" ({content['date']})"
            else:
                summary = f"{i}. {str(content)[:100]}..."
            summaries.append(summary)
        return "\n".join(summaries)

    def _determine_graph_type(self, row: Dict[str, Any]) -> str:
        """ê·¸ë˜í”„ ë°ì´í„° íƒ€ì… ê²°ì • (ëª¨ë“  ìƒì¥ì‚¬ ëŒ€ì‘)"""
        labels = row.get("labels", [])

        # ê¸°ì—… ê´€ë ¨
        if "Company" in labels:
            return "company"
        elif "Industry" in labels or "Sector" in labels:
            return "industry"
        elif "Stock" in labels or "Security" in labels:
            return "stock"

        # ì¬ë¬´/ê³„ì•½ ê´€ë ¨
        elif "Contract" in labels or "Deal" in labels:
            return "contract"
        elif "Financial" in labels or "Revenue" in labels:
            return "financial"
        elif "Investment" in labels or "Funding" in labels:
            return "investment"

        # ë‰´ìŠ¤/ì´ë²¤íŠ¸ ê´€ë ¨
        elif "Event" in labels:
            return "event"
        elif "News" in labels or "Article" in labels:
            return "news"
        elif "Announcement" in labels or "Disclosure" in labels:
            return "announcement"

        # ê¸°íƒ€ ë¹„ì¦ˆë‹ˆìŠ¤ ì—”í„°í‹°
        elif "Product" in labels or "Service" in labels:
            return "product"
        elif "Person" in labels or "Executive" in labels:
            return "person"
        elif "Location" in labels or "Region" in labels:
            return "location"
        elif "Technology" in labels or "Patent" in labels:
            return "technology"

        # ê·œì œ/ì •ì±… ê´€ë ¨
        elif "Regulation" in labels or "Policy" in labels:
            return "regulation"
        elif "ESG" in labels or "Sustainability" in labels:
            return "esg"

        else:
            return "entity"

    def _calculate_relevance(self, row: Dict[str, Any], query: str) -> float:
        """ê·¸ë˜í”„ ë°ì´í„° ê´€ë ¨ì„± ê³„ì‚°"""
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­ ê¸°ë°˜ ê´€ë ¨ì„± ê³„ì‚°
        query_words = query.lower().split()
        content_text = json.dumps(row, ensure_ascii=False).lower()

        matches = sum(1 for word in query_words if word in content_text)
        return min(matches / len(query_words), 1.0)

    def _calculate_news_relevance(self, source_data: Dict[str, Any], query: str) -> float:
        """ë‰´ìŠ¤ ë°ì´í„° ê´€ë ¨ì„± ê³„ì‚°"""
        query_words = query.lower().split()
        title = source_data.get("title", "").lower()
        content = source_data.get("content", "").lower()

        title_matches = sum(1 for word in query_words if word in title)
        content_matches = sum(1 for word in query_words if word in content)

        # ì œëª© ë§¤ì¹˜ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜
        relevance = (title_matches * 2 + content_matches) / (len(query_words) * 3)
        return min(relevance, 1.0)

    async def _generate_extended_queries(self, original_query: str) -> List[str]:
        """í™•ì¥ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±"""
        expand_prompt = f"""
        ë‹¤ìŒ ì›ë³¸ ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ í™•ì¥ ê²€ìƒ‰ í‚¤ì›Œë“œ 3ê°œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:

        ì›ë³¸ ì¿¼ë¦¬: {original_query}

        ì—°ê´€ì„± ìˆëŠ” í‚¤ì›Œë“œë“¤ì„ JSON ë°°ì—´ë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”:
        ["í™•ì¥í‚¤ì›Œë“œ1", "í™•ì¥í‚¤ì›Œë“œ2", "í™•ì¥í‚¤ì›Œë“œ3"]
        """

        try:
            response = await self._llm_invoke(expand_prompt)
            extended_queries = json.loads(response)
            return extended_queries[:3]  # ìµœëŒ€ 3ê°œë§Œ
        except Exception:
            # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ í™•ì¥ í‚¤ì›Œë“œ ë°˜í™˜
            return [f"{original_query} ë¶„ì„", f"{original_query} ì „ë§", f"{original_query} ë™í–¥"]

    def _generate_data_summary(self, contexts: List[ContextItem]) -> str:
        """ë°ì´í„° ìš”ì•½ ìƒì„±"""
        summary_lines = [
            f"ğŸ“Š **ì´ ìˆ˜ì§‘ ë°ì´í„°**: {len(contexts)}ê°œ",
            "",
            "**ë°ì´í„° ì¶œì²˜ë³„ ë¶„í¬:**"
        ]

        # ì¶œì²˜ë³„ í†µê³„
        source_counts = {}
        for ctx in contexts:
            source_counts[ctx.source] = source_counts.get(ctx.source, 0) + 1

        for source, count in source_counts.items():
            summary_lines.append(f"- {source}: {count}ê°œ")

        summary_lines.extend([
            "",
            "**ë°ì´í„° í’ˆì§ˆ:**",
            f"- í‰ê·  ì‹ ë¢°ë„: {(sum(ctx.confidence for ctx in contexts) / len(contexts)) if len(contexts) else 0 :.2f}",
            f"- í‰ê·  ê´€ë ¨ì„±: {(sum(ctx.relevance for ctx in contexts) / len(contexts)) if len(contexts) else 0 :.2f}"
        ])

        return "\n".join(summary_lines)

    def _initialize_state(
        self,
        query: str,
        domain: Optional[str] = None,
        lookback_days: int = 180,
        analysis_depth: str = "standard",
        symbol: Optional[str] = None
    ) -> LangGraphReportState:
        """ì•ˆì „í•œ ìƒíƒœ ì´ˆê¸°í™”"""

        # ì…ë ¥ ê²€ì¦ ë° ì •ê·œí™”
        query = query.strip() if query else ""
        if not query:
            raise ValueError("ì¿¼ë¦¬ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")

        domain = domain.strip() if domain else None
        lookback_days = max(1, min(lookback_days, 365))  # 1~365ì¼ ë²”ìœ„ ì œí•œ

        # ë¶„ì„ ê¹Šì´ ê²€ì¦
        try:
            depth_enum = AnalysisDepth(analysis_depth)
        except ValueError:
            logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” ë¶„ì„ ê¹Šì´: {analysis_depth}, ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •")
            depth_enum = AnalysisDepth.STANDARD

        # ê¸°ë³¸ ìƒíƒœ ìƒì„±
        state = LangGraphReportState(
            # ì…ë ¥ ì •ë³´
            query=query,
            domain=domain,
            lookback_days=lookback_days,
            analysis_depth=depth_enum,

            # ìˆ˜ì§‘ëœ ì»¨í…ìŠ¤íŠ¸
            contexts=[],

            # ë¶„ì„ ê²°ê³¼
            insights=[],
            relationships=[],

            # ë¦¬í¬íŠ¸ ìƒì„±
            report_sections={},
            final_report="",

            # í’ˆì§ˆ ê´€ë¦¬
            quality_score=0.0,
            quality_level=ReportQuality.POOR,
            retry_count=0,

            # ë©”íƒ€ë°ì´í„°
            execution_log=[f"ğŸš€ LangGraph ì›Œí¬í”Œë¡œìš° ì‹œì‘ - ì¿¼ë¦¬: {query}"],
            processing_time=0.0
        )

        # ì„ íƒì  íŒŒë¼ë¯¸í„° ì¶”ê°€
        if symbol and symbol.strip():
            state["symbol"] = symbol.strip()

        return state

    # ========== ê³µê°œ API ==========

    async def stream_report(
        self,
        query: str,
        *,
        domain: Optional[str] = None,
        lookback_days: int = 180,
        analysis_depth: str = "standard",
        symbol: Optional[str] = None
    ):
        """ìŠ¤íŠ¸ë¦¬ë° ë³´ê³ ì„œ ìƒì„± (ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© ì „ì†¡)"""
        import time
        from typing import AsyncIterator

        start_time = time.time()

        # ì§„í–‰ë¥  ë§¤í•‘ (Context Engineering ì¶”ê°€)
        WORKFLOW_STAGES = {
            "analyze_query": (0.08, "ì¿¼ë¦¬ ë¶„ì„"),
            "plan_analysis": (0.12, "ë¶„ì„ ì „ëµ ìˆ˜ë¦½"),
            "collect_parallel_data": (0.18, "ë°ì´í„° ìˆ˜ì§‘"),
            "apply_context_engineering": (0.25, "ì»¨í…ìŠ¤íŠ¸ ìµœì í™”"),  # NEW
            "cross_validate_contexts": (0.30, "ë°ì´í„° ê²€ì¦"),
            "generate_insights": (0.45, "ì¸ì‚¬ì´íŠ¸ ìƒì„±"),
            "analyze_relationships": (0.60, "ê´€ê³„ ë¶„ì„"),
            "deep_reasoning": (0.75, "ì‹¬í™” ì¶”ë¡ "),
            "synthesize_report": (0.85, "ë³´ê³ ì„œ ì‘ì„±"),
            "quality_check": (0.95, "í’ˆì§ˆ ê²€ì‚¬"),
            "enhance_report": (1.00, "ë³´ê³ ì„œ ê°œì„ ")
        }

        try:
            # ì´ˆê¸° ìƒíƒœ
            initial_state = self._initialize_state(
                query=query,
                domain=domain,
                lookback_days=lookback_days,
                analysis_depth=analysis_depth,
                symbol=symbol
            )

            logger.info(f"[Streaming] ì‹œì‘: query={query}, depth={analysis_depth}")

            # ì‹œì‘ ì´ë²¤íŠ¸
            yield {
                "type": "start",
                "data": {
                    "query": query,
                    "analysis_depth": analysis_depth,
                    "timestamp": time.time()
                }
            }

            # LangGraph ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ (astream ì‚¬ìš©)
            final_state = None
            node_sequence = [
                "analyze_query", "plan_analysis", "collect_parallel_data",
                "apply_context_engineering",  # NEW
                "cross_validate_contexts", "generate_insights", "analyze_relationships",
                "deep_reasoning", "synthesize_report", "quality_check", "enhance_report"
            ]
            current_node_idx = 0

            async for state_chunk in self.workflow.astream(initial_state):
                # state_chunkëŠ” ê° ë…¸ë“œ ì‹¤í–‰ í›„ì˜ ì „ì²´ state
                final_state = state_chunk

                # ë…¸ë“œ ìˆœì„œëŒ€ë¡œ ì§„í–‰ ì´ë²¤íŠ¸ ì „ì†¡
                if current_node_idx < len(node_sequence):
                    node_name = node_sequence[current_node_idx]
                    progress, message = WORKFLOW_STAGES.get(node_name, (0.0, "ì²˜ë¦¬ ì¤‘"))

                    # ì™„ë£Œ ì´ë²¤íŠ¸ ì „ì†¡
                    partial_data = {}
                    if "insights" in state_chunk:
                        partial_data["insights_count"] = len(state_chunk["insights"])
                    if "relationships" in state_chunk:
                        partial_data["relationships_count"] = len(state_chunk["relationships"])
                    if "quality_score" in state_chunk:
                        partial_data["quality_score"] = state_chunk["quality_score"]

                    yield {
                        "type": "step",
                        "data": {
                            "node": node_name,
                            "status": "completed",
                            "elapsed_time": time.time() - start_time,
                            **partial_data
                        }
                    }

                    current_node_idx += 1

                    # ë‹¤ìŒ ë…¸ë“œ ì‹œì‘ ì´ë²¤íŠ¸ (ë§ˆì§€ë§‰ ë…¸ë“œê°€ ì•„ë‹ˆë©´)
                    if current_node_idx < len(node_sequence):
                        next_node = node_sequence[current_node_idx]
                        next_progress, next_message = WORKFLOW_STAGES.get(next_node, (0.0, "ì²˜ë¦¬ ì¤‘"))
                        yield {
                            "type": "progress",
                            "data": {
                                "stage": next_node,
                                "status": "started",
                                "message": next_message,
                                "progress": next_progress,
                                "elapsed_time": time.time() - start_time
                            }
                        }

            if not final_state:
                final_state = initial_state

            final_state["processing_time"] = time.time() - start_time

            # í’ˆì§ˆ ë ˆë²¨ ì²˜ë¦¬
            quality_level = "poor"
            if "quality_level" in final_state:
                if isinstance(final_state["quality_level"], ReportQuality):
                    quality_level = final_state["quality_level"].value
                else:
                    quality_level = str(final_state["quality_level"]).lower()
                    if quality_level not in ["poor", "acceptable", "good", "excellent"]:
                        quality_level = "poor"

            # ìµœì¢… ê²°ê³¼ ì „ì†¡
            yield {
                "type": "final",
                "data": {
                    "markdown": final_state.get("final_report", "ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨"),
                    "quality_score": final_state.get("quality_score", 0.0),
                    "quality_level": quality_level,
                    "contexts_count": len(final_state.get("contexts", [])),
                    "insights_count": len(final_state.get("insights", [])),
                    "relationships_count": len(final_state.get("relationships", [])),
                    "processing_time": final_state.get("processing_time", 0.0),
                    "retry_count": final_state.get("retry_count", 0),
                    "execution_log": final_state.get("execution_log", [])
                }
            }

            logger.info(f"[Streaming] ì™„ë£Œ: time={final_state['processing_time']:.1f}s, quality={final_state['quality_score']:.2f}")

        except Exception as e:
            logger.error(f"[Streaming] ì˜¤ë¥˜: {e}")
            logger.error(traceback.format_exc())

            yield {
                "type": "error",
                "data": {
                    "error": str(e),
                    "stage": current_node or "unknown",
                    "elapsed_time": time.time() - start_time
                }
            }

    async def generate_langgraph_report(
        self,
        query: str,
        *,
        domain: Optional[str] = None,
        lookback_days: int = 180,
        analysis_depth: str = "standard",
        symbol: Optional[str] = None
    ) -> Dict[str, Any]:
        """LangGraph ê¸°ë°˜ ê³ ê¸‰ ë¦¬í¬íŠ¸ ìƒì„± (ë™ê¸° ë²„ì „)"""

        import time
        start_time = time.time()

        try:
            # ì•ˆì „í•œ ìƒíƒœ ì´ˆê¸°í™”
            initial_state = self._initialize_state(
                query=query,
                domain=domain,
                lookback_days=lookback_days,
                analysis_depth=analysis_depth,
                symbol=symbol
            )

            logger.info(f"[LangGraph] ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘ - ì¿¼ë¦¬: {query}, ê¹Šì´: {analysis_depth}")

            # LangGraph ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
            final_state = await self.workflow.ainvoke(initial_state)

            # ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
            final_state["processing_time"] = time.time() - start_time

            # í’ˆì§ˆ ë ˆë²¨ ì•ˆì „ ì ‘ê·¼
            quality_level = "poor"  # ê¸°ë³¸ê°’ì„ poorë¡œ ë³€ê²½
            if "quality_level" in final_state:
                if isinstance(final_state["quality_level"], ReportQuality):
                    quality_level = final_state["quality_level"].value
                else:
                    # ë¬¸ìì—´ì´ë‚˜ ë‹¤ë¥¸ íƒ€ì…ì¸ ê²½ìš°
                    quality_level = str(final_state["quality_level"]).lower()
                    # ìœ íš¨í•œ ê°’ì¸ì§€ í™•ì¸
                    if quality_level not in ["poor", "acceptable", "good", "excellent"]:
                        quality_level = "poor"

            logger.info(f"[LangGraph] ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ - í’ˆì§ˆì ìˆ˜: {final_state['quality_score']:.2f}, ì²˜ë¦¬ì‹œê°„: {final_state['processing_time']:.2f}ì´ˆ")

            return {
                "markdown": final_state.get("final_report", "ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."),
                "quality_score": final_state.get("quality_score", 0.0),
                "quality_level": quality_level,
                "contexts_count": len(final_state.get("contexts", [])),
                "insights_count": len(final_state.get("insights", [])),
                "relationships_count": len(final_state.get("relationships", [])),
                "processing_time": final_state.get("processing_time", 0.0),
                "retry_count": final_state.get("retry_count", 0),
                "execution_log": final_state.get("execution_log", []),
                "sections": final_state.get("report_sections", {}),
                "type": "langgraph_enhanced",
                "meta": {
                    "query": query,
                    "domain": domain,
                    "lookback_days": lookback_days,
                    "analysis_depth": analysis_depth,
                    "confidence": final_state.get("quality_score", 0.0) * 100,
                    "coverage": min(len(final_state.get("contexts", [])) / 50 * 100, 100),
                    "search_time": final_state.get("processing_time", 0.0)
                }
            }

        except ValueError as e:
            logger.error(f"ì…ë ¥ ê²€ì¦ ì˜¤ë¥˜: {e}")
            return {
                "markdown": f"# ì…ë ¥ ì˜¤ë¥˜\n\n{str(e)}",
                "quality_score": 0.0,
                "quality_level": "poor",
                "error": str(e),
                "processing_time": time.time() - start_time,
                "type": "langgraph_validation_error"
            }
        except Exception as e:
            logger.error(f"LangGraph ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            logger.error(traceback.format_exc())
            return {
                "markdown": f"# ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨\n\nì‹œìŠ¤í…œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "quality_score": 0.0,
                "quality_level": "poor",
                "error": str(e),
                "processing_time": time.time() - start_time,
                "type": "langgraph_system_error"
            }

    # ========== í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì§€ì› ë©”ì„œë“œ ==========

    async def _langgraph_hybrid_search(
        self,
        query: str,
        lookback_days: int = 180,
        size: int = 50
    ) -> List[Dict[str, Any]]:
        """LangGraph ì „ìš© í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (í•œë²ˆì˜ ì¿¼ë¦¬ë¡œ í‚¤ì›Œë“œ + ë²¡í„°)"""

        try:
            from datetime import datetime, timedelta
            cutoff_date = (datetime.now() - timedelta(days=lookback_days)).strftime("%Y-%m-%d")

            # ì„ë² ë”©ì´ ìˆìœ¼ë©´ í•˜ì´ë¸Œë¦¬ë“œ, ì—†ìœ¼ë©´ í‚¤ì›Œë“œë§Œ
            if self.embedding_client:
                try:
                    # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
                    query_embedding = await self.embedding_client.encode(query)

                    # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê²€ìƒ‰ (ì„ë² ë”©ì€ ë‚˜ì¤‘ì— í›„ì²˜ë¦¬ë¡œ)
                    hybrid_query = {
                        "size": size * 2,  # ë²¡í„° í•„í„°ë§ì„ ìœ„í•´ ë” ë§ì´ ê°€ì ¸ì˜´
                        "query": {
                            "bool": {
                                "should": [
                                    {
                                        "multi_match": {
                                            "query": query,
                                            "fields": ["title^3", "text^2", "content"],
                                            "type": "best_fields",
                                            "fuzziness": "AUTO"
                                        }
                                    },
                                    {
                                        "match_phrase": {
                                            "title": {
                                                "query": query,
                                                "boost": 2.0
                                            }
                                        }
                                    }
                                ],
                                "filter": [
                                    {
                                        "range": {
                                            "created_datetime": {
                                                "gte": cutoff_date
                                            }
                                        }
                                    }
                                ]
                            }
                        },
                        "_source": ["text", "title", "url", "created_datetime", "metadata"]
                    }

                    result = await self.opensearch.search(
                        index=settings.news_bulk_index,
                        query=hybrid_query,
                        size=size * 2
                    )
                    hits = result.get("hits", {}).get("hits", [])

                    # ë²¡í„° ìœ ì‚¬ë„ë¡œ ì¬ì •ë ¬
                    if hits:
                        scored_hits = []
                        for hit in hits:
                            source = hit.get("_source", {})
                            doc_text = f"{source.get('title', '')} {source.get('text', source.get('content', ''))}"

                            if doc_text.strip():
                                try:
                                    doc_embedding = await self.embedding_client.encode(doc_text[:500])
                                    similarity = self._calculate_cosine_similarity(query_embedding, doc_embedding)

                                    # í‚¤ì›Œë“œ ì ìˆ˜ì™€ ë²¡í„° ìœ ì‚¬ë„ ê²°í•©
                                    keyword_score = hit.get("_score", 0)
                                    combined_score = (keyword_score * 0.4) + (similarity * 10)  # ë²¡í„°ì— ë†’ì€ ê°€ì¤‘ì¹˜

                                    hit["_vector_similarity"] = similarity
                                    hit["_combined_score"] = combined_score
                                    scored_hits.append(hit)

                                except Exception as e:
                                    logger.warning(f"ë¬¸ì„œ ì„ë² ë”© ì‹¤íŒ¨: {e}")
                                    hit["_combined_score"] = hit.get("_score", 0)
                                    scored_hits.append(hit)

                        # ê²°í•© ì ìˆ˜ë¡œ ì •ë ¬ í›„ ìƒìœ„ Nê°œ ë°˜í™˜
                        scored_hits.sort(key=lambda x: x.get("_combined_score", 0), reverse=True)
                        hits = scored_hits[:size]

                    logger.info(f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì™„ë£Œ: {len(hits)}ê±´")
                    return hits

                except Exception as e:
                    logger.warning(f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤íŒ¨, í‚¤ì›Œë“œë§Œ ì‚¬ìš©: {e}")

            # í‚¤ì›Œë“œ ê²€ìƒ‰ë§Œ
            keyword_query = {
                "size": size,
                "query": {
                    "bool": {
                        "should": [
                            {
                                "multi_match": {
                                    "query": query,
                                    "fields": ["title^3", "text^2", "content"],
                                    "type": "best_fields",
                                    "fuzziness": "AUTO"
                                }
                            }
                        ],
                        "filter": [
                            {
                                "range": {
                                    "created_datetime": {
                                        "gte": cutoff_date
                                    }
                                }
                            }
                        ]
                    }
                },
                "_source": ["text", "title", "url", "created_datetime", "metadata"]
            }

            result = await self.opensearch.search(
                index=settings.news_bulk_index,
                query=keyword_query,
                size=size
            )
            hits = result.get("hits", {}).get("hits", [])
            logger.info(f"í‚¤ì›Œë“œ ê²€ìƒ‰ ì™„ë£Œ: {len(hits)}ê±´")
            return hits

        except Exception as e:
            logger.error(f"ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    def _calculate_cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """ë‘ ë²¡í„° ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        try:
            import numpy as np

            # numpy ë°°ì—´ë¡œ ë³€í™˜
            a = np.array(vec1)
            b = np.array(vec2)

            # ë²¡í„° í¬ê¸°ê°€ ë‹¤ë¥´ë©´ 0 ë°˜í™˜
            if len(a) != len(b):
                return 0.0

            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            dot_product = np.dot(a, b)
            norm_a = np.linalg.norm(a)
            norm_b = np.linalg.norm(b)

            # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
            if norm_a == 0 or norm_b == 0:
                return 0.0

            similarity = dot_product / (norm_a * norm_b)

            # -1 ~ 1 ë²”ìœ„ë¥¼ 0 ~ 1 ë²”ìœ„ë¡œ ì •ê·œí™”
            return (similarity + 1.0) / 2.0

        except Exception as e:
            logger.warning(f"ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0

    def _calculate_graph_relevance(self, row: Dict[str, Any], query: str) -> float:
        """ê·¸ë˜í”„ ë°ì´í„°ì˜ ê´€ë ¨ì„± ê³„ì‚°"""
        try:
            relevance = 0.0
            query_lower = query.lower()

            # ë¼ë²¨ ê¸°ë°˜ ê´€ë ¨ì„±
            labels = row.get("labels", [])
            for label in labels:
                if any(keyword in label.lower() for keyword in query_lower.split()):
                    relevance += 0.3

            # ì†ì„± ê¸°ë°˜ ê´€ë ¨ì„±
            for key, value in row.items():
                if isinstance(value, str) and any(keyword in value.lower() for keyword in query_lower.split()):
                    relevance += 0.2

            # ê³„ì•½ ê¸ˆì•¡ì´ ìˆìœ¼ë©´ ê°€ì¤‘ì¹˜ ì¶”ê°€
            if "amount" in row and row.get("amount"):
                relevance += 0.1

            return min(relevance, 1.0)

        except Exception:
            return 0.5  # ê¸°ë³¸ê°’

    def _generate_sector_specific_insight_prompt(self, ctx_type: str, query: str, data_summary: str) -> str:
        """ì„¹í„°ë³„ ë§ì¶¤í˜• ì¸ì‚¬ì´íŠ¸ í”„ë¡¬í”„íŠ¸ ìƒì„±"""

        base_template = f"ì§ˆì˜ '{query}'ì— ëŒ€í•œ {ctx_type} ë°ì´í„° ë¶„ì„:\n\n{data_summary}\n\n"

        if ctx_type == "company":
            return base_template + """
ìƒì¥ì‚¬ ê¸°ì—… ë¶„ì„ ê´€ì ì—ì„œ ë‹¤ìŒ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:
1. [ê¸°ì—… ê²½ìŸë ¥] - ì‹œì¥ ë‚´ í¬ì§€ì…˜ ë° ì°¨ë³„í™” ìš”ì†Œ
2. [ì¬ë¬´ ê±´ì „ì„±] - ìˆ˜ìµì„±, ì„±ì¥ì„±, ì•ˆì •ì„± ì¢…í•© í‰ê°€
3. [íˆ¬ì ë§¤ë ¥ë„] - ì£¼ê°€ ì „ë§ ë° íˆ¬ì í¬ì¸íŠ¸
4. [ë¦¬ìŠ¤í¬ ìš”ì†Œ] - ì£¼ìš” ìš°ë ¤ì‚¬í•­ ë° ëŒ€ì‘ ì „ëµ

ê° í•­ëª©ì„ 2-3ë¬¸ì¥ìœ¼ë¡œ êµ¬ì²´ì ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”."""

        elif ctx_type in ["financial", "investment", "stock"]:
            return base_template + """
ì¬ë¬´/íˆ¬ì ë¶„ì„ ê´€ì ì—ì„œ ë‹¤ìŒ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:
1. [ìˆ˜ìµì„± ë¶„ì„] - ë§¤ì¶œ, ì˜ì—…ì´ìµ, ìˆœì´ìµ ì¶”ì„¸ ë° ë§ˆì§„ ë¶„ì„
2. [ì„±ì¥ì„± í‰ê°€] - ì „ë…„ ëŒ€ë¹„ ì„±ì¥ë¥  ë° ë¯¸ë˜ ì„±ì¥ ë™ë ¥
3. [ë°¸ë¥˜ì—ì´ì…˜] - ì£¼ê°€ ì ì •ì„± ë° íˆ¬ì ì‹œì  íŒë‹¨
4. [ë°°ë‹¹/ì£¼ì£¼í™˜ì›] - ë°°ë‹¹ ì •ì±… ë° ì£¼ì£¼ ì¹œí™” ì •ì±…

íˆ¬ìì ê´€ì ì—ì„œ ì‹¤ìš©ì ì¸ ë¶„ì„ì„ í•´ì£¼ì„¸ìš”."""

        elif ctx_type in ["news", "announcement"]:
            return base_template + """
ë‰´ìŠ¤/ê³µì‹œ ë¶„ì„ ê´€ì ì—ì„œ ë‹¤ìŒ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:
1. [ì‹œì¥ ë°˜ì‘] - ë‰´ìŠ¤ê°€ ì£¼ê°€ ë° ì‹œì¥ ì„¼í‹°ë¨¼íŠ¸ì— ë¯¸ì¹˜ëŠ” ì˜í–¥
2. [ì‚¬ì—… ì˜í–¥] - ê¸°ì—…ì˜ ì¤‘ì¥ê¸° ì‚¬ì—… ì „ëµì— ëŒ€í•œ ì‹œì‚¬ì 
3. [ê²½ìŸ í™˜ê²½] - ë™ì¢… ì—…ê³„ ë° ê²½ìŸì‚¬ ëŒ€ë¹„ ìƒëŒ€ì  ìœ„ì¹˜ ë³€í™”
4. [íˆ¬ì ì„íŒ©íŠ¸] - íˆ¬ìì í–‰ë™ ë³€í™” ë° ì£¼ëª©í•´ì•¼ í•  í¬ì¸íŠ¸

ì‹œì¥ ë¶„ì„ê°€ ê´€ì ì—ì„œ ì „ë¬¸ì ì¸ í•´ì„ì„ í•´ì£¼ì„¸ìš”."""

        elif ctx_type in ["industry", "sector"]:
            return base_template + """
ì‚°ì—…/ì„¹í„° ë¶„ì„ ê´€ì ì—ì„œ ë‹¤ìŒ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:
1. [ì‚°ì—… íŠ¸ë Œë“œ] - ì—…ê³„ ì „ë°˜ì˜ ì„±ì¥ ë™ë ¥ ë° ë³€í™” ìš”ì¸
2. [ê²½ìŸ êµ¬ë„] - ì£¼ìš” í”Œë ˆì´ì–´ë“¤ì˜ ì‹œì¥ ì ìœ ìœ¨ ë° ê²½ìŸ ì „ëµ
3. [ê·œì œ í™˜ê²½] - ì •ì±… ë³€í™”ê°€ ì—…ê³„ì— ë¯¸ì¹˜ëŠ” ì˜í–¥
4. [íˆ¬ì ê¸°íšŒ] - ì„¹í„° ë‚´ ìœ ë§ ì¢…ëª© ë° íˆ¬ì í…Œë§ˆ

ì„¹í„° ì• ë„ë¦¬ìŠ¤íŠ¸ ê´€ì ì—ì„œ ì¢…í•©ì ì¸ ë¶„ì„ì„ í•´ì£¼ì„¸ìš”."""

        elif ctx_type in ["technology", "product"]:
            return base_template + """
ê¸°ìˆ /ì œí’ˆ ë¶„ì„ ê´€ì ì—ì„œ ë‹¤ìŒ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:
1. [ê¸°ìˆ  í˜ì‹ ] - ì‹ ê¸°ìˆ  ë„ì… ë° íŠ¹í—ˆ í¬íŠ¸í´ë¦¬ì˜¤ ê°•í™” íš¨ê³¼
2. [ì‹œì¥ í™•ì¥] - ì‹ ì œí’ˆ/ì„œë¹„ìŠ¤ë¥¼ í†µí•œ ì‹œì¥ í™•ëŒ€ ê°€ëŠ¥ì„±
3. [ìˆ˜ìµ ê¸°ì—¬] - ê¸°ìˆ /ì œí’ˆì´ ë§¤ì¶œ ë° ìˆ˜ìµì„±ì— ë¯¸ì¹˜ëŠ” ì˜í–¥
4. [ê²½ìŸ ìš°ìœ„] - ê¸°ìˆ ë ¥ ê¸°ë°˜ ì§€ì†ê°€ëŠ¥í•œ ê²½ìŸ ìš°ìœ„ êµ¬ì¶•

ê¸°ìˆ  ë¶„ì„ ì „ë¬¸ê°€ ê´€ì ì—ì„œ í‰ê°€í•´ì£¼ì„¸ìš”."""

        else:
            # ê¸°ë³¸ í…œí”Œë¦¿
            return base_template + """
ë‹¤ìŒ ê´€ì ì—ì„œ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:
1. [ì£¼ìš” ë°œê²¬ì‚¬í•­] - ë°ì´í„°ì—ì„œ ë‚˜íƒ€ë‚˜ëŠ” í•µì‹¬ íŠ¸ë Œë“œ
2. [ë¹„ì¦ˆë‹ˆìŠ¤ ì˜í–¥] - ê¸°ì—… ìš´ì˜ ë° ì „ëµì— ë¯¸ì¹˜ëŠ” ì˜í–¥
3. [ì‹œì¥ ì˜ë¯¸] - ì£¼ì‹ì‹œì¥ ë° íˆ¬ììë“¤ì—ê²Œ ì£¼ëŠ” ì‹œì‚¬ì 
4. [í–¥í›„ ì „ë§] - ì˜ˆìƒë˜ëŠ” ë³€í™” ë° ì£¼ëª©í•  í¬ì¸íŠ¸

ì „ë¬¸ ì• ë„ë¦¬ìŠ¤íŠ¸ ê´€ì ì—ì„œ ë¶„ì„í•´ì£¼ì„¸ìš”."""

    async def _collect_stock_data(self, state: LangGraphReportState, contexts: List[ContextItem]):
        """ì£¼ê°€ ë° ì¬ë¬´ ë°ì´í„° ìˆ˜ì§‘"""
        try:
            # ê¸°ì—…ëª… ë˜ëŠ” ì‹¬ë³¼ì—ì„œ ì£¼ê°€ ë°ì´í„° ìˆ˜ì§‘
            company_names = []
            symbols = []

            # ì»¨í…ìŠ¤íŠ¸ì—ì„œ ê¸°ì—… ì •ë³´ ì¶”ì¶œ
            for ctx in contexts:
                if ctx.type == "company":
                    content = ctx.content
                    if isinstance(content, dict):
                        if "name" in content:
                            company_names.append(content["name"])
                        if "symbol" in content or "ticker" in content:
                            symbol = content.get("symbol") or content.get("ticker")
                            if symbol:
                                symbols.append(symbol)

            # ì¿¼ë¦¬ì—ì„œë„ ì‹¬ë³¼ ì¶”ì¶œ ì‹œë„
            if state.get("symbol"):
                symbols.append(state["symbol"])

            # ì£¼ê°€ ë°ì´í„° ìˆ˜ì§‘
            for symbol in symbols[:3]:  # ìµœëŒ€ 3ê°œ ì‹¬ë³¼
                try:
                    # StockMCPë¥¼ í†µí•œ ì£¼ê°€ ë°ì´í„° ìˆ˜ì§‘
                    stock_data = await self._fetch_stock_info(symbol)

                    if stock_data:
                        stock_context = ContextItem(
                            source="stock_api",
                            type="stock",
                            content=stock_data,
                            confidence=0.9,
                            relevance=0.8,
                            timestamp=str(asyncio.get_event_loop().time())
                        )
                        contexts.append(stock_context)

                except Exception as e:
                    logger.warning(f"ì£¼ê°€ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨ ({symbol}): {e}")

        except Exception as e:
            logger.warning(f"ì£¼ê°€ ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")

    async def _fetch_stock_info(self, symbol: str) -> Dict[str, Any]:
        """ì£¼ì‹ ì •ë³´ ì¡°íšŒ"""
        try:
            # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ ë”ë¯¸ ë°ì´í„°ë¥¼ ë°˜í™˜
            # ì‹¤ì œë¡œëŠ” ì™¸ë¶€ API (Yahoo Finance, Alpha Vantage ë“±) ì—°ë™
            return {
                "symbol": symbol,
                "current_price": "ì¡°íšŒ í•„ìš”",
                "price_change": "ì¡°íšŒ í•„ìš”",
                "price_change_percent": "ì¡°íšŒ í•„ìš”",
                "volume": "ì¡°íšŒ í•„ìš”",
                "market_cap": "ì¡°íšŒ í•„ìš”",
                "pe_ratio": "ì¡°íšŒ í•„ìš”",
                "dividend_yield": "ì¡°íšŒ í•„ìš”",
                "52_week_high": "ì¡°íšŒ í•„ìš”",
                "52_week_low": "ì¡°íšŒ í•„ìš”",
                "note": "ì‹¤ì œ API ì—°ë™ ì‹œ ì‹¤ì‹œê°„ ë°ì´í„° ì œê³µ"
            }

        except Exception as e:
            logger.error(f"ì£¼ì‹ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨ ({symbol}): {e}")
            return None

    def _evaluate_investment_content_quality(self, report: str, query: str, investment_keywords: List[str]) -> float:
        """íˆ¬ì ê´€ë ¨ ì»¨í…ì¸  í’ˆì§ˆ í‰ê°€"""
        try:
            if not report or len(report) < 100:
                return 0.0

            # íˆ¬ì í‚¤ì›Œë“œ í¬í•¨ ì •ë„
            keyword_count = sum(1 for keyword in investment_keywords if keyword in report)
            keyword_score = min(keyword_count / len(investment_keywords), 1.0)

            # ì¿¼ë¦¬ ê´€ë ¨ì„±
            query_words = set(query.lower().split())
            report_words = set(report.lower().split())
            relevance_score = len(query_words.intersection(report_words)) / len(query_words)

            # íˆ¬ì ë¶„ì„ ë¬¸êµ¬ í™•ì¸
            analysis_phrases = ["íˆ¬ì í¬ì¸íŠ¸", "ë¦¬ìŠ¤í¬ ìš”ì¸", "ëª©í‘œì£¼ê°€", "íˆ¬ìì˜ê²¬", "ì¬ë¬´ ë¶„ì„"]
            phrase_score = sum(1 for phrase in analysis_phrases if phrase in report) / len(analysis_phrases)

            # ê°€ì¤‘ í‰ê· 
            return (keyword_score * 0.4 + relevance_score * 0.3 + phrase_score * 0.3)

        except Exception:
            return 0.5

    def _evaluate_insight_depth(self, insights: List[Dict[str, Any]]) -> float:
        """ì¸ì‚¬ì´íŠ¸ ê¹Šì´ í‰ê°€"""
        try:
            if not insights:
                return 0.0

            depth_score = 0.0
            total_insights = len(insights)

            for insight in insights:
                content = insight.get("content", "")
                if isinstance(content, str):
                    # ë¶„ì„ ê¹Šì´ ì§€í‘œë“¤
                    depth_indicators = [
                        "ë¶„ì„", "ì˜í–¥", "ì „ë§", "ì˜ˆìƒ", "í‰ê°€",
                        "ë¹„êµ", "ë³€í™”", "ì„±ì¥", "ìœ„í—˜", "ê¸°íšŒ"
                    ]

                    indicator_count = sum(1 for indicator in depth_indicators if indicator in content)

                    # ë¬¸ì¥ ê¸¸ì´ë„ ê³ ë ¤ (ë” ê¸´ ì„¤ëª… = ë” ê¹Šì€ ë¶„ì„)
                    sentence_length_score = min(len(content) / 200, 1.0)

                    insight_depth = (indicator_count / len(depth_indicators)) * 0.7 + sentence_length_score * 0.3
                    depth_score += insight_depth

            return min(depth_score / total_insights, 1.0)

        except Exception:
            return 0.5