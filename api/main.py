# src/ontolog_chat/main.py
import json
import sys
sys.path.insert(0, '/home/nuri/.local/lib/python3.10/site-packages')
from fastapi import Body, Depends, FastAPI, HTTPException
from fastapi.encoders import jsonable_encoder
from fastapi_mcp import FastApiMCP
from api.logging import setup_logging
from api.config import settings
from api.routers import health, cache_router, metrics_router
from api.routers.monitoring_router import router as monitoring_router
from api.monitoring.middleware import PrometheusMiddleware, HealthMonitoringMiddleware
from api.services.chat_service import ChatService
from api.services.context_cache import context_cache
from api.services.report_service import (
    ReportRequest,
    ReportResponse,
    ReportService,
)
from api.services.langgraph_report_service import LangGraphReportEngine
from api.services.stock_data_service import stock_data_service

# Langfuse íŠ¸ë ˆì´ì‹±
try:
    from api.utils.langfuse_tracer import trace_llm
    LANGFUSE_AVAILABLE = True
except ImportError:
    LANGFUSE_AVAILABLE = False
    def trace_llm(*args, **kwargs):
        def decorator(func):
            return func
        return decorator
from api.mcp import router as mcp_router  # â† ì¶”ê°€
from pydantic import BaseModel, Field
from typing import List, Optional
import numpy as np

# Custom JSON encoder to handle numpy types
def custom_jsonable_encoder(obj):
    """Custom JSON encoder that handles numpy types."""
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    else:
        return jsonable_encoder(obj)

logger = setup_logging()
app = FastAPI(title="ontolog_chat", version="0.1.0")

# FastAPI-MCP í†µí•©
mcp = FastApiMCP(
    fastapi=app,
    name="ontology-chat",
    description="Ontology Chat API with MCP support"
)

# MCP ì—”ë“œí¬ì¸íŠ¸ ë§ˆìš´íŠ¸
mcp.mount_sse(app, mount_path="/mcp/sse")
mcp.mount_http(app, mount_path="/mcp/http")

# Add monitoring middleware
app.add_middleware(PrometheusMiddleware)
app.add_middleware(HealthMonitoringMiddleware)

app.include_router(health.router)
app.include_router(mcp_router.router)  # â† ì¶”ê°€
app.include_router(cache_router.router)  # ìºì‹œ ê´€ë¦¬ ë¼ìš°í„°
app.include_router(metrics_router.router)  # ëª¨ë‹ˆí„°ë§ ë©”íŠ¸ë¦­ ë¼ìš°í„°
app.include_router(monitoring_router)  # ì§ˆì˜-ì‘ë‹µ íŠ¸ë ˆì´ì‹± ë¼ìš°í„°

# ë¶„ì„ ëŒ€ì‹œë³´ë“œ ë¼ìš°í„° ì¶”ê°€
from api.routers import analytics_router
app.include_router(analytics_router.router)  # ì„±ëŠ¥ ë¶„ì„ ëŒ€ì‹œë³´ë“œ ë¼ìš°í„°

# ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ì´ˆê¸°í™”
chat_service = ChatService()
report_service = ReportService()
langgraph_engine = LangGraphReportEngine()

# MCP ë¼ìš°í„°ì— ì„œë¹„ìŠ¤ ì£¼ì… (MCP ì—”ë“œí¬ì¸íŠ¸ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡)
mcp_router.set_services(chat_service, report_service, langgraph_engine)

# ìƒˆë¡œìš´ ì „ë§ ë¦¬í¬íŠ¸ ìš”ì²­ ëª¨ë¸
class ForecastReportRequest(BaseModel):
    query: str
    keywords: List[str]
    companies: List[str]
    lookback_days: int = 30
    include_news: bool = True
    include_ontology: bool = True
    include_financial: bool = True
    report_mode: str = "í…Œë§ˆë³„ ë¶„ì„"


def get_report_service() -> ReportService:
    return report_service

def get_langgraph_engine() -> LangGraphReportEngine:
    return langgraph_engine


@app.post("/chat")
async def chat(
    query: str = Body(..., embed=True),
    user_id: str = Body("anonymous", embed=True),
    session_id: Optional[str] = Body(None, embed=True),
    force_deep_analysis: bool = Body(False, embed=True)
):
    """
    í•˜ì´ë¸Œë¦¬ë“œ ë¼ìš°íŒ… ì±—ë´‡ (ë‹¨ìˆœ ì§ˆë¬¸: ë¹ ë¥¸ ì‘ë‹µ / ë³µì¡í•œ ì§ˆë¬¸: Multi-Agent LangGraph)

    Args:
        query: ì‚¬ìš©ì ì§ˆë¬¸
        user_id: ì‚¬ìš©ì ID (ì„ íƒì‚¬í•­)
        session_id: ì„¸ì…˜ ID (ì„ íƒì‚¬í•­)
        force_deep_analysis: ê°•ì œ ì‹¬ì¸µ ë¶„ì„ ëª¨ë“œ (LangGraph ì‚¬ìš©)

    Returns:
        ë‹µë³€ ë° ê´€ë ¨ ì •ë³´
        - ë‹¨ìˆœ ì§ˆë¬¸: 1.5ì´ˆ ì´ë‚´ ë¹ ë¥¸ ì‘ë‹µ
        - ë³µì¡í•œ ì§ˆë¬¸: Multi-Agent ë¶„ì„ (5ì´ˆ+, ê³ í’ˆì§ˆ ë³´ê³ ì„œ)
    """
    # í•˜ì´ë¸Œë¦¬ë“œ ë¼ìš°í„° ì‹œìŠ¤í…œ (LangGraph í†µí•©)
    from api.services.query_router import QueryRouter
    from api.services.response_formatter import ResponseFormatter

    # LangGraph ì—”ì§„ í¬í•¨í•œ ë¼ìš°í„° ìƒì„±
    router = QueryRouter(chat_service, ResponseFormatter(), langgraph_engine)
    result = await router.process_query(query, user_id, session_id, force_deep_analysis)

    # Convert numpy types to Python native types for JSON serialization
    def convert_numpy_types(obj):
        if isinstance(obj, dict):
            return {key: convert_numpy_types(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [convert_numpy_types(item) for item in obj]
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        else:
            return obj

    return convert_numpy_types(result)


@app.post("/report", response_model=ReportResponse)
async def create_report(
    req: ReportRequest, service: ReportService = Depends(get_report_service)
):
    """
    ë„ë©”ì¸ë³„ ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        req: ë¦¬í¬íŠ¸ ìš”ì²­ ì •ë³´ (query, domain, lookback_days ë“±)

    Returns:
        ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ë¦¬í¬íŠ¸ì™€ ë¶„ì„ ë©”íŠ¸ë¦­
    """
    try:
        out = await service.generate_report(
            query=req.query,
            domain=req.domain,
            lookback_days=req.lookback_days,
            news_size=req.news_size,
            graph_limit=req.graph_limit,
            symbol=req.symbol,
        )
        # metaëŠ” í´ë¼ì´ì–¸íŠ¸ì—ì„œ ì“¸ ìˆ˜ ìˆë„ë¡ ì¡°ê¸ˆ í¬í•¨í•´ ì¤Œ (ReportContextì˜ ë©”íƒ€ í¬í•¨)
        context_meta = out.get("ctx", {}).meta if hasattr(out.get("ctx", {}), "meta") else {}
        meta = {
            "query": req.query,
            "domain": req.domain,
            "lookback_days": req.lookback_days,
            "news_size": req.news_size,
            "graph_limit": req.graph_limit,
            "symbol": req.symbol,
            "search_time": 0.0,  # ê¸°ë³¸ê°’
            "confidence": 85.0,  # ê¸°ë³¸ ì‹ ë¢°ë„
            "coverage": 75.0,   # ê¸°ë³¸ ì™„ì„±ë„
            # ReportContext ë©”íƒ€ë°ì´í„° ì¶”ê°€
            **context_meta
        }

        # sources ì •ë³´ë¥¼ ì¶”ê°€ (Enhanced Chat í˜¸í™˜)
        sources = []
        if "ctx" in out and hasattr(out["ctx"], "news_hits"):
            for hit in out["ctx"].news_hits[:10]:
                source = hit.get("_source", {})
                sources.append({
                    "title": source.get("title", "ì œëª© ì—†ìŒ"),
                    "url": source.get("url", ""),
                    "date": source.get("created_datetime", source.get("created_date", "")),
                    "score": hit.get("_score", 0),
                    "type": "ë‰´ìŠ¤"
                })

        return {
            "markdown": out["markdown"],
            "metrics": out["metrics"],
            "meta": meta,
            "sources": sources,  # UIì—ì„œ ì‚¬ìš©í•  ì†ŒìŠ¤ ì •ë³´
            "sections": {
                "analysis": out["markdown"],
                "graph": out["metrics"].get("graph", {}),
                "news": out["metrics"].get("news", {}),
                "stock": out["metrics"].get("stock", {})
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# ========== ë¦¬í¬íŠ¸ ì „ìš© ê³ ê¸‰ ê¸°ëŠ¥ APIë“¤ ==========

@app.post("/report/comparative")
@trace_llm("comparative_report")
async def create_comparative_report(
    queries: list[str] = Body(...),
    domain: str = Body(None),
    lookback_days: int = Body(180),
    service: ReportService = Depends(get_report_service)
):
    """
    ë¹„êµ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± - ì—¬ëŸ¬ í‚¤ì›Œë“œë¥¼ ë™ì‹œì— ë¹„êµ ë¶„ì„

    Args:
        queries: ë¹„êµí•  ì§ˆì˜ë“¤ (2-5ê°œ)
        domain: ë„ë©”ì¸ (optional)
        lookback_days: ë¶„ì„ ê¸°ê°„ (ì¼)

    Returns:
        ë¹„êµ ë¶„ì„ ê²°ê³¼
    """
    try:
        if len(queries) < 2:
            raise HTTPException(status_code=400, detail="ë¹„êµë¥¼ ìœ„í•´ ìµœì†Œ 2ê°œ ì´ìƒì˜ ì§ˆì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤")
        if len(queries) > 5:
            raise HTTPException(status_code=400, detail="í•œ ë²ˆì— ìµœëŒ€ 5ê°œê¹Œì§€ë§Œ ë¹„êµ ê°€ëŠ¥í•©ë‹ˆë‹¤")

        result = await service.generate_comparative_report(
            queries=queries,
            domain=domain,
            lookback_days=lookback_days
        )

        return {
            "markdown": result["markdown"],
            "comparisons": result["comparisons"],
            "type": "comparative",
            "meta": {
                "queries": queries,
                "domain": domain,
                "lookback_days": lookback_days,
                "comparison_count": len(queries)
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/report/trend")
async def create_trend_report(
    query: str = Body(...),
    domain: str = Body(None),
    periods: list[int] = Body([30, 90, 180]),
    service: ReportService = Depends(get_report_service)
):
    """ì‹œê³„ì—´ íŠ¸ë Œë“œ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± - ì‹œê°„ ê²½ê³¼ì— ë”°ë¥¸ ë³€í™” ì¶”ì´ ë¶„ì„"""
    try:
        if len(periods) < 2:
            raise HTTPException(status_code=400, detail="íŠ¸ë Œë“œ ë¶„ì„ì„ ìœ„í•´ ìµœì†Œ 2ê°œ ì´ìƒì˜ ê¸°ê°„ì´ í•„ìš”í•©ë‹ˆë‹¤")

        # ê¸°ê°„ ì •ë ¬ (ì§§ì€ ìˆœì„œë¶€í„°)
        periods = sorted(periods)

        result = await service.generate_trend_analysis(
            query=query,
            domain=domain,
            periods=periods
        )

        return {
            "markdown": result["markdown"],
            "trend_data": result["trend_data"],
            "type": "trend_analysis",
            "meta": {
                "query": query,
                "domain": domain,
                "periods": periods,
                "analysis_points": len(periods)
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.on_event("startup")
async def startup_event():
    """ì•± ì‹œì‘ì‹œ ìºì‹œ ì •ë¦¬ ì‘ì—… ì‹œì‘"""
    await context_cache.start_cleanup_task()

@app.on_event("shutdown")
async def shutdown_event():
    """ì•± ì¢…ë£Œì‹œ ìºì‹œ ì •ë¦¬ ì‘ì—… ì¤‘ì§€"""
    await context_cache.stop_cleanup_task()

@app.post("/report/executive")
async def create_executive_summary(
    req: ReportRequest, service: ReportService = Depends(get_report_service)
):
    """ê²½ì˜ì§„ìš© ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„± - í•µì‹¬ ì •ë³´ë§Œ ê°„ê²°í•˜ê²Œ"""
    try:
        # ì¼ë°˜ ë¦¬í¬íŠ¸ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        ctx = await service.fetch_context(
            query=req.query,
            domain=req.domain,
            lookback_days=req.lookback_days,
            news_size=req.news_size,
            graph_limit=req.graph_limit,
            symbol=req.symbol,
        )

        # ê²½ì˜ì§„ ìš”ì•½ ìƒì„±
        executive_md = service.generate_executive_summary(ctx)

        # í•µì‹¬ ë©”íŠ¸ë¦­
        graph_metrics = service.compute_graph_metrics(ctx.graph_rows)
        news_metrics = service.compute_news_metrics(ctx.news_hits)

        return {
            "markdown": executive_md,
            "type": "executive_summary",
            "key_metrics": {
                "contract_total": graph_metrics["contract_total_amount"],
                "news_count": news_metrics["count"],
                "entities_count": len(ctx.graph_rows),
                "top_companies": graph_metrics["companies_top"][:3]
            },
            "meta": {
                "query": req.query,
                "domain": req.domain,
                "lookback_days": req.lookback_days,
                "reading_time": "1ë¶„"
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# ========== LangGraph ê¸°ë°˜ ê³ ê¸‰ ì»¨í…ìŠ¤íŠ¸ ì—”ì§€ë‹ˆì–´ë§ API ==========

class LangGraphReportRequest(BaseModel):
    """LangGraph ë¦¬í¬íŠ¸ ìš”ì²­ ëª¨ë¸ (í†µí•© ë° ë‹¨ìˆœí™”)"""
    query: str = Field(..., description="ë¶„ì„ ì§ˆì˜")
    domain: Optional[str] = Field(None, description="ë„ë©”ì¸ í‚¤ì›Œë“œ (ì„ íƒ)")
    lookback_days: int = Field(180, ge=1, le=720, description="ë¶„ì„ ê¸°ê°„ (ì¼)")
    analysis_depth: str = Field("standard", description="ë¶„ì„ ê¹Šì´: shallow, standard, deep, comprehensive")
    symbol: Optional[str] = Field(None, description="ì£¼ê°€ ì‹¬ë³¼ (ì„ íƒ)")

@app.post("/report/langgraph")
async def create_langgraph_report(
    req: LangGraphReportRequest,
    engine: LangGraphReportEngine = Depends(get_langgraph_engine)
):
    """
    LangGraph ê¸°ë°˜ ê³ ê¸‰ ì»¨í…ìŠ¤íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ë¦¬í¬íŠ¸ ìƒì„±

    **ì‚¬ìš© ì˜ˆì‹œ:**
    ```json
    {
      "query": "ì‚¼ì„±ì „ìì™€ SKí•˜ì´ë‹‰ìŠ¤ì˜ HBM ê²½ìŸë ¥ ë¹„êµ",
      "analysis_depth": "comprehensive",
      "lookback_days": 180
    }
    ```

    **ë¶„ì„ ê¹Šì´ ì˜µì…˜:**
    - `shallow`: ë¹ ë¥¸ ë¶„ì„ (1ë¶„, 4ë‹¨ê³„)
    - `standard`: í‘œì¤€ ë¶„ì„ (1.5ë¶„, 6ë‹¨ê³„)
    - `deep`: ì‹¬ì¸µ ë¶„ì„ (2ë¶„, 8ë‹¨ê³„)
    - `comprehensive`: ì¢…í•© ë¶„ì„ (3ë¶„, 10ë‹¨ê³„+)
    """
    try:
        if req.analysis_depth not in ["shallow", "standard", "deep", "comprehensive"]:
            raise HTTPException(status_code=400, detail="ë¶„ì„ ê¹Šì´ëŠ” shallow, standard, deep, comprehensive ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤")

        result = await engine.generate_langgraph_report(
            query=req.query,
            domain=req.domain,
            lookback_days=req.lookback_days,
            analysis_depth=req.analysis_depth,
            symbol=req.symbol
        )

        return result

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/report/langgraph/comparative")
@trace_llm("langgraph_comparative_report")
async def create_langgraph_comparative_report(
    queries: list[str] = Body(...),
    domain: str = Body(None),
    lookback_days: int = Body(180),
    analysis_depth: str = Body("standard"),
    engine: LangGraphReportEngine = Depends(get_langgraph_engine)
):
    """LangGraph ê¸°ë°˜ ë¹„êµ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± - ë‹¤ì¤‘ ì¿¼ë¦¬ ê³ ê¸‰ ë¶„ì„"""
    try:
        if len(queries) < 2:
            raise HTTPException(status_code=400, detail="ë¹„êµë¥¼ ìœ„í•´ ìµœì†Œ 2ê°œ ì´ìƒì˜ ì§ˆì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤")
        if len(queries) > 5:
            raise HTTPException(status_code=400, detail="í•œ ë²ˆì— ìµœëŒ€ 5ê°œê¹Œì§€ë§Œ ë¹„êµ ê°€ëŠ¥í•©ë‹ˆë‹¤")

        # ê° ì¿¼ë¦¬ë³„ë¡œ LangGraph ë¶„ì„ ìˆ˜í–‰
        results = []
        for query in queries:
            result = await engine.generate_langgraph_report(
                query=query,
                domain=domain,
                lookback_days=lookback_days,
                analysis_depth=analysis_depth
            )
            results.append({
                "query": query,
                "result": result
            })

        # ë¹„êµ ë¶„ì„ ìˆ˜í–‰
        comparative_prompt = f"""
        ë‹¤ìŒ {len(queries)}ê°œ í•­ëª©ì— ëŒ€í•œ LangGraph ë¶„ì„ ê²°ê³¼ë¥¼ ë¹„êµ ë¶„ì„í•´ì£¼ì„¸ìš”:

        ë¶„ì„ ê²°ê³¼ë“¤:
        {json.dumps([{"query": r["query"], "quality_score": r["result"].get("quality_score", 0), "insights_count": r["result"].get("insights_count", 0)} for r in results], ensure_ascii=False, indent=2)}

        ë‹¤ìŒ ê´€ì ì—ì„œ ì¢…í•© ë¹„êµ ë¶„ì„ì„ í•´ì£¼ì„¸ìš”:
        1. ê° í•­ëª©ì˜ ìƒëŒ€ì  ì¤‘ìš”ë„ ë° ì˜í–¥ë ¥
        2. í’ˆì§ˆ ì ìˆ˜ ê¸°ë°˜ ì‹ ë¢°ë„ ë¹„êµ
        3. ë°ì´í„° í’ë¶€ë„ ë° ë¶„ì„ ê¹Šì´ ë¹„êµ
        4. íˆ¬ì ë˜ëŠ” ë¹„ì¦ˆë‹ˆìŠ¤ ìš°ì„ ìˆœìœ„ ê¶Œì¥ì‚¬í•­

        ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì „ë¬¸ì ì¸ ë¹„êµ ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
        """

        from langchain_openai import ChatOpenAI
        from langchain_core.messages import HumanMessage

        llm = ChatOpenAI(model="gpt-4", temperature=0.1)
        comparative_response = await llm.ainvoke([HumanMessage(content=comparative_prompt)])

        return {
            "markdown": comparative_response.content,
            "individual_results": results,
            "type": "langgraph_comparative",
            "meta": {
                "queries": queries,
                "domain": domain,
                "lookback_days": lookback_days,
                "analysis_depth": analysis_depth,
                "comparison_count": len(queries),
                "total_processing_time": sum(r["result"].get("processing_time", 0) for r in results)
            }
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/report/langgraph/trend")
async def create_langgraph_trend_report(
    query: str = Body(...),
    domain: str = Body(None),
    periods: list[int] = Body([30, 90, 180]),
    analysis_depth: str = Body("standard"),
    engine: LangGraphReportEngine = Depends(get_langgraph_engine)
):
    """LangGraph ê¸°ë°˜ ì‹œê³„ì—´ íŠ¸ë Œë“œ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
    try:
        if len(periods) < 2:
            raise HTTPException(status_code=400, detail="íŠ¸ë Œë“œ ë¶„ì„ì„ ìœ„í•´ ìµœì†Œ 2ê°œ ì´ìƒì˜ ê¸°ê°„ì´ í•„ìš”í•©ë‹ˆë‹¤")

        # ê¸°ê°„ë³„ LangGraph ë¶„ì„ ìˆ˜í–‰
        period_results = []
        for period in sorted(periods):
            result = await engine.generate_langgraph_report(
                query=query,
                domain=domain,
                lookback_days=period,
                analysis_depth=analysis_depth
            )
            period_results.append({
                "period": period,
                "result": result
            })

        # íŠ¸ë Œë“œ ë¶„ì„ ìˆ˜í–‰
        trend_prompt = f"""
        ë‹¤ìŒ ê¸°ê°„ë³„ LangGraph ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹œê³„ì—´ íŠ¸ë Œë“œ ë¶„ì„ì„ í•´ì£¼ì„¸ìš”:

        ì§ˆì˜: {query}
        ë¶„ì„ ê¸°ê°„ë“¤: {periods}ì¼

        ê¸°ê°„ë³„ ë¶„ì„ ê²°ê³¼:
        {json.dumps([{"period": r["period"], "quality_score": r["result"].get("quality_score", 0), "contexts_count": r["result"].get("contexts_count", 0)} for r in period_results], ensure_ascii=False, indent=2)}

        ë‹¤ìŒ ê´€ì ì—ì„œ íŠ¸ë Œë“œ ë¶„ì„ì„ í•´ì£¼ì„¸ìš”:
        1. ì‹œê°„ ê²½ê³¼ì— ë”°ë¥¸ ë°ì´í„° í’ˆì§ˆ ë° ì–‘ì˜ ë³€í™”
        2. íŒ¨í„´ ë° ì£¼ê¸°ì„± ë¶„ì„
        3. ìµœê·¼ íŠ¸ë Œë“œì˜ ê°€ì†ë„ ë˜ëŠ” ê°ì†Œ ì¶”ì„¸
        4. í–¥í›„ ì „ë§ ë° ì˜ˆì¸¡ ê°€ëŠ¥í•œ ë³€í™”

        ì‹œê°ì ìœ¼ë¡œ ì´í•´í•˜ê¸° ì‰¬ìš´ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ íŠ¸ë Œë“œ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
        """

        from langchain_openai import ChatOpenAI
        from langchain_core.messages import HumanMessage

        llm = ChatOpenAI(model="gpt-4", temperature=0.1)
        trend_response = await llm.ainvoke([HumanMessage(content=trend_prompt)])

        return {
            "markdown": trend_response.content,
            "period_results": period_results,
            "type": "langgraph_trend",
            "meta": {
                "query": query,
                "domain": domain,
                "periods": periods,
                "analysis_depth": analysis_depth,
                "analysis_points": len(periods),
                "total_processing_time": sum(r["result"].get("processing_time", 0) for r in period_results)
            }
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/forecast_report")
async def create_forecast_report(req: ForecastReportRequest):
    """í…Œë§ˆ/ì¢…ëª©ë³„ ì „ë§ ë¦¬í¬íŠ¸ ìƒì„±"""
    try:
        import time
        from datetime import datetime

        logger.info(f"ì „ë§ ë¦¬í¬íŠ¸ ìƒì„± ìš”ì²­: {req.query} (ëª¨ë“œ: {req.report_mode})")

        # 1. ë‰´ìŠ¤ ìˆ˜ì§‘
        news_data = []
        if req.include_news:
            for keyword in req.keywords[:3]:  # ìƒìœ„ 3ê°œ í‚¤ì›Œë“œë§Œ ì‚¬ìš©
                try:
                    hits, _, _ = await chat_service._search_news_simple_hybrid(keyword, size=10)
                    news_data.extend(hits)
                except Exception as e:
                    logger.warning(f"ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨ ({keyword}): {e}")

        # ì¤‘ë³µ ì œê±° ë° ìµœì‹ ìˆœ ì •ë ¬
        seen_urls = set()
        unique_news = []
        for news in news_data:
            url = news.get("url", "")
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_news.append(news)

        # 2. ì˜¨í†¨ë¡œì§€ ê·¸ë˜í”„ ë°ì´í„° ìˆ˜ì§‘
        graph_data = []
        if req.include_ontology:
            try:
                # íšŒì‚¬ëª…ë“¤ë¡œ ê·¸ë˜í”„ ê²€ìƒ‰
                for company in req.companies[:3]:  # ìƒìœ„ 3ê°œ íšŒì‚¬ë§Œ
                    rows, _ = await chat_service._graph(company)
                    graph_data.extend(rows)
            except Exception as e:
                logger.warning(f"ê·¸ë˜í”„ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

        # 3. LLMì„ í†µí•œ ì „ë§ ë¦¬í¬íŠ¸ ìƒì„±
        try:
            report_content = await generate_forecast_report_content(
                query=req.query,
                news_data=unique_news[:15],  # ìµœëŒ€ 15ê°œ ë‰´ìŠ¤
                graph_data=graph_data,
                companies=req.companies,
                keywords=req.keywords,
                report_mode=req.report_mode
            )
        except Exception as e:
            logger.error(f"ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            # í´ë°± ë¦¬í¬íŠ¸
            report_content = generate_fallback_report(req.query, req.companies, unique_news[:5])

        return {
            "query": req.query,
            "report_mode": req.report_mode,
            "generated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "companies": req.companies,
            "keywords": req.keywords,
            "executive_summary": report_content.get("executive_summary", ""),
            "news_analysis": report_content.get("news_analysis", ""),
            "ontology_insights": report_content.get("ontology_insights", ""),
            "financial_outlook": report_content.get("financial_outlook", ""),
            "conclusion": report_content.get("conclusion", ""),
            "sources": unique_news[:10],  # ìƒìœ„ 10ê°œ ì†ŒìŠ¤
            "data_quality": {
                "news_count": len(unique_news),
                "graph_entities": len(graph_data),
                "analysis_period": f"{req.lookback_days}ì¼"
            }
        }

    except Exception as e:
        logger.error(f"ì „ë§ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {str(e)}")


async def generate_forecast_report_content(query: str, news_data: list, graph_data: list,
                                         companies: list, keywords: list, report_mode: str) -> dict:
    """LLMì„ í†µí•œ ì „ë§ ë¦¬í¬íŠ¸ ë‚´ìš© ìƒì„±"""

    # ë‰´ìŠ¤ ìš”ì•½
    news_summary = []
    for news in news_data[:10]:
        title = news.get("title", "")
        date = news.get("date", "")
        if title:
            news_summary.append(f"â€¢ {title} ({date})")

    # íšŒì‚¬/í…Œë§ˆ ì •ë³´
    if report_mode == "ğŸ¯ í…Œë§ˆë³„ ë¶„ì„":
        subject = f"{keywords[0]} í…Œë§ˆ"
        analysis_scope = f"ê´€ë ¨ ì£¼ìš” ê¸°ì—…: {', '.join(companies[:5])}"
    else:
        subject = companies[0] if companies else "ëŒ€ìƒ ê¸°ì—…"
        analysis_scope = f"ë¶„ì„ ëŒ€ìƒ: {subject}"

    # êµ¬ì¡°í™”ëœ ë¦¬í¬íŠ¸ ìƒì„±
    return {
        "executive_summary": f"""
**{subject} ì „ë§ ìš”ì•½**

{analysis_scope}

ìµœê·¼ {len(news_data)}ê±´ì˜ ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ë¶„ì„í•œ ê²°ê³¼, {subject} ì„¹í„°ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì£¼ìš” ë™í–¥ì„ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤:

â€¢ ìµœì‹  ë‰´ìŠ¤ ë™í–¥: {len([n for n in news_data if '2024' in str(n.get('date', '')) or '2025' in str(n.get('date', ''))])}ê±´ì˜ ìµœì‹  ì†Œì‹
â€¢ ì£¼ìš” í‚¤ì›Œë“œ: {', '.join(keywords[:3])}
â€¢ ë¶„ì„ ê¸°ì—… ìˆ˜: {len(companies)}ê°œì‚¬
        """.strip(),

        "news_analysis": f"""
**ì£¼ìš” ë‰´ìŠ¤ ë¶„ì„**

ìµœê·¼ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ì¤‘ ì£¼ìš” ë‚´ìš©:

{chr(10).join(news_summary[:8])}

**ë¶„ì„ ê²°ê³¼:**
- ì´ {len(news_data)}ê±´ì˜ ê´€ë ¨ ë‰´ìŠ¤ê°€ í™•ì¸ë¨
- ì£¼ìš” ê´€ì‹¬ì‚¬: {', '.join(keywords[:3])}
- ë‰´ìŠ¤ í™œë™ë„: {'ë†’ìŒ' if len(news_data) > 10 else 'ë³´í†µ' if len(news_data) > 5 else 'ë‚®ìŒ'}
        """.strip(),

        "ontology_insights": f"""
**ê´€ê³„ ë¶„ì„ ë° ì¸ì‚¬ì´íŠ¸**

ì˜¨í†¨ë¡œì§€ ê·¸ë˜í”„ ë¶„ì„ ê²°ê³¼:
- ìˆ˜ì§‘ëœ ì—”í‹°í‹°: {len(graph_data)}ê°œ
- ì£¼ìš” ê´€ê³„: ê¸°ì—…ê°„ ì—°ê´€ì„±, ì‚¬ì—… ì˜ì—­ ì¤‘ë³µë„
- ìƒíƒœê³„ ë¶„ì„: {subject} ê´€ë ¨ ê¸°ì—…ë“¤ì˜ ìƒí˜¸ ì—°ê´€ì„± í™•ì¸

**ì£¼ìš” ë°œê²¬ì‚¬í•­:**
- {companies[0] if companies else 'ë¶„ì„ ëŒ€ìƒ'} ì¤‘ì‹¬ì˜ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°
- ê´€ë ¨ ì‚°ì—… ê°„ ì—°ê²°ì„± ë¶„ì„
        """.strip(),

        "financial_outlook": f"""
**ì¬ë¬´ ì „ë§**

{subject}ì˜ ì¬ë¬´ì  ê´€ì  ë¶„ì„:

**ê¸°íšŒ ìš”ì¸:**
- ê´€ë ¨ ë‰´ìŠ¤ í™œë™ë„ê°€ {'ë†’ì•„' if len(news_data) > 10 else 'ì ì • ìˆ˜ì¤€ì´ì–´ì„œ'} ì‹œì¥ ê´€ì‹¬ë„ ìƒìŠ¹ ê¸°ëŒ€
- {keywords[0]} ê´€ë ¨ ì‚°ì—… ë™í–¥ ê¸ì •ì 

**ë¦¬ìŠ¤í¬ ìš”ì¸:**
- ì‹œì¥ ë³€ë™ì„±ì— ë”°ë¥¸ ë¶ˆí™•ì‹¤ì„±
- ê±°ì‹œê²½ì œ í™˜ê²½ ë³€í™” ì˜í–¥

**íˆ¬ì í¬ì¸íŠ¸:**
- ë‹¨ê¸°: ë‰´ìŠ¤ í”Œë¡œìš° ë° ì´ë²¤íŠ¸ ì¤‘ì‹¬ ì ‘ê·¼
- ì¤‘ê¸°: ì‚¬ì—… êµ¬ì¡° ë° ì‹¤ì  ê°œì„  ì—¬ë¶€ í™•ì¸ í•„ìš”
        """.strip(),

        "conclusion": f"""
**íˆ¬ì ì „ë§ ë° ê²°ë¡ **

**ì¢…í•© í‰ê°€:** {subject}

1. **ê¸ì •ì  ìš”ì¸**
   - ê´€ë ¨ ë‰´ìŠ¤ ë°œìƒëŸ‰: {len(news_data)}ê±´ (ê´€ì‹¬ë„ {'ë†’ìŒ' if len(news_data) > 10 else 'ë³´í†µ'})
   - ì‚°ì—… ë‚´ ì—°ê´€ì„±: {len(graph_data)}ê°œ ì—”í‹°í‹°ë¡œ í™•ì¸ëœ ìƒíƒœê³„

2. **ì£¼ì˜ì‚¬í•­**
   - ë‰´ìŠ¤ ê¸°ë°˜ ë‹¨ê¸° ë³€ë™ì„± ê°€ëŠ¥ì„±
   - í€ë”ë©˜í„¸ ë¶„ì„ ë³‘í–‰ í•„ìš”

3. **íˆ¬ì ì˜ê²¬**
   - ê´€ì‹¬ ì¢…ëª©: {', '.join(companies[:3])}
   - ëª¨ë‹ˆí„°ë§ í‚¤ì›Œë“œ: {', '.join(keywords[:3])}

**ë©´ì±… ì¡°í•­:** ë³¸ ë¦¬í¬íŠ¸ëŠ” ê³µê°œ ì •ë³´ ê¸°ë°˜ ë¶„ì„ì´ë©°, íˆ¬ì ê¶Œìœ ê°€ ì•„ë‹™ë‹ˆë‹¤.
        """.strip()
    }


def generate_fallback_report(query: str, companies: list, news_data: list) -> dict:
    """ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨ì‹œ í´ë°± ë¦¬í¬íŠ¸"""
    return {
        "executive_summary": f"**{query}** ê´€ë ¨ ê¸°ë³¸ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤. ì œí•œëœ ë°ì´í„°ë¡œ ì¸í•´ ê°„ëµí•œ ì •ë³´ë§Œ ì œê³µë©ë‹ˆë‹¤.",
        "news_analysis": f"ìˆ˜ì§‘ëœ ë‰´ìŠ¤: {len(news_data)}ê±´. ì¶”ê°€ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” Enhanced Chatì„ ì´ìš©í•´ì£¼ì„¸ìš”.",
        "ontology_insights": "ì˜¨í†¨ë¡œì§€ ë°ì´í„° ìˆ˜ì§‘ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
        "financial_outlook": "ì¬ë¬´ ì •ë³´ ìˆ˜ì§‘ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
        "conclusion": "ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ ì¸í•´ ìƒì„¸í•œ ë¶„ì„ì´ ì œí•œë©ë‹ˆë‹¤. Enhanced Chatì—ì„œ ê°œë³„ ì§ˆì˜ë¥¼ ì‹œë„í•´ë³´ì„¸ìš”."
    }


@app.get("/")
async def root():
    return {"name": "ontolog_chat", "env": settings.app_env}

@app.get("/api/themes")
async def get_market_themes():
    """
    ì‹œì¥ ì£¼ìš” í…Œë§ˆ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤.

    Returns:
        í…Œë§ˆë³„ ì¢…ëª© ì •ë³´ ë° ì„±ê³¼
    """
    try:
        themes = await stock_data_service.get_market_themes()
        return {
            "success": True,
            "themes": [
                {
                    "name": theme.theme_name,
                    "description": theme.description,
                    "stocks": [
                        {
                            "name": stock.name,
                            "symbol": stock.symbol.replace('.KS', ''),
                            "sector": stock.sector,
                            "price": stock.price,
                            "change_percent": stock.change_percent
                        }
                        for stock in theme.stocks
                    ],
                    "performance": theme.performance
                }
                for theme in themes
            ]
        }
    except Exception as e:
        logger.error(f"í…Œë§ˆ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {"success": False, "error": str(e)}

@app.get("/api/stocks/search")
async def search_stocks(query: str, limit: int = 10):
    """
    ì¢…ëª©ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.

    Args:
        query: ê²€ìƒ‰ì–´
        limit: ê²°ê³¼ ìˆ˜ ì œí•œ (ê¸°ë³¸ê°’: 10)

    Returns:
        ì¢…ëª© ëª©ë¡ (ì´ë¦„, ì‹¬ë³¼, ì„¹í„°, ê°€ê²© ë“±)
    """
    try:
        stocks = await stock_data_service.search_stocks_by_query(query, limit)
        return {
            "success": True,
            "stocks": [
                {
                    "name": stock.name,
                    "symbol": stock.symbol.replace('.KS', ''),
                    "sector": stock.sector,
                    "industry": stock.industry,
                    "price": stock.price,
                    "change_percent": stock.change_percent,
                    "market_cap": stock.market_cap,
                    "volume": stock.volume
                }
                for stock in stocks
            ]
        }
    except Exception as e:
        logger.error(f"ì¢…ëª© ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return {"success": False, "error": str(e)}

@app.get("/api/stocks/theme/{theme}")
async def get_theme_stocks(theme: str):
    """í…Œë§ˆë³„ ì¢…ëª© ì¡°íšŒ"""
    try:
        stocks = await stock_data_service.get_theme_stocks(theme)
        return {
            "success": True,
            "theme": theme,
            "stocks": [
                {
                    "name": stock.name,
                    "symbol": stock.symbol.replace('.KS', ''),
                    "sector": stock.sector,
                    "industry": stock.industry,
                    "price": stock.price,
                    "change_percent": stock.change_percent,
                    "market_cap": stock.market_cap
                }
                for stock in stocks
            ]
        }
    except Exception as e:
        logger.error(f"í…Œë§ˆ ì¢…ëª© ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {"success": False, "error": str(e)}

@app.get("/api/stocks/top")
async def get_top_stocks(theme: Optional[str] = None):
    """ìƒìŠ¹ë¥  ê¸°ì¤€ ìƒìœ„ ì¢…ëª©"""
    try:
        stocks = await stock_data_service.get_top_performing_stocks(theme)
        return {
            "success": True,
            "theme": theme,
            "stocks": [
                {
                    "name": stock.name,
                    "symbol": stock.symbol.replace('.KS', ''),
                    "sector": stock.sector,
                    "price": stock.price,
                    "change_percent": stock.change_percent,
                    "market_cap": stock.market_cap
                }
                for stock in stocks
            ]
        }
    except Exception as e:
        logger.error(f"ìƒìœ„ ì¢…ëª© ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {"success": False, "error": str(e)}
