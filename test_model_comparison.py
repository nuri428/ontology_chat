#!/usr/bin/env python3
"""
ì„¤ì¹˜ëœ Ollama ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸

ëª©í‘œ:
1. ì†ë„: í† í° ìƒì„± ì†ë„ ì¸¡ì •
2. í’ˆì§ˆ: í•œêµ­ì–´ ê¸ˆìœµ ë¶„ì„ í’ˆì§ˆ í‰ê°€
3. ìµœì  ëª¨ë¸ ì„ ì •
"""

import asyncio
import time
import httpx
import json
from datetime import datetime


# í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ ëª©ë¡ (í¬ê¸°ì™€ ì„±ëŠ¥ ê³ ë ¤)
TEST_MODELS = [
    # í˜„ì¬ ì‚¬ìš© ì¤‘
    {"name": "llama3.1:8b", "size": "4.9GB", "priority": "baseline"},

    # Qwen ì‹œë¦¬ì¦ˆ (í•œêµ­ì–´ ê°•ì )
    {"name": "qwen3:8b", "size": "5.2GB", "priority": "high"},
    {"name": "qwen3:8b-q8_0", "size": "8.9GB", "priority": "high"},
    {"name": "qwen2.5:14b", "size": "9.0GB", "priority": "medium"},
    {"name": "qwen2:7b-instruct-q4_K_M", "size": "4.7GB", "priority": "medium"},

    # ê¸°íƒ€ ê°•ë ¥í•œ ëª¨ë¸
    {"name": "deepseek-r1:14b", "size": "9.0GB", "priority": "medium"},
    {"name": "mistral-nemo:latest", "size": "7.1GB", "priority": "low"},
]

# í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ (ì‹¤ì œ ì‚¬ìš© ì¼€ì´ìŠ¤)
TEST_PROMPTS = [
    {
        "name": "ê°„ë‹¨í•œ ë¶„ì„",
        "prompt": """ê¸ˆìœµ ì• ë„ë¦¬ìŠ¤íŠ¸ë¡œì„œ ë‹¤ìŒ ì§ˆì˜ì— ë‹µë³€í•˜ì„¸ìš”.

**ì§ˆì˜**: ì‚¼ì„±ì „ì HBM ê²½ìŸë ¥

ê°„ë‹¨íˆ 3-4 ë¬¸ì¥ìœ¼ë¡œ í•µì‹¬ë§Œ ë‹µë³€í•˜ì„¸ìš”.""",
        "expected_tokens": 150
    },
    {
        "name": "ì¢…í•© ë¶„ì„",
        "prompt": """ê¸ˆìœµ ì• ë„ë¦¬ìŠ¤íŠ¸ë¡œì„œ ë‹¤ìŒ ì§ˆì˜ì— ëŒ€í•œ íˆ¬ì ë¶„ì„ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.

**ì§ˆì˜**: ì‚¼ì„±ì „ìì™€ SKí•˜ì´ë‹‰ìŠ¤ì˜ HBM ê²½ìŸë ¥ ë¹„êµ
**ë°ì´í„°**:
- ì‚¼ì„±ì „ì: HBM3E 16ë‹¨ ê°œë°œ ì¤‘, í•˜ì´ë¸Œë¦¬ë“œ ë³¸ë”© ê¸°ìˆ  ë„ì… ê³„íš
- SKí•˜ì´ë‹‰ìŠ¤: HBM3E ì´ë¯¸ ì–‘ì‚° ì¤‘, ì—”ë¹„ë””ì•„ ì£¼ìš” ê³µê¸‰ì—…ì²´

ë‹¤ìŒ êµ¬ì¡°ë¡œ Markdown ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš” (800ì ì´ë‚´):

# Executive Summary
- í•µì‹¬ ë°œê²¬ì‚¬í•­ 3ê°œ (bullet points)

# Market Analysis
- ì‹œì¥ ìƒí™© (100-150ì)

# Key Insights
- ê¸°ìˆ  ê²½ìŸë ¥ ë¹„êµ
- ì‹œì¥ ì ìœ ìœ¨ ë¶„ì„

# Investment Perspective
- íˆ¬ì ê´€ì  ê¶Œì¥ì‚¬í•­

ë°”ë¡œ ì‹œì‘:""",
        "expected_tokens": 600
    }
]


async def test_model_speed(model_name: str, prompt: str, ollama_url: str = "http://192.168.0.11:11434") -> dict:
    """ëª¨ë¸ ì†ë„ í…ŒìŠ¤íŠ¸"""

    print(f"  [Testing] {model_name}...")

    start_time = time.time()

    try:
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                f"{ollama_url}/api/generate",
                json={
                    "model": model_name,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.7,
                        "num_predict": 800  # ìµœëŒ€ í† í° ìˆ˜
                    }
                }
            )

            elapsed = time.time() - start_time

            if response.status_code == 200:
                data = response.json()
                response_text = data.get("response", "")

                # í†µê³„ ì¶”ì¶œ
                eval_count = data.get("eval_count", 0)  # ìƒì„±ëœ í† í° ìˆ˜
                eval_duration = data.get("eval_duration", 0) / 1e9  # ë‚˜ë…¸ì´ˆ â†’ ì´ˆ

                tokens_per_sec = eval_count / eval_duration if eval_duration > 0 else 0

                return {
                    "success": True,
                    "total_time": elapsed,
                    "response_text": response_text,
                    "response_length": len(response_text),
                    "tokens_generated": eval_count,
                    "generation_time": eval_duration,
                    "tokens_per_sec": tokens_per_sec,
                    "error": None
                }
            else:
                return {
                    "success": False,
                    "error": f"HTTP {response.status_code}: {response.text[:200]}"
                }

    except Exception as e:
        elapsed = time.time() - start_time
        return {
            "success": False,
            "total_time": elapsed,
            "error": str(e)
        }


def evaluate_response_quality(response_text: str, prompt_type: str) -> dict:
    """ì‘ë‹µ í’ˆì§ˆ í‰ê°€ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)"""

    scores = {}

    # 1. ê¸¸ì´ ì ì ˆì„±
    length = len(response_text)
    if prompt_type == "ê°„ë‹¨í•œ ë¶„ì„":
        # 150-400ì ì ì ˆ
        if 150 <= length <= 400:
            scores["length"] = 1.0
        elif 100 <= length <= 600:
            scores["length"] = 0.7
        else:
            scores["length"] = 0.3
    else:  # ì¢…í•© ë¶„ì„
        # 600-1200ì ì ì ˆ
        if 600 <= length <= 1200:
            scores["length"] = 1.0
        elif 400 <= length <= 1500:
            scores["length"] = 0.7
        else:
            scores["length"] = 0.3

    # 2. í•œêµ­ì–´ í’ˆì§ˆ (ê°„ë‹¨í•œ ì²´í¬)
    korean_chars = sum(1 for c in response_text if 'ê°€' <= c <= 'í£')
    korean_ratio = korean_chars / len(response_text) if length > 0 else 0

    if korean_ratio > 0.3:  # í•œêµ­ì–´ 30% ì´ìƒ
        scores["korean_quality"] = 1.0
    elif korean_ratio > 0.1:
        scores["korean_quality"] = 0.5
    else:
        scores["korean_quality"] = 0.0

    # 3. êµ¬ì¡°í™” (Markdown ì‚¬ìš© ì—¬ë¶€)
    has_headers = "#" in response_text
    has_bullets = ("â€¢" in response_text or "-" in response_text or "*" in response_text)

    if has_headers and has_bullets:
        scores["structure"] = 1.0
    elif has_headers or has_bullets:
        scores["structure"] = 0.6
    else:
        scores["structure"] = 0.2

    # 4. ê¸ˆìœµ ìš©ì–´ ì‚¬ìš© (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì²´í¬)
    finance_keywords = ["íˆ¬ì", "ê²½ìŸë ¥", "ì‹œì¥", "ì„±ì¥", "ìˆ˜ìµ", "ë¦¬ìŠ¤í¬", "ì „ë§", "ë¶„ì„", "ê¸°ì—…", "ë§¤ì¶œ"]
    keyword_count = sum(1 for kw in finance_keywords if kw in response_text)
    scores["finance_terms"] = min(keyword_count / 5.0, 1.0)

    # 5. ì¢…í•© ì ìˆ˜
    overall = sum(scores.values()) / len(scores)
    scores["overall"] = overall

    return scores


async def compare_models():
    """ëª¨ë“  ëª¨ë¸ ë¹„êµ í…ŒìŠ¤íŠ¸"""

    print("=" * 80)
    print("ğŸš€ Ollama ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    print()

    results = {
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "models": {}
    }

    for model_info in TEST_MODELS:
        model_name = model_info["name"]
        model_size = model_info["size"]
        priority = model_info["priority"]

        print(f"\n{'='*80}")
        print(f"ğŸ“Š ëª¨ë¸: {model_name} ({model_size}) [Priority: {priority}]")
        print(f"{'='*80}")

        model_results = {
            "model_name": model_name,
            "size": model_size,
            "priority": priority,
            "tests": []
        }

        for test_prompt in TEST_PROMPTS:
            prompt_name = test_prompt["name"]
            prompt_text = test_prompt["prompt"]

            print(f"\n  ğŸ“ í…ŒìŠ¤íŠ¸: {prompt_name}")

            # ì†ë„ í…ŒìŠ¤íŠ¸
            speed_result = await test_model_speed(model_name, prompt_text)

            if speed_result["success"]:
                # í’ˆì§ˆ í‰ê°€
                quality_scores = evaluate_response_quality(
                    speed_result["response_text"],
                    prompt_name
                )

                test_result = {
                    "prompt_type": prompt_name,
                    "speed": {
                        "total_time": speed_result["total_time"],
                        "generation_time": speed_result["generation_time"],
                        "tokens_per_sec": speed_result["tokens_per_sec"],
                        "tokens_generated": speed_result["tokens_generated"]
                    },
                    "quality": quality_scores,
                    "response_preview": speed_result["response_text"][:300]
                }

                print(f"    âœ… ì„±ê³µ")
                print(f"       ì´ ì‹œê°„: {speed_result['total_time']:.2f}ì´ˆ")
                print(f"       í† í°/ì´ˆ: {speed_result['tokens_per_sec']:.1f} tokens/sec")
                print(f"       í’ˆì§ˆ ì ìˆ˜: {quality_scores['overall']:.2f}")
                print(f"       ì‘ë‹µ ê¸¸ì´: {speed_result['response_length']}ì")

            else:
                test_result = {
                    "prompt_type": prompt_name,
                    "error": speed_result["error"]
                }
                print(f"    âŒ ì‹¤íŒ¨: {speed_result['error']}")

            model_results["tests"].append(test_result)

            # ë‹¤ìŒ í…ŒìŠ¤íŠ¸ ì „ ì ì‹œ ëŒ€ê¸°
            await asyncio.sleep(2)

        # ëª¨ë¸ë³„ í‰ê·  ê³„ì‚°
        successful_tests = [t for t in model_results["tests"] if "error" not in t]
        if successful_tests:
            avg_speed = sum(t["speed"]["tokens_per_sec"] for t in successful_tests) / len(successful_tests)
            avg_quality = sum(t["quality"]["overall"] for t in successful_tests) / len(successful_tests)

            ì¢…í•©ë¶„ì„_test = next((t for t in successful_tests if t["prompt_type"] == "ì¢…í•© ë¶„ì„"), None)
            ì¢…í•©ë¶„ì„_time = ì¢…í•©ë¶„ì„_test["speed"]["total_time"] if ì¢…í•©ë¶„ì„_test else 0

            model_results["summary"] = {
                "avg_tokens_per_sec": avg_speed,
                "avg_quality_score": avg_quality,
                "comprehensive_analysis_time": ì¢…í•©ë¶„ì„_time,
                "score": avg_speed * 0.4 + avg_quality * 60  # ì¢…í•© ì ìˆ˜ (ì†ë„ 40%, í’ˆì§ˆ 60%)
            }

            print(f"\n  ğŸ“ˆ ìš”ì•½:")
            print(f"     í‰ê·  ì†ë„: {avg_speed:.1f} tokens/sec")
            print(f"     í‰ê·  í’ˆì§ˆ: {avg_quality:.2f}")
            print(f"     ì¢…í•© ë¶„ì„ ì‹œê°„: {ì¢…í•©ë¶„ì„_time:.2f}ì´ˆ")
            print(f"     ì¢…í•© ì ìˆ˜: {model_results['summary']['score']:.1f}")

        results["models"][model_name] = model_results

    # ìµœì¢… ê²°ê³¼ ìš”ì•½ ë° ìˆœìœ„
    print(f"\n{'='*80}")
    print("ğŸ† ìµœì¢… ê²°ê³¼ ìš”ì•½ ë° ê¶Œì¥ì‚¬í•­")
    print(f"{'='*80}\n")

    # ìˆœìœ„ ë§¤ê¸°ê¸°
    ranked_models = []
    for model_name, model_data in results["models"].items():
        if "summary" in model_data:
            ranked_models.append({
                "name": model_name,
                "size": model_data["size"],
                "score": model_data["summary"]["score"],
                "speed": model_data["summary"]["avg_tokens_per_sec"],
                "quality": model_data["summary"]["avg_quality_score"],
                "time": model_data["summary"]["comprehensive_analysis_time"]
            })

    ranked_models.sort(key=lambda x: x["score"], reverse=True)

    print("ìˆœìœ„ | ëª¨ë¸ëª… | í¬ê¸° | ì†ë„ | í’ˆì§ˆ | ì¢…í•©ë¶„ì„ ì‹œê°„ | ì¢…í•©ì ìˆ˜")
    print("-" * 90)

    for i, model in enumerate(ranked_models, 1):
        print(f"{i:2d}. {model['name']:30s} | {model['size']:7s} | "
              f"{model['speed']:5.1f} t/s | {model['quality']:4.2f} | "
              f"{model['time']:6.2f}ì´ˆ | {model['score']:6.1f}")

    # ê¶Œì¥ì‚¬í•­
    if ranked_models:
        best_model = ranked_models[0]
        print(f"\nâœ¨ ê¶Œì¥ ëª¨ë¸: {best_model['name']}")
        print(f"   - ì¢…í•© ì ìˆ˜: {best_model['score']:.1f}")
        print(f"   - ì˜ˆìƒ ê°œì„ : {best_model['time']:.1f}ì´ˆ (í˜„ì¬ 13.8ì´ˆ ëŒ€ë¹„)")

        # í˜„ì¬ ëª¨ë¸ê³¼ ë¹„êµ
        current_model = next((m for m in ranked_models if m["name"] == "llama3.1:8b"), None)
        if current_model and best_model["name"] != "llama3.1:8b":
            time_improvement = (current_model["time"] - best_model["time"]) / current_model["time"] * 100
            quality_improvement = (best_model["quality"] - current_model["quality"]) / current_model["quality"] * 100

            print(f"\nğŸ“Š llama3.1:8b ëŒ€ë¹„ ê°œì„ ìœ¨:")
            print(f"   - ì†ë„: {time_improvement:+.1f}%")
            print(f"   - í’ˆì§ˆ: {quality_improvement:+.1f}%")

    # ê²°ê³¼ ì €ì¥
    output_file = f"model_comparison_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"\nê²°ê³¼ ì €ì¥: {output_file}")

    return results


if __name__ == "__main__":
    print("ğŸ” Ollama ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸ ì‹œì‘\n")
    print("í…ŒìŠ¤íŠ¸ ëŒ€ìƒ:")
    for model in TEST_MODELS:
        print(f"  - {model['name']} ({model['size']}) [Priority: {model['priority']}]")
    print()
    print("ì˜ˆìƒ ì†Œìš” ì‹œê°„: ì•½ 10-15ë¶„")
    print()

    results = asyncio.run(compare_models())
