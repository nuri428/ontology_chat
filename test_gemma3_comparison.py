#!/usr/bin/env python3
"""
Gemma3 vs ê¸°ì¡´ ìµœì  ëª¨ë¸ ë¹„êµ í…ŒìŠ¤íŠ¸

ë¹„êµ ëŒ€ìƒ:
1. llama3.1:8b (í˜„ì¬ ì‚¬ìš© ì¤‘, ë² ì´ìŠ¤ë¼ì¸)
2. qwen2:7b-instruct-q4_K_M (í•œêµ­ì–´ í›„ë³´)
3. gemma3 (ìƒˆë¡œ ë‹¤ìš´ë¡œë“œ, TBD)
"""

import asyncio
import time
import httpx
import json
from datetime import datetime


# ìµœì¢… í›„ë³´ ëª¨ë¸ë“¤
CANDIDATE_MODELS = [
    {"name": "llama3.1:8b", "status": "baseline", "note": "í˜„ì¬ ì‚¬ìš© ì¤‘"},
    {"name": "qwen2:7b-instruct-q4_K_M", "status": "candidate", "note": "í•œêµ­ì–´ í›„ë³´"},
    {"name": "gemma3:4b", "status": "new", "note": "Google ê²½ëŸ‰ ëª¨ë¸"},
    {"name": "gemma3:12b", "status": "new", "note": "Google ê³ ì„±ëŠ¥ ëª¨ë¸"},
]

# ì‹¤ì œ í”„ë¡œì íŠ¸ ì‚¬ìš© ì¼€ì´ìŠ¤ í…ŒìŠ¤íŠ¸
TEST_CASES = [
    {
        "name": "ë‹¨ìˆœ ë‰´ìŠ¤ ì¡°íšŒ",
        "prompt": """ê¸ˆìœµ ì• ë„ë¦¬ìŠ¤íŠ¸ë¡œì„œ ë‹µë³€í•˜ì„¸ìš”.

**ì§ˆì˜**: ì‚¼ì„±ì „ì ìµœê·¼ ë‰´ìŠ¤

3-4ë¬¸ì¥ìœ¼ë¡œ ê°„ë‹¨íˆ ìš”ì•½í•˜ì„¸ìš”.""",
        "expected_time": "3-5ì´ˆ",
        "weight": 0.2
    },
    {
        "name": "ê¸°ì—… ë¶„ì„",
        "prompt": """ê¸ˆìœµ ì• ë„ë¦¬ìŠ¤íŠ¸ë¡œì„œ ë‹µë³€í•˜ì„¸ìš”.

**ì§ˆì˜**: ì‚¼ì„±ì „ì HBM ê²½ìŸë ¥

5-7ë¬¸ì¥ìœ¼ë¡œ í•µì‹¬ ê²½ìŸë ¥ì„ ë¶„ì„í•˜ì„¸ìš”.""",
        "expected_time": "5-8ì´ˆ",
        "weight": 0.3
    },
    {
        "name": "ë¹„êµ ë¶„ì„ (ì¢…í•©)",
        "prompt": """ê¸ˆìœµ ì• ë„ë¦¬ìŠ¤íŠ¸ë¡œì„œ ë‹¤ìŒ ì§ˆì˜ì— ëŒ€í•œ íˆ¬ì ë¶„ì„ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.

**ì§ˆì˜**: ì‚¼ì„±ì „ìì™€ SKí•˜ì´ë‹‰ìŠ¤ì˜ HBM ê²½ìŸë ¥ ë¹„êµ
**ë°ì´í„°**:
- ì‚¼ì„±ì „ì: HBM3E 16ë‹¨ ê°œë°œ ì¤‘, í•˜ì´ë¸Œë¦¬ë“œ ë³¸ë”© ê¸°ìˆ  ë„ì… ê³„íš
- SKí•˜ì´ë‹‰ìŠ¤: HBM3E ì´ë¯¸ ì–‘ì‚° ì¤‘, ì—”ë¹„ë””ì•„ ì£¼ìš” ê³µê¸‰ì—…ì²´

ë‹¤ìŒ êµ¬ì¡°ë¡œ Markdown ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš” (800ì ì´ë‚´):

# Executive Summary
- í•µì‹¬ ë°œê²¬ì‚¬í•­ 3ê°œ (bullet points)

# Market Analysis
- ì‹œì¥ ìƒí™© (100-150ì)

# Key Insights
- ê¸°ìˆ  ê²½ìŸë ¥ ë¹„êµ
- ì‹œì¥ ì ìœ ìœ¨ ë¶„ì„

# Investment Perspective
- íˆ¬ì ê´€ì  ê¶Œì¥ì‚¬í•­

ë°”ë¡œ ì‹œì‘:""",
        "expected_time": "10-15ì´ˆ",
        "weight": 0.5
    }
]


async def test_single_model(model_name: str, test_case: dict, ollama_url: str = "http://192.168.0.11:11434") -> dict:
    """ë‹¨ì¼ ëª¨ë¸ í…ŒìŠ¤íŠ¸"""

    start_time = time.time()

    try:
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                f"{ollama_url}/api/generate",
                json={
                    "model": model_name,
                    "prompt": test_case["prompt"],
                    "stream": False,
                    "options": {
                        "temperature": 0.7,
                        "num_predict": 1000
                    }
                }
            )

            total_time = time.time() - start_time

            if response.status_code == 200:
                data = response.json()
                response_text = data.get("response", "")

                # í†µê³„
                tokens_generated = data.get("eval_count", 0)
                generation_time = data.get("eval_duration", 0) / 1e9
                tokens_per_sec = tokens_generated / generation_time if generation_time > 0 else 0

                # í’ˆì§ˆ í‰ê°€
                quality = evaluate_quality(response_text, test_case["name"])

                return {
                    "success": True,
                    "total_time": total_time,
                    "response_text": response_text,
                    "response_length": len(response_text),
                    "tokens_generated": tokens_generated,
                    "generation_time": generation_time,
                    "tokens_per_sec": tokens_per_sec,
                    "quality_score": quality,
                    "error": None
                }
            else:
                return {
                    "success": False,
                    "total_time": total_time,
                    "error": f"HTTP {response.status_code}"
                }

    except Exception as e:
        total_time = time.time() - start_time
        return {
            "success": False,
            "total_time": total_time,
            "error": str(e)
        }


def evaluate_quality(response_text: str, test_type: str) -> dict:
    """ì‘ë‹µ í’ˆì§ˆ í‰ê°€"""

    scores = {}

    # 1. ê¸¸ì´ ì ì ˆì„±
    length = len(response_text)
    if test_type == "ë‹¨ìˆœ ë‰´ìŠ¤ ì¡°íšŒ":
        target_range = (150, 400)
    elif test_type == "ê¸°ì—… ë¶„ì„":
        target_range = (300, 600)
    else:  # ì¢…í•© ë¶„ì„
        target_range = (600, 1200)

    if target_range[0] <= length <= target_range[1]:
        scores["length"] = 1.0
    elif target_range[0] * 0.7 <= length <= target_range[1] * 1.3:
        scores["length"] = 0.7
    else:
        scores["length"] = 0.3

    # 2. í•œêµ­ì–´ í’ˆì§ˆ
    korean_chars = sum(1 for c in response_text if 'ê°€' <= c <= 'í£')
    korean_ratio = korean_chars / length if length > 0 else 0

    if korean_ratio > 0.4:
        scores["korean"] = 1.0
    elif korean_ratio > 0.2:
        scores["korean"] = 0.6
    elif korean_ratio > 0.05:
        scores["korean"] = 0.3
    else:
        scores["korean"] = 0.0

    # 3. êµ¬ì¡°í™” (Markdown)
    has_headers = "#" in response_text
    has_bullets = ("â€¢" in response_text or "-" in response_text or "*" in response_text)

    if test_type == "ë¹„êµ ë¶„ì„ (ì¢…í•©)":
        if has_headers and has_bullets:
            scores["structure"] = 1.0
        elif has_headers or has_bullets:
            scores["structure"] = 0.5
        else:
            scores["structure"] = 0.2
    else:
        scores["structure"] = 1.0 if has_bullets else 0.5

    # 4. ê¸ˆìœµ ìš©ì–´ (íˆ¬ì ë¶„ì„ í’ˆì§ˆ)
    finance_keywords = ["íˆ¬ì", "ê²½ìŸë ¥", "ì‹œì¥", "ì„±ì¥", "ìˆ˜ìµ", "ë¦¬ìŠ¤í¬", "ì „ë§", "ë¶„ì„", "ë§¤ì¶œ", "ê¸°ì—…"]
    keyword_count = sum(1 for kw in finance_keywords if kw in response_text)
    scores["finance"] = min(keyword_count / 5.0, 1.0)

    # 5. ì´ìƒí•œ ì¶œë ¥ ê°ì§€ (qwen3ì˜ <think> ê°™ì€ ê²ƒ)
    has_artifacts = any(tag in response_text for tag in ["<think>", "<|", "```thinking"])
    scores["clean"] = 0.0 if has_artifacts else 1.0

    # ì¢…í•© ì ìˆ˜
    weights = {
        "length": 0.15,
        "korean": 0.35,
        "structure": 0.15,
        "finance": 0.20,
        "clean": 0.15
    }

    overall = sum(scores[k] * weights[k] for k in scores)
    scores["overall"] = overall

    return scores


async def compare_models():
    """ëª¨ë¸ ë¹„êµ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""

    print("=" * 80)
    print("ğŸ”¥ Gemma3 vs ìµœì  í›„ë³´ ëª¨ë¸ ë¹„êµ í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    print()
    print(f"í…ŒìŠ¤íŠ¸ ëŒ€ìƒ ëª¨ë¸: {len(CANDIDATE_MODELS)}ê°œ")
    for model in CANDIDATE_MODELS:
        print(f"  - {model['name']} [{model['status']}] {model['note']}")
    print()
    print(f"í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤: {len(TEST_CASES)}ê°œ")
    print()

    results = {
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "models": {}
    }

    for model_info in CANDIDATE_MODELS:
        model_name = model_info["name"]

        print(f"\n{'='*80}")
        print(f"ğŸ“Š ëª¨ë¸: {model_name}")
        print(f"{'='*80}")

        model_results = {
            "model_name": model_name,
            "status": model_info["status"],
            "tests": []
        }

        for test_case in TEST_CASES:
            print(f"\n  ğŸ“ í…ŒìŠ¤íŠ¸: {test_case['name']} (ëª©í‘œ: {test_case['expected_time']})")

            test_result = await test_single_model(model_name, test_case)

            if test_result["success"]:
                model_results["tests"].append({
                    "test_name": test_case["name"],
                    "weight": test_case["weight"],
                    "speed": {
                        "total_time": test_result["total_time"],
                        "tokens_per_sec": test_result["tokens_per_sec"],
                        "tokens_generated": test_result["tokens_generated"]
                    },
                    "quality": test_result["quality_score"],
                    "response_preview": test_result["response_text"][:300]
                })

                print(f"    âœ… ì„±ê³µ")
                print(f"       ì‹œê°„: {test_result['total_time']:.2f}ì´ˆ")
                print(f"       ì†ë„: {test_result['tokens_per_sec']:.1f} t/s")
                print(f"       í’ˆì§ˆ: {test_result['quality_score']['overall']:.2f}")
                print(f"       ê¸¸ì´: {test_result['response_length']}ì")
                print(f"       í•œêµ­ì–´: {test_result['quality_score']['korean']:.2f}")
                print(f"       ê¹”ë”í•¨: {test_result['quality_score']['clean']:.2f}")

            else:
                model_results["tests"].append({
                    "test_name": test_case["name"],
                    "error": test_result["error"]
                })
                print(f"    âŒ ì‹¤íŒ¨: {test_result['error']}")

            await asyncio.sleep(2)

        # ëª¨ë¸ë³„ ì¢…í•© ì ìˆ˜ ê³„ì‚°
        successful_tests = [t for t in model_results["tests"] if "error" not in t]
        if successful_tests:
            # ê°€ì¤‘ í‰ê· 
            weighted_quality = sum(
                t["quality"]["overall"] * t["weight"]
                for t in successful_tests
            ) / sum(t["weight"] for t in successful_tests)

            weighted_time = sum(
                t["speed"]["total_time"] * t["weight"]
                for t in successful_tests
            ) / sum(t["weight"] for t in successful_tests)

            avg_speed = sum(
                t["speed"]["tokens_per_sec"] for t in successful_tests
            ) / len(successful_tests)

            # ì¢…í•© ì ìˆ˜ (í’ˆì§ˆ 60%, ì†ë„ 40%)
            # ì†ë„ ì ìˆ˜: ë¹ ë¥¼ìˆ˜ë¡ ë†’ìŒ (ê¸°ì¤€: 15ì´ˆ = 0ì , 5ì´ˆ = 100ì )
            speed_score = max(0, min(100, (15 - weighted_time) * 10))
            quality_score = weighted_quality * 100

            final_score = quality_score * 0.6 + speed_score * 0.4

            model_results["summary"] = {
                "weighted_quality": weighted_quality,
                "weighted_time": weighted_time,
                "avg_tokens_per_sec": avg_speed,
                "speed_score": speed_score,
                "quality_score": quality_score,
                "final_score": final_score
            }

            print(f"\n  ğŸ“ˆ ì¢…í•© í‰ê°€:")
            print(f"     í’ˆì§ˆ ì ìˆ˜: {quality_score:.1f} (ê°€ì¤‘ í‰ê· : {weighted_quality:.2f})")
            print(f"     ì†ë„ ì ìˆ˜: {speed_score:.1f} (ê°€ì¤‘ ì‹œê°„: {weighted_time:.2f}ì´ˆ)")
            print(f"     í† í° ì†ë„: {avg_speed:.1f} t/s")
            print(f"     ğŸ† ìµœì¢… ì ìˆ˜: {final_score:.1f}")

        results["models"][model_name] = model_results

    # ìµœì¢… ìˆœìœ„ ë° ê¶Œì¥ì‚¬í•­
    print(f"\n{'='*80}")
    print("ğŸ† ìµœì¢… ìˆœìœ„ ë° ê¶Œì¥ì‚¬í•­")
    print(f"{'='*80}\n")

    ranked = []
    for model_name, data in results["models"].items():
        if "summary" in data:
            ranked.append({
                "name": model_name,
                "score": data["summary"]["final_score"],
                "quality": data["summary"]["weighted_quality"],
                "time": data["summary"]["weighted_time"],
                "speed": data["summary"]["avg_tokens_per_sec"],
                "status": data["status"]
            })

    ranked.sort(key=lambda x: x["score"], reverse=True)

    print(f"{'ìˆœìœ„':<4} {'ëª¨ë¸ëª…':<35} {'ìµœì¢…ì ìˆ˜':<10} {'í’ˆì§ˆ':<8} {'ì‹œê°„':<10} {'ì†ë„':<12} {'ìƒíƒœ'}")
    print("-" * 100)

    for i, model in enumerate(ranked, 1):
        status_emoji = "ğŸŒŸ" if model["status"] == "baseline" else "ğŸ†•" if model["status"] == "new" else "ğŸ”¸"
        print(f"{i:<4} {status_emoji} {model['name']:<33} "
              f"{model['score']:>6.1f} {' '*3} "
              f"{model['quality']:>4.2f} {' '*3} "
              f"{model['time']:>6.2f}ì´ˆ {' '*2} "
              f"{model['speed']:>6.1f} t/s {' '*2} "
              f"{model['status']}")

    # ê¶Œì¥ì‚¬í•­
    if ranked:
        best = ranked[0]
        baseline = next((m for m in ranked if m["status"] == "baseline"), None)

        print(f"\nâœ¨ ê¶Œì¥ ëª¨ë¸: {best['name']}")
        print(f"   ìµœì¢… ì ìˆ˜: {best['score']:.1f}")
        print(f"   í’ˆì§ˆ: {best['quality']:.2f}")
        print(f"   ì†ë„: {best['time']:.2f}ì´ˆ")

        if baseline and best["name"] != baseline["name"]:
            quality_diff = (best["quality"] - baseline["quality"]) / baseline["quality"] * 100
            time_diff = (baseline["time"] - best["time"]) / baseline["time"] * 100

            print(f"\nğŸ“Š {baseline['name']} ëŒ€ë¹„:")
            print(f"   í’ˆì§ˆ: {quality_diff:+.1f}%")
            print(f"   ì†ë„: {time_diff:+.1f}%")

            if quality_diff > 5 or time_diff > 10:
                print(f"\nâœ… ê¶Œì¥: {best['name']}ë¡œ ë³€ê²½")
            else:
                print(f"\nâš ï¸ ê°œì„  í­ì´ ì‘ì•„ í˜„ì¬ ëª¨ë¸ ìœ ì§€ ê¶Œì¥")
        else:
            print(f"\nâœ… í˜„ì¬ ëª¨ë¸({baseline['name']})ì´ ìµœì ì…ë‹ˆë‹¤!")

    # ê²°ê³¼ ì €ì¥
    output_file = f"gemma3_comparison_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"\nê²°ê³¼ ì €ì¥: {output_file}")

    return results


if __name__ == "__main__":
    print("â³ Gemma3 ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ëŒ€ê¸° ì¤‘...")
    print("   ë‹¤ìš´ë¡œë“œ ì™„ë£Œ í›„ í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.\n")

    results = asyncio.run(compare_models())
