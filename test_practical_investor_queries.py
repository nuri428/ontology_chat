#!/usr/bin/env python3
"""
ì‹¤ìš©ì  íˆ¬ìì ì§ˆì˜ í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ
- ì‹¤ì œ íˆ¬ììê°€ ìì£¼ í•˜ëŠ” ì§ˆë¬¸ë“¤
- íŠ¹ì • ì‚¬ì•ˆ, ì¢…ëª©, ë‰´ìŠ¤, ì‹¤ì  ê´€ë ¨ ì§ˆì˜
- í’ˆì§ˆ í–¥ìƒ í¬ì¸íŠ¸ ë„ì¶œ
"""

import asyncio
import json
import time
from typing import List, Dict, Any, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
import statistics

# í”„ë¡œì íŠ¸ ëª¨ë“ˆë“¤
from api.services.chat_service import ChatService
from api.services.enhanced_chat_service import EnhancedChatService

@dataclass
class PracticalTestQuestion:
    """ì‹¤ìš©ì  í…ŒìŠ¤íŠ¸ ì§ˆë¬¸"""
    id: str
    question: str
    category: str
    query_type: str  # news, performance, recent_issues, product_launch, analysis
    expected_content: List[str]  # ì‘ë‹µì— í¬í•¨ë˜ì–´ì•¼ í•  í•µì‹¬ ë‚´ìš©
    evaluation_focus: List[str]  # í‰ê°€ ì¤‘ì  ì‚¬í•­
    target_response_time: float = 3.0  # ëª©í‘œ ì‘ë‹µ ì‹œê°„(ì´ˆ)

@dataclass
class PracticalEvaluation:
    """ì‹¤ìš©ì  í‰ê°€ ê²°ê³¼"""
    question_id: str
    response_text: str
    response_time: float

    # ì‹¤ìš©ì  í‰ê°€ ì§€í‘œ
    relevance_score: float  # ê´€ë ¨ì„± (0-10)
    timeliness_score: float  # ì‹œì˜ì„± (0-10)
    specificity_score: float  # êµ¬ì²´ì„± (0-10)
    actionable_score: float  # ì‹¤í–‰ê°€ëŠ¥ì„± (0-10)
    data_richness_score: float  # ë°ì´í„° í’ë¶€ì„± (0-10)
    overall_score: float  # ì¢…í•©ì ìˆ˜ (0-10)

    missing_content: List[str]
    strengths: List[str]
    weaknesses: List[str]
    improvement_suggestions: List[str]

class PracticalInvestorQueryTester:
    """ì‹¤ìš©ì  íˆ¬ìì ì§ˆì˜ í…ŒìŠ¤í„°"""

    def __init__(self):
        self.chat_service = None
        self.enhanced_chat_service = None
        self.test_results = []

    async def initialize_services(self):
        """ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        try:
            self.enhanced_chat_service = EnhancedChatService()
            print("âœ… Enhanced Chat Service ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ Enhanced Chat Service ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            try:
                self.chat_service = ChatService()
                print("âœ… Basic Chat Service ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e2:
                print(f"âŒ Chat Service ì´ˆê¸°í™” ì‹¤íŒ¨: {e2}")
                raise

    def create_practical_questions(self) -> List[PracticalTestQuestion]:
        """ì‹¤ì œ íˆ¬ììë“¤ì´ ìì£¼ í•˜ëŠ” ì§ˆë¬¸ë“¤"""
        questions = [
            # === ë‰´ìŠ¤ ì¡°íšŒ ê´€ë ¨ ===
            PracticalTestQuestion(
                id="news_01",
                question="ì‚¼ì„±ì „ì ê´€ë ¨ ìµœê·¼ ë‰´ìŠ¤ ì¤‘ì—ì„œ ì£¼ê°€ì— ì˜í–¥ì„ ì¤„ë§Œí•œ ë‰´ìŠ¤ê°€ ìˆë‚˜ìš”?",
                category="ë‰´ìŠ¤ì¡°íšŒ",
                query_type="news",
                expected_content=["ì‚¼ì„±ì „ì", "ìµœê·¼ ë‰´ìŠ¤", "ì£¼ê°€ ì˜í–¥", "êµ¬ì²´ì  ë‰´ìŠ¤ ì œëª©"],
                evaluation_focus=["ìµœì‹ ì„±", "ì£¼ê°€ ì—°ê´€ì„±", "êµ¬ì²´ì„±"],
                target_response_time=2.0
            ),

            PracticalTestQuestion(
                id="news_02",
                question="ë°˜ë„ì²´ ì—…ê³„ì—ì„œ ì˜¤ëŠ˜ ë°œí‘œëœ ì¤‘ìš”í•œ ë‰´ìŠ¤ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
                category="ë‰´ìŠ¤ì¡°íšŒ",
                query_type="news",
                expected_content=["ë°˜ë„ì²´", "ì˜¤ëŠ˜", "ì¤‘ìš” ë‰´ìŠ¤", "ì—…ê³„ ë™í–¥"],
                evaluation_focus=["ë‹¹ì¼ì„±", "ì—…ê³„ íŠ¹í™”", "ì¤‘ìš”ë„ íŒë³„"],
                target_response_time=2.0
            ),

            # === ì‹¤ì  ê´€ë ¨ ===
            PracticalTestQuestion(
                id="performance_01",
                question="ìµœê·¼ ë§¤ì¶œì´ í¬ê²Œ ì˜¤ë¥¸ ì¢…ëª©ë“¤ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”",
                category="ì‹¤ì ë¶„ì„",
                query_type="performance",
                expected_content=["ë§¤ì¶œ ì¦ê°€", "ì¢…ëª© ì¶”ì²œ", "êµ¬ì²´ì  íšŒì‚¬ëª…", "ì¦ê°€ìœ¨"],
                evaluation_focus=["ì‹¤ì  ë°ì´í„°", "ì¢…ëª© ì„ ë³„", "ì¶”ì²œ ê·¼ê±°"],
                target_response_time=3.0
            ),

            PracticalTestQuestion(
                id="performance_02",
                question="3ë¶„ê¸° ì‹¤ì ì´ ì˜ˆìƒë³´ë‹¤ ì¢‹ì•˜ë˜ íšŒì‚¬ë“¤ì€ ì–´ë””ì¸ê°€ìš”?",
                category="ì‹¤ì ë¶„ì„",
                query_type="performance",
                expected_content=["3ë¶„ê¸°", "ì‹¤ì ", "ì˜ˆìƒ ëŒ€ë¹„", "íšŒì‚¬ëª…"],
                evaluation_focus=["ë¶„ê¸°ë³„ ë°ì´í„°", "ì˜ˆìƒì¹˜ ë¹„êµ", "ì‹¤ì  ì„œí”„ë¼ì´ì¦ˆ"],
                target_response_time=3.0
            ),

            # === ìµœê·¼ ì´ìŠˆ ì¢…ëª© ===
            PracticalTestQuestion(
                id="issues_01",
                question="ìµœê·¼ ì´ìŠˆê°€ ë˜ëŠ” ì¢…ëª©ì€ ë¬´ì—‡ì´ê³  ì´ìœ ëŠ” ë­”ê°€ìš”?",
                category="ì´ìŠˆë¶„ì„",
                query_type="recent_issues",
                expected_content=["ìµœê·¼ ì´ìŠˆ", "ì¢…ëª©ëª…", "ì´ìŠˆ ì´ìœ ", "ì‹œì¥ ë°˜ì‘"],
                evaluation_focus=["ì´ìŠˆ íŒŒì•…", "ì›ì¸ ë¶„ì„", "ì‹œì˜ì„±"],
                target_response_time=2.5
            ),

            PracticalTestQuestion(
                id="issues_02",
                question="ê¸‰ë“±í•œ ì¢…ëª©ë“¤ì˜ ê¸‰ë“± ì´ìœ ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”",
                category="ì´ìŠˆë¶„ì„",
                query_type="recent_issues",
                expected_content=["ê¸‰ë“± ì¢…ëª©", "ê¸‰ë“± ì´ìœ ", "ë¶„ì„", "ì£¼ê°€ ë³€ë™"],
                evaluation_focus=["ê¸‰ë“± ì¢…ëª© ì‹ë³„", "ì›ì¸ ë¶„ì„", "ë…¼ë¦¬ì  ì„¤ëª…"],
                target_response_time=3.0
            ),

            # === ì œí’ˆ ë°œí‘œ ê´€ë ¨ ===
            PracticalTestQuestion(
                id="product_01",
                question="ì•„ì´ì˜¨2 ë°œí‘œì— ëŒ€í•œ NCì†Œí”„íŠ¸ì˜ ì „ë§ì€ ì–´ë–¤ê°€ìš”?",
                category="ì œí’ˆë°œí‘œ",
                query_type="product_launch",
                expected_content=["ì•„ì´ì˜¨2", "NCì†Œí”„íŠ¸", "ì „ë§", "ê²Œì„ ì‚°ì—…"],
                evaluation_focus=["íŠ¹ì • ì œí’ˆ ì •ë³´", "íšŒì‚¬ë³„ ì˜í–¥", "ì „ë§ ë¶„ì„"],
                target_response_time=2.5
            ),

            PracticalTestQuestion(
                id="product_02",
                question="ìµœê·¼ ë°œí‘œëœ ì‹ ì œí’ˆì´ ì£¼ê°€ì— ê¸ì •ì  ì˜í–¥ì„ ì¤€ íšŒì‚¬ëŠ”?",
                category="ì œí’ˆë°œí‘œ",
                query_type="product_launch",
                expected_content=["ì‹ ì œí’ˆ ë°œí‘œ", "ì£¼ê°€ ìƒìŠ¹", "íšŒì‚¬ëª…", "ì œí’ˆ ì •ë³´"],
                evaluation_focus=["ì‹ ì œí’ˆ ì‹ë³„", "ì£¼ê°€ ì—°ê´€ì„±", "ì˜í–¥ ë¶„ì„"],
                target_response_time=3.0
            ),

            # === íŠ¹ì • ë¶„ì•¼ ë¶„ì„ ===
            PracticalTestQuestion(
                id="analysis_01",
                question="2ì°¨ì „ì§€ ê´€ë ¨ì£¼ ì¤‘ì—ì„œ ì˜¬í•´ ì‹¤ì ì´ ê°€ì¥ ì¢‹ì€ íšŒì‚¬ëŠ”?",
                category="ë¶„ì•¼ë¶„ì„",
                query_type="analysis",
                expected_content=["2ì°¨ì „ì§€", "ê´€ë ¨ì£¼", "ì˜¬í•´ ì‹¤ì ", "íšŒì‚¬ ë¹„êµ"],
                evaluation_focus=["ì„¹í„° ë¶„ì„", "ì‹¤ì  ë¹„êµ", "ìˆœìœ„ ì„ ì •"],
                target_response_time=3.5
            ),

            PracticalTestQuestion(
                id="analysis_02",
                question="K-ë·°í‹° ê´€ë ¨ ì¢…ëª©ë“¤ì˜ í•´ì™¸ ì§„ì¶œ ì„±ê³¼ëŠ” ì–´ë–¤ê°€ìš”?",
                category="ë¶„ì•¼ë¶„ì„",
                query_type="analysis",
                expected_content=["K-ë·°í‹°", "ê´€ë ¨ ì¢…ëª©", "í•´ì™¸ ì§„ì¶œ", "ì„±ê³¼ ë¶„ì„"],
                evaluation_focus=["í…Œë§ˆì£¼ íŒŒì•…", "í•´ì™¸ ì‹¤ì ", "ì„±ê³¼ ì¸¡ì •"],
                target_response_time=3.5
            ),

            # === ì‹œí™© ë¶„ì„ ===
            PracticalTestQuestion(
                id="market_01",
                question="ì˜¤ëŠ˜ ì½”ìŠ¤í”¼ê°€ ì˜¤ë¥¸ ì´ìœ ì™€ ìƒìŠ¹ ì£¼ë„ì£¼ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
                category="ì‹œí™©ë¶„ì„",
                query_type="market_analysis",
                expected_content=["ì½”ìŠ¤í”¼", "ìƒìŠ¹ ì´ìœ ", "ì£¼ë„ì£¼", "ì‹œì¥ ë™í–¥"],
                evaluation_focus=["ë‹¹ì¼ ì‹œí™©", "ìƒìŠ¹ ìš”ì¸", "ì£¼ë„ì£¼ ì‹ë³„"],
                target_response_time=2.0
            ),
        ]

        return questions

    async def run_practical_test(self, question: PracticalTestQuestion) -> Tuple[str, float]:
        """ì‹¤ìš©ì  ì§ˆë¬¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        start_time = time.time()

        try:
            if self.enhanced_chat_service:
                response = await self.enhanced_chat_service.generate_answer(question.question)
            else:
                response = await self.chat_service.generate_answer(question.question)

            response_time = time.time() - start_time

            # ì‘ë‹µ êµ¬ì¡°ì— ë”°ë¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            if isinstance(response, dict):
                response_text = response.get("answer", "") or response.get("markdown", "") or response.get("response", "")
            else:
                response_text = str(response)

            return response_text, response_time

        except Exception as e:
            response_time = time.time() - start_time
            return f"ERROR: {str(e)}", response_time

    def evaluate_practical_response(
        self,
        question: PracticalTestQuestion,
        response: str,
        response_time: float
    ) -> PracticalEvaluation:
        """ì‹¤ìš©ì  ê´€ì ì—ì„œ ì‘ë‹µ í‰ê°€"""

        response_lower = response.lower()
        missing_content = []
        strengths = []
        weaknesses = []
        suggestions = []

        # === ê´€ë ¨ì„± í‰ê°€ ===
        expected_found = 0
        for content in question.expected_content:
            if content.lower() in response_lower:
                expected_found += 1
            else:
                missing_content.append(content)

        relevance_score = (expected_found / len(question.expected_content)) * 10

        # === ì‹œì˜ì„± í‰ê°€ ===
        timeliness_indicators = ["ìµœê·¼", "ì˜¤ëŠ˜", "ì–´ì œ", "ì´ë²ˆì£¼", "3ë¶„ê¸°", "ì˜¬í•´", "2024", "ë°œí‘œ", "ê³µì‹œ"]
        timeliness_count = sum(1 for indicator in timeliness_indicators if indicator in response_lower)
        timeliness_score = min(timeliness_count * 2, 10)

        if timeliness_score >= 6:
            strengths.append("ì‹œì˜ì„± ì •ë³´ í¬í•¨")
        else:
            weaknesses.append("ì‹œì˜ì„± ì •ë³´ ë¶€ì¡±")
            suggestions.append("ìµœì‹  ì •ë³´ ë° êµ¬ì²´ì  ì‹œì  ëª…ì‹œ í•„ìš”")

        # === êµ¬ì²´ì„± í‰ê°€ ===
        specific_indicators = 0

        # íšŒì‚¬ëª…, ì¢…ëª©ëª… ì–¸ê¸‰
        company_patterns = ["ì‚¼ì„±", "LG", "SK", "í˜„ëŒ€", "í¬ìŠ¤ì½”", "ë„¤ì´ë²„", "ì¹´ì¹´ì˜¤", "NCì†Œí”„íŠ¸", "ì…€íŠ¸ë¦¬ì˜¨"]
        company_mentions = sum(1 for company in company_patterns if company in response)
        specific_indicators += min(company_mentions, 3)

        # ìˆ«ì, ìˆ˜ì¹˜ ì–¸ê¸‰
        import re
        numbers = re.findall(r'\d+(?:\.\d+)?%|\d+(?:,\d+)*(?:\.\d+)?[ì›ì–µë§Œì¡°]|\d+(?:\.\d+)?ë°°', response)
        specific_indicators += min(len(numbers), 3)

        # êµ¬ì²´ì  ì œí’ˆëª…, ì„œë¹„ìŠ¤ëª…
        if any(term in response for term in ["ì•„ì´ì˜¨", "ì œí’ˆ", "ì„œë¹„ìŠ¤", "ê¸°ìˆ "]):
            specific_indicators += 1

        specificity_score = min(specific_indicators * 1.5, 10)

        # === ì‹¤í–‰ê°€ëŠ¥ì„± í‰ê°€ ===
        actionable_indicators = ["ì¶”ì²œ", "ì „ë§", "ì˜ˆìƒ", "ëª©í‘œ", "ì „ëµ", "íˆ¬ì", "ë§¤ìˆ˜", "ë§¤ë„", "ë³´ìœ "]
        actionable_count = sum(1 for indicator in actionable_indicators if indicator in response_lower)
        actionable_score = min(actionable_count * 2, 10)

        if actionable_score >= 6:
            strengths.append("íˆ¬ì ì‹¤í–‰ ê´€ë ¨ ì •ë³´ ì œê³µ")
        else:
            suggestions.append("êµ¬ì²´ì  íˆ¬ì ê´€ì ì´ë‚˜ ì‹¤í–‰ ê°€ëŠ¥í•œ ì •ë³´ ì¶”ê°€ í•„ìš”")

        # === ë°ì´í„° í’ë¶€ì„± í‰ê°€ ===
        data_indicators = 0

        # ë‰´ìŠ¤ ì¸ìš©
        if any(term in response for term in ["ë‰´ìŠ¤", "ë°œí‘œ", "ê³µì‹œ", "ë³´ê³ ì„œ"]):
            data_indicators += 2

        # ì‹¤ì  ë°ì´í„°
        if any(term in response for term in ["ë§¤ì¶œ", "ì˜ì—…ì´ìµ", "ì‹¤ì ", "ë¶„ê¸°"]):
            data_indicators += 2

        # ì£¼ê°€ ì •ë³´
        if any(term in response for term in ["ì£¼ê°€", "ì£¼ì‹", "ìƒìŠ¹", "í•˜ë½", "ë“±ë½"]):
            data_indicators += 1

        # êµ¬ì²´ì  ì¶œì²˜
        if any(term in response for term in ["according to", "ë³´ê³ ì„œì— ë”°ë¥´ë©´", "ë°œí‘œì— ì˜í•˜ë©´"]):
            data_indicators += 1

        data_richness_score = min(data_indicators * 1.5, 10)

        # === ì‘ë‹µ ì‹œê°„ í‰ê°€ ===
        time_penalty = 1.0
        if response_time > question.target_response_time * 2:
            time_penalty = 0.7
            weaknesses.append(f"ì‘ë‹µ ì‹œê°„ ê³¼ë‹¤ ({response_time:.1f}ì´ˆ)")
        elif response_time > question.target_response_time:
            time_penalty = 0.85
            suggestions.append("ì‘ë‹µ ì†ë„ ê°œì„  í•„ìš”")
        else:
            strengths.append(f"ë¹ ë¥¸ ì‘ë‹µ ì†ë„ ({response_time:.1f}ì´ˆ)")

        # === ERROR ì²˜ë¦¬ ===
        if "ERROR" in response or "ì˜¤ë¥˜" in response:
            relevance_score *= 0.1
            actionable_score = 0
            weaknesses.append("ì‹œìŠ¤í…œ ì˜¤ë¥˜ ë°œìƒ")

        # === ì¢…í•© ì ìˆ˜ ê³„ì‚° ===
        overall_score = (
            relevance_score * 0.25 +      # ê´€ë ¨ì„± 25%
            timeliness_score * 0.20 +     # ì‹œì˜ì„± 20%
            specificity_score * 0.20 +    # êµ¬ì²´ì„± 20%
            actionable_score * 0.20 +     # ì‹¤í–‰ê°€ëŠ¥ì„± 20%
            data_richness_score * 0.15    # ë°ì´í„° í’ë¶€ì„± 15%
        ) * time_penalty

        # ì¹´í…Œê³ ë¦¬ë³„ ì¶”ê°€ í‰ê°€
        if question.query_type == "news" and "ë‰´ìŠ¤" not in response_lower:
            weaknesses.append("ë‰´ìŠ¤ ì •ë³´ ë¶€ì¡±")
            overall_score *= 0.8

        if question.query_type == "performance" and not any(term in response_lower for term in ["ì‹¤ì ", "ë§¤ì¶œ", "ìˆ˜ìµ"]):
            weaknesses.append("ì‹¤ì  ì •ë³´ ë¶€ì¡±")
            overall_score *= 0.8

        return PracticalEvaluation(
            question_id=question.id,
            response_text=response,
            response_time=response_time,
            relevance_score=relevance_score,
            timeliness_score=timeliness_score,
            specificity_score=specificity_score,
            actionable_score=actionable_score,
            data_richness_score=data_richness_score,
            overall_score=overall_score,
            missing_content=missing_content,
            strengths=strengths,
            weaknesses=weaknesses,
            improvement_suggestions=suggestions
        )

    async def run_comprehensive_practical_test(self) -> List[PracticalEvaluation]:
        """ì¢…í•© ì‹¤ìš©ì  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        await self.initialize_services()

        questions = self.create_practical_questions()
        evaluations = []

        print(f"\nğŸ¯ ì‹¤ìš©ì  íˆ¬ìì ì§ˆì˜ í…ŒìŠ¤íŠ¸ ì‹œì‘ ({len(questions)}ê°œ ì§ˆë¬¸)")
        print("=" * 80)

        for i, question in enumerate(questions, 1):
            print(f"\n[{i}/{len(questions)}] {question.query_type.upper()} - {question.category}")
            print(f"Q: {question.question}")
            print(f"ëª©í‘œ ì‹œê°„: {question.target_response_time}ì´ˆ")
            print("-" * 60)

            response, response_time = await self.run_practical_test(question)
            evaluation = self.evaluate_practical_response(question, response, response_time)
            evaluations.append(evaluation)

            # ê²°ê³¼ ì¶œë ¥
            print(f"â±ï¸ ì‘ë‹µì‹œê°„: {response_time:.2f}ì´ˆ (ëª©í‘œ: {question.target_response_time}ì´ˆ)")
            print(f"ğŸ¯ ì¢…í•©ì ìˆ˜: {evaluation.overall_score:.1f}/10")
            print(f"ğŸ“Š ì„¸ë¶€ì ìˆ˜: ê´€ë ¨ì„±({evaluation.relevance_score:.1f}) "
                  f"ì‹œì˜ì„±({evaluation.timeliness_score:.1f}) "
                  f"êµ¬ì²´ì„±({evaluation.specificity_score:.1f}) "
                  f"ì‹¤í–‰ì„±({evaluation.actionable_score:.1f}) "
                  f"ë°ì´í„°({evaluation.data_richness_score:.1f})")

            if evaluation.strengths:
                print(f"âœ… ê°•ì : {', '.join(evaluation.strengths)}")
            if evaluation.weaknesses:
                print(f"âš ï¸ ì•½ì : {', '.join(evaluation.weaknesses)}")
            if evaluation.missing_content:
                print(f"âŒ ëˆ„ë½: {', '.join(evaluation.missing_content)}")

            # ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°
            preview = response[:150] + ("..." if len(response) > 150 else "")
            print(f"ğŸ’¬ ì‘ë‹µ: {preview}")

        return evaluations

    def generate_practical_improvement_report(self, evaluations: List[PracticalEvaluation]) -> Dict[str, Any]:
        """ì‹¤ìš©ì  ê°œì„  ë³´ê³ ì„œ ìƒì„±"""
        if not evaluations:
            return {"error": "í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤"}

        # ê¸°ë³¸ í†µê³„
        overall_scores = [e.overall_score for e in evaluations]
        avg_score = statistics.mean(overall_scores)

        # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„
        category_stats = {}
        query_type_stats = {}

        for eval in evaluations:
            # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ (question_idì—ì„œ)
            category = eval.question_id.split('_')[0]
            if category not in category_stats:
                category_stats[category] = []
            category_stats[category].append(eval.overall_score)

        for category, scores in category_stats.items():
            query_type_stats[category] = {
                "í‰ê· ì ìˆ˜": round(statistics.mean(scores), 2),
                "ë¬¸ì œìˆ˜": len(scores),
                "í‰ê· ì‘ë‹µì‹œê°„": round(statistics.mean([e.response_time for e in evaluations if e.question_id.startswith(category)]), 2)
            }

        # ì£¼ìš” ë¬¸ì œì  ì§‘ê³„
        all_weaknesses = []
        all_suggestions = []
        all_missing = []

        for eval in evaluations:
            all_weaknesses.extend(eval.weaknesses)
            all_suggestions.extend(eval.improvement_suggestions)
            all_missing.extend(eval.missing_content)

        # ë¹ˆë„ ë¶„ì„
        weakness_freq = {}
        for weakness in all_weaknesses:
            weakness_freq[weakness] = weakness_freq.get(weakness, 0) + 1

        missing_freq = {}
        for missing in all_missing:
            missing_freq[missing] = missing_freq.get(missing, 0) + 1

        top_weaknesses = sorted(weakness_freq.items(), key=lambda x: x[1], reverse=True)[:5]
        top_missing = sorted(missing_freq.items(), key=lambda x: x[1], reverse=True)[:5]

        # ì„±ëŠ¥ ë¶„ì„
        response_times = [e.response_time for e in evaluations]
        slow_responses = [e for e in evaluations if e.response_time > 3.0]

        return {
            "summary": {
                "ì´_ì§ˆë¬¸ìˆ˜": len(evaluations),
                "í‰ê· ì ìˆ˜": round(avg_score, 2),
                "í‰ê· _ì‘ë‹µì‹œê°„": round(statistics.mean(response_times), 2),
                "ëª©í‘œì‹œê°„_ì´ˆê³¼": len(slow_responses),
                "ì„±ëŠ¥_ë“±ê¸‰": "ìš°ìˆ˜" if avg_score >= 8 else "ì–‘í˜¸" if avg_score >= 6 else "ê°œì„ í•„ìš”"
            },
            "ì¹´í…Œê³ ë¦¬ë³„_ì„±ëŠ¥": query_type_stats,
            "ì£¼ìš”_ë¬¸ì œì ": {
                "ë¹ˆë°œ_ì•½ì ": top_weaknesses,
                "ìì£¼_ëˆ„ë½ë˜ëŠ”_ë‚´ìš©": top_missing,
                "ëŠë¦°_ì‘ë‹µ": [{"ì§ˆë¬¸": e.question_id, "ì‹œê°„": e.response_time} for e in slow_responses]
            },
            "ê°œì„ _ìš°ì„ ìˆœìœ„": {
                "1ìˆœìœ„": "ë‰´ìŠ¤ ì •ë³´ ì œê³µ ê°•í™”" if any("ë‰´ìŠ¤" in w[0] for w in top_weaknesses) else "ì‘ë‹µ ì†ë„ ê°œì„ ",
                "2ìˆœìœ„": "êµ¬ì²´ì  ì¢…ëª©ëª… ë° ìˆ˜ì¹˜ ì •ë³´ ê°•í™”",
                "3ìˆœìœ„": "ì‹¤ì  ë°ì´í„° ì—°ê³„ ê°œì„ ",
                "4ìˆœìœ„": "ì‹œì˜ì„± ì •ë³´ ê°•í™”",
                "5ìˆœìœ„": "ì‹¤í–‰ ê°€ëŠ¥í•œ íˆ¬ì ì •ë³´ ì œê³µ"
            },
            "ìƒì„¸_ê²°ê³¼": [asdict(e) for e in evaluations]
        }

async def main():
    """ë©”ì¸ ì‹¤í–‰"""
    tester = PracticalInvestorQueryTester()

    try:
        # ì‹¤ìš©ì  í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        evaluations = await tester.run_comprehensive_practical_test()

        # ê°œì„  ë³´ê³ ì„œ ìƒì„±
        report = tester.generate_practical_improvement_report(evaluations)

        # ê²°ê³¼ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = f"/data/dev/git/ontology_chat/practical_investor_report_{timestamp}.json"

        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        # ìš”ì•½ ì¶œë ¥
        print(f"\nğŸ“Š ì‹¤ìš©ì  íˆ¬ìì ì§ˆì˜ í…ŒìŠ¤íŠ¸ ê²°ê³¼")
        print("=" * 80)
        print(f"ğŸ“ˆ í‰ê·  ì ìˆ˜: {report['summary']['í‰ê· ì ìˆ˜']}/10")
        print(f"â±ï¸ í‰ê·  ì‘ë‹µì‹œê°„: {report['summary']['í‰ê· _ì‘ë‹µì‹œê°„']:.1f}ì´ˆ")
        print(f"ğŸ¯ ì„±ëŠ¥ ë“±ê¸‰: {report['summary']['ì„±ëŠ¥_ë“±ê¸‰']}")

        print(f"\nğŸ† ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥:")
        for category, stats in report['ì¹´í…Œê³ ë¦¬ë³„_ì„±ëŠ¥'].items():
            print(f"  - {category}: {stats['í‰ê· ì ìˆ˜']}ì  ({stats['í‰ê· ì‘ë‹µì‹œê°„']:.1f}ì´ˆ)")

        print(f"\nâš ï¸ ì£¼ìš” ê°œì„  í•„ìš” ì‚¬í•­:")
        for i, (issue, count) in enumerate(report['ì£¼ìš”_ë¬¸ì œì ']['ë¹ˆë°œ_ì•½ì '][:3], 1):
            print(f"  {i}. {issue} ({count}íšŒ)")

        print(f"\nğŸ¯ ê°œì„  ìš°ì„ ìˆœìœ„:")
        for priority, action in report['ê°œì„ _ìš°ì„ ìˆœìœ„'].items():
            print(f"  {priority}: {action}")

        print(f"\nğŸ“ ìƒì„¸ ë³´ê³ ì„œ: {report_file}")

        return report

    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    asyncio.run(main())