# gemma3:4b 모델 평가 결과

## 실행 일시
2025-10-02 14:40

## 테스트 목적
gemma3:4b 모델이 단순 Q&A에서 llama3.1:8b보다 나은 선택인지 확인

## 주요 발견

### 1. Fast Handler (단순 Q&A)에서는 모델 차이 없음

**테스트 케이스:**
- "삼성전자 최근 뉴스"
- "SK하이닉스 주가"
- "현대차 전기차"
- "LG에너지솔루션"

**결과:**
- 응답 시간: 0.2~3.2초 (매우 빠름)
- 처리 방식: Fast Handler (템플릿 기반)
- **LLM 사용 거의 없음** → 모델 성능 무관

**결론:** Fast Handler는 데이터 조회 + 템플릿 기반 포맷팅이므로 **모델 선택이 의미 없음**

### 2. 데이터 조회 시간이 주요 병목

**테스트:**
- "반도체 시장 트렌드 분석"
  - gemma3:4b: 30.2초
  - llama3.1:8b: 30.2초
  - **동일한 30초**

**분석:**
- Neo4j/OpenSearch 검색 시간이 30초
- LLM 추론 시간은 전체의 10% 미만
- 모델 변경으로는 개선 불가

### 3. LangGraph (복잡한 분석)에서 gemma3:4b는 2배 느림

**이전 테스트 결과 (profile_results_20251002_212713.json):**
```
llama3.1:8b:
- 통합 쿼리 분석: 3.1초
- 통합 종합 분석: 13.8초
- 합계: ~17초

gemma3:4b:
- 통합 쿼리 분석: 6.1초 (2x slower)
- 통합 종합 분석: 26.3초 (2x slower)
- 합계: ~32초 (timeout)
```

**원인:**
- 단순 테스트 프롬프트: 300-600 토큰 → gemma3:4b 빠름
- 프로덕션 프롬프트: 2000+ 토큰 (컨텍스트 포함) → gemma3:4b 매우 느림
- **긴 컨텍스트 처리 능력 차이**

## 최종 결론

### gemma3:4b는 사용하지 않는 것이 최선

**이유:**
1. **Fast Handler**: 모델 무관 (템플릿 기반)
2. **LangGraph**: llama3.1:8b가 2배 빠름
3. **데이터 병목**: 모델 변경으로 해결 불가

### 권장 사항
- **계속 llama3.1:8b 사용**
- 성능 개선은 다른 방법으로:
  - Smart Caching (데이터 조회 캐싱)
  - Neo4j 쿼리 최적화
  - OpenSearch 인덱스 튜닝

## 추가 관찰

### 모델 다운로드 했지만 사용 안 함
- gemma3:12b: 38.5초 (너무 느림)
- qwen3:8b: `<think>` 출력 문제
- qwen3:8b-q8_0: 37.1초 (최악)

### 단순 테스트의 함정
- 고립된 단순 프롬프트 테스트는 프로덕션 성능 예측 불가
- 실제 워크로드 (긴 컨텍스트 + 복잡한 데이터)에서 테스트 필수
- gemma3:4b가 10.5초로 빨랐던 것은 **짧은 테스트 프롬프트 때문**

## 다음 단계
- [x] gemma3:4b 평가 완료
- [x] llama3.1:8b로 롤백
- [ ] Smart Caching 구현 (Phase 3)
- [ ] Neo4j 쿼리 최적화 (Phase 4)
