#!/usr/bin/env python3
"""ì˜ë¯¸ì  ìœ ì‚¬ë„ í•„í„°ë§ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
import asyncio
import sys
import time
sys.path.append('.')

async def test_semantic_filtering():
    """ì˜ë¯¸ì  ìœ ì‚¬ë„ í•„í„°ë§ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§  ì˜ë¯¸ì  ìœ ì‚¬ë„ í•„í„°ë§ í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    # í…ŒìŠ¤íŠ¸ìš© ë¬¸ì„œë“¤ (ì‹¤ì œë¡œëŠ” OpenSearchì—ì„œ ê°€ì ¸ì˜´)
    test_documents = [
        {
            "content": "ì‚¼ì„±ì „ìê°€ ìƒˆë¡œìš´ ë°˜ë„ì²´ ê¸°ìˆ ì„ ê°œë°œí–ˆë‹¤. ë©”ëª¨ë¦¬ ë°˜ë„ì²´ ë¶„ì•¼ì—ì„œ í˜ì‹ ì ì¸ ì„±ê³¼ë¥¼ ë³´ì˜€ë‹¤.",
            "title": "ì‚¼ì„±ì „ì ë°˜ë„ì²´ ê¸°ìˆ  í˜ì‹ ",
            "score": 0.95
        },
        {
            "content": "SKí•˜ì´ë‹‰ìŠ¤ë„ ë©”ëª¨ë¦¬ ì¹© ìƒì‚° ê¸°ìˆ ì„ í–¥ìƒì‹œì¼°ë‹¤. ì°¨ì„¸ëŒ€ ë©”ëª¨ë¦¬ ê¸°ìˆ  ê°œë°œì— ë°•ì°¨ë¥¼ ê°€í•˜ê³  ìˆë‹¤.",
            "title": "SKí•˜ì´ë‹‰ìŠ¤ ë©”ëª¨ë¦¬ ê¸°ìˆ  ë°œì „",
            "score": 0.88
        },
        {
            "content": "í˜„ëŒ€ìë™ì°¨ê°€ ì „ê¸°ì°¨ ì‹ ëª¨ë¸ì„ ì¶œì‹œí–ˆë‹¤. ë°°í„°ë¦¬ ê¸°ìˆ ê³¼ ììœ¨ì£¼í–‰ ê¸°ëŠ¥ì„ ê°•í™”í–ˆë‹¤.",
            "title": "í˜„ëŒ€ì°¨ ì „ê¸°ì°¨ ì‹ ëª¨ë¸ ì¶œì‹œ",
            "score": 0.82
        },
        {
            "content": "LGì—ë„ˆì§€ì†”ë£¨ì…˜ì´ ë°°í„°ë¦¬ ìƒì‚°ì„ í™•ëŒ€í•œë‹¤. ì „ê¸°ì°¨ìš© ë°°í„°ë¦¬ ìˆ˜ìš” ì¦ê°€ì— ëŒ€ì‘í•˜ê³  ìˆë‹¤.",
            "title": "LGì—ë„ˆì§€ì†”ë£¨ì…˜ ë°°í„°ë¦¬ ìƒì‚° í™•ëŒ€",
            "score": 0.79
        },
        {
            "content": "ì‚¼ì„±ì „ìì˜ ë©”ëª¨ë¦¬ ë°˜ë„ì²´ ì‚¬ì—… ì‹¤ì ì´ í˜¸ì¡°ë¥¼ ë³´ì´ê³  ìˆë‹¤. ê¸€ë¡œë²Œ ì‹œì¥ì—ì„œ ì ìœ ìœ¨ì„ ë†’ì´ê³  ìˆë‹¤.",
            "title": "ì‚¼ì„±ì „ì ë©”ëª¨ë¦¬ ì‚¬ì—… í˜¸ì¡°",
            "score": 0.91
        }
    ]

    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë“¤
    test_queries = [
        "ë°˜ë„ì²´ ë©”ëª¨ë¦¬ ê¸°ìˆ  ë°œì „",
        "ì „ê¸°ì°¨ ë°°í„°ë¦¬ ê¸°ìˆ ",
        "ì‚¼ì„±ì „ì ë°˜ë„ì²´ ì‹¤ì "
    ]

    try:
        from api.services.semantic_similarity import semantic_filter, filter_similar_content, semantic_rerank

        # ê²½ëŸ‰ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´)
        print("âš™ï¸  ê²½ëŸ‰ ëª¨ë¸ë¡œ ì´ˆê¸°í™” ì¤‘...")
        # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ë” ê°•ë ¥í•œ ëª¨ë¸ ì‚¬ìš©

        for i, query in enumerate(test_queries, 1):
            print(f"\n{i}ï¸âƒ£  í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: '{query}'")
            print("-" * 50)

            # 1. ì›ë³¸ ë¬¸ì„œ ì ìˆ˜
            print("ğŸ“„ ì›ë³¸ ë¬¸ì„œ ìˆœìœ„:")
            for j, doc in enumerate(test_documents[:3], 1):
                print(f"   {j}. {doc['title']} (ì ìˆ˜: {doc['score']:.2f})")

            # 2. ì˜ë¯¸ì  ì¬ì •ë ¬ í…ŒìŠ¤íŠ¸
            print("\nğŸ”„ ì˜ë¯¸ì  ì¬ì •ë ¬ ì¤‘...")
            start_time = time.perf_counter()

            try:
                reranked = await semantic_rerank(query, test_documents.copy())
                elapsed = (time.perf_counter() - start_time) * 1000

                print(f"   âœ“ ì¬ì •ë ¬ ì™„ë£Œ ({elapsed:.1f}ms)")
                print("\nğŸ“Š ì¬ì •ë ¬ ê²°ê³¼:")
                for j, doc in enumerate(reranked[:3], 1):
                    semantic_score = doc.get('semantic_score', 0)
                    combined_score = doc.get('combined_score', 0)
                    print(f"   {j}. {doc['title']}")
                    print(f"      ì˜ë¯¸ì ìˆ˜: {semantic_score:.3f}, í†µí•©ì ìˆ˜: {combined_score:.3f}")

            except Exception as e:
                print(f"   âŒ ì¬ì •ë ¬ ì‹¤íŒ¨: {e}")

            # 3. ìœ ì‚¬ë„ í•„í„°ë§ í…ŒìŠ¤íŠ¸
            print(f"\nğŸ¯ ìœ ì‚¬ë„ í•„í„°ë§ (ì„ê³„ê°’: 0.6)")
            start_time = time.perf_counter()

            try:
                filtered = await filter_similar_content(
                    query,
                    test_documents.copy(),
                    threshold=0.6,
                    top_k=3
                )
                elapsed = (time.perf_counter() - start_time) * 1000

                print(f"   âœ“ í•„í„°ë§ ì™„ë£Œ ({elapsed:.1f}ms)")
                print(f"   ğŸ“‰ {len(test_documents)}ê±´ â†’ {len(filtered)}ê±´")

                if filtered:
                    print("\nğŸ† ìµœì¢… ì„ íƒëœ ë¬¸ì„œ:")
                    for j, doc in enumerate(filtered, 1):
                        semantic_score = doc.get('semantic_score', 0)
                        print(f"   {j}. {doc['title']} (ì˜ë¯¸ì ìˆ˜: {semantic_score:.3f})")

            except Exception as e:
                print(f"   âŒ í•„í„°ë§ ì‹¤íŒ¨: {e}")

    except ImportError as e:
        print(f"âŒ ì˜ì¡´ì„± ëˆ„ë½: {e}")
        print("ğŸ’¡ ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”:")
        print("   uv add sentence-transformers scikit-learn torch")
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

async def test_semantic_diversity():
    """ì˜ë¯¸ì  ë‹¤ì–‘ì„± í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 60)
    print("ğŸŒˆ ì˜ë¯¸ì  ë‹¤ì–‘ì„± í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    # ì¤‘ë³µì„±ì´ ë†’ì€ ë¬¸ì„œë“¤
    similar_docs = [
        {"content": "ì‚¼ì„±ì „ì ë©”ëª¨ë¦¬ ë°˜ë„ì²´ ì„±ê³¼", "title": "ì‚¼ì„± ë©”ëª¨ë¦¬ 1"},
        {"content": "ì‚¼ì„± ë°˜ë„ì²´ ë©”ëª¨ë¦¬ ê¸°ìˆ  ë°œì „", "title": "ì‚¼ì„± ë©”ëª¨ë¦¬ 2"},
        {"content": "í˜„ëŒ€ì°¨ ì „ê¸°ì°¨ ê¸°ìˆ  í˜ì‹ ", "title": "í˜„ëŒ€ì°¨ ì „ê¸°ì°¨"},
        {"content": "LGë°°í„°ë¦¬ ìƒì‚° í™•ëŒ€ ê³„íš", "title": "LG ë°°í„°ë¦¬"}
    ]

    try:
        from api.services.semantic_similarity import semantic_filter

        # ë‹¤ì–‘ì„± ì ìˆ˜ ê³„ì‚°
        diversity_score = semantic_filter.calculate_semantic_diversity(similar_docs)
        print(f"ğŸ“Š ë¬¸ì„œ ì§‘í•©ì˜ ë‹¤ì–‘ì„± ì ìˆ˜: {diversity_score:.3f}")

        if diversity_score < 0.3:
            print("   âš ï¸  ë‚®ì€ ë‹¤ì–‘ì„± - ì¤‘ë³µ ì»¨í…ì¸ ê°€ ë§ìŒ")
        elif diversity_score < 0.6:
            print("   ğŸ”¶ ë³´í†µ ë‹¤ì–‘ì„±")
        else:
            print("   âœ… ë†’ì€ ë‹¤ì–‘ì„± - ë‹¤ì–‘í•œ ì£¼ì œ í¬í•¨")

        # í´ëŸ¬ìŠ¤í„°ë§ í…ŒìŠ¤íŠ¸
        print(f"\nğŸ—‚ï¸  ì˜ë¯¸ì  í´ëŸ¬ìŠ¤í„°ë§ (2ê°œ í´ëŸ¬ìŠ¤í„°):")
        clusters = semantic_filter.find_semantic_clusters(similar_docs, n_clusters=2)

        for i, cluster in enumerate(clusters, 1):
            print(f"   í´ëŸ¬ìŠ¤í„° {i}: {len(cluster)}ê°œ ë¬¸ì„œ")
            for doc in cluster:
                print(f"      - {doc['title']}")

    except Exception as e:
        print(f"âŒ ë‹¤ì–‘ì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

async def performance_test():
    """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 60)
    print("âš¡ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    # ë” ë§ì€ ë¬¸ì„œë¡œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
    large_doc_set = [
        {"content": f"í…ŒìŠ¤íŠ¸ ë¬¸ì„œ {i} - ë°˜ë„ì²´ ê´€ë ¨ ë‚´ìš©ì…ë‹ˆë‹¤", "title": f"ë¬¸ì„œ {i}"}
        for i in range(20)
    ]

    query = "ë°˜ë„ì²´ ê¸°ìˆ  ë°œì „"

    try:
        from api.services.semantic_similarity import semantic_rerank, filter_similar_content

        print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ì¡°ê±´: {len(large_doc_set)}ê°œ ë¬¸ì„œ")

        # ì¬ì •ë ¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        print("\n1ï¸âƒ£ ì˜ë¯¸ì  ì¬ì •ë ¬ ì„±ëŠ¥:")
        start = time.perf_counter()
        reranked = await semantic_rerank(query, large_doc_set.copy())
        elapsed = (time.perf_counter() - start) * 1000
        print(f"   â±ï¸  ì‹¤í–‰ ì‹œê°„: {elapsed:.1f}ms")
        print(f"   ğŸ“ˆ ì²˜ë¦¬ ì†ë„: {len(large_doc_set)/elapsed*1000:.1f} docs/sec")

        # í•„í„°ë§ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        print("\n2ï¸âƒ£ ìœ ì‚¬ë„ í•„í„°ë§ ì„±ëŠ¥:")
        start = time.perf_counter()
        filtered = await filter_similar_content(query, large_doc_set.copy(), top_k=5)
        elapsed = (time.perf_counter() - start) * 1000
        print(f"   â±ï¸  ì‹¤í–‰ ì‹œê°„: {elapsed:.1f}ms")
        print(f"   ğŸ“‰ í•„í„°ë§ ë¹„ìœ¨: {len(filtered)/len(large_doc_set)*100:.1f}%")

    except Exception as e:
        print(f"âŒ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    asyncio.run(test_semantic_filtering())
    asyncio.run(test_semantic_diversity())
    asyncio.run(performance_test())