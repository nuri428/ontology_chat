#!/usr/bin/env python3
"""LLM ê¸°ë°˜ ë‹µë³€ ìƒì„± í…ŒìŠ¤íŠ¸"""
import asyncio
import sys
sys.path.append('.')

async def test_llm_answer_generation():
    """LLM ë‹µë³€ ìƒì„± ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    print("ğŸ”¬ LLM ê¸°ë°˜ ë‹µë³€ ìƒì„± í…ŒìŠ¤íŠ¸")
    print("=" * 70)

    try:
        from api.services.chat_service import ChatService
        import time

        service = ChatService()

        # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë“¤
        test_queries = [
            "SMR ê´€ë ¨ ìœ ë§ íˆ¬ì ì¢…ëª©ì€?",
            "ë°˜ë„ì²´ ì‹œì¥ ì „ë§ì´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
            "ìµœê·¼ ì „ê¸°ì°¨ ë°°í„°ë¦¬ ê´€ë ¨ ì´ìŠˆëŠ”?",
            "ì‚¼ì„±ì „ì ì£¼ê°€ ì „ë§ì€?"
        ]

        for query in test_queries:
            print(f"\nğŸ“ ì§ˆë¬¸: {query}")
            print("-" * 50)

            start_time = time.time()

            try:
                # ë‹µë³€ ìƒì„±
                result = await service.generate_answer(query)

                # ê²°ê³¼ í™•ì¸
                answer = result.get("answer", "")
                sources_count = len(result.get("sources", []))
                processing_time = (time.time() - start_time) * 1000

                print(f"â±ï¸ ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ms")
                print(f"ğŸ“š ì†ŒìŠ¤ ê°œìˆ˜: {sources_count}ê°œ")

                # LLM ì¸ì‚¬ì´íŠ¸ê°€ í¬í•¨ë˜ì—ˆëŠ”ì§€ í™•ì¸
                if "ğŸ’¡" in answer or "ì¸ì‚¬ì´íŠ¸" in answer:
                    print("âœ… LLM ì¸ì‚¬ì´íŠ¸ ìƒì„± ì„±ê³µ")
                else:
                    print("âš ï¸ LLM ì¸ì‚¬ì´íŠ¸ ë¯¸í¬í•¨")

                # ë‹µë³€ ì¼ë¶€ ì¶œë ¥ (ì²˜ìŒ 500ì)
                print(f"\në‹µë³€ ë¯¸ë¦¬ë³´ê¸°:")
                print("-" * 40)
                print(answer[:500] + "..." if len(answer) > 500 else answer)

            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

        # LLM ìƒíƒœ í™•ì¸
        print("\n" + "=" * 70)
        print("ğŸ“Š LLM ìƒíƒœ í™•ì¸:")
        if service.ollama_llm:
            print("âœ… Ollama LLM í™œì„±í™”")
            print(f"   - ëª¨ë¸: {service.ollama_llm.model}")
            print(f"   - ì˜¨ë„: {service.ollama_llm.temperature}")
        else:
            print("âŒ Ollama LLM ë¹„í™œì„±í™”")

    except ImportError as e:
        print(f"âŒ ëª¨ë“ˆ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(test_llm_answer_generation())